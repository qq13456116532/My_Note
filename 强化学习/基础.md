#### 状态值函数
当智能体采⽤策略$π$时，累积回报服从⼀个分布，累积回报在状态s处 的期望值定义为状态-值函数
![](images/Pasted%20image%2020230727073505.png)


#### 状态-⾏为值函数
![](images/Pasted%20image%2020230727074110.png)


上面分别给出了状态值函数和状态-⾏为值函数的定 义计算式，但在实际真正计算和编程的时候并不会按照定义式编程。接


#### 状态值函数与状态-⾏为值函数的⻉尔曼⽅程
![](images/Pasted%20image%2020230727074720.png)
![](images/Pasted%20image%2020230727074739.png)

可以参考状态值函数的计算示意图来看上面的贝尔曼方程：
空⼼圆圈表⽰状态，实⼼圆圈表⽰状态-⾏为对
![状态值函数的计算示意图](images/Pasted%20image%2020230727074814.png)
可以得到![](images/Pasted%20image%2020230727075651.png)，给出了状态值函数与状态-⾏为值函数的关系
第三张图计算状态- ⾏为值函数为![](images/Pasted%20image%2020230727080136.png)
然后根据这个图可以得到
![](images/Pasted%20image%2020230727080228.png)


下面这个图是状态-⾏为值函数的计算
![状态-⾏为值函数的计算示意图](images/Pasted%20image%2020230727080333.png)
由图三得到![](images/Pasted%20image%2020230727080405.png)
然后得到：
![](images/Pasted%20image%2020230727080421.png)



计算状态值函数的⽬的是为了构建学习算法从数据中得到最优策略。 每个策略对应着⼀个状态值函数，最优策略⾃然对应着最优状态值函数。
最优状态值函数![](images/Pasted%20image%2020230727083009.png)为在所有策略中值最⼤的值函数
最优状态-⾏为值函数![](images/Pasted%20image%2020230727083026.png)为在所有策略中 最⼤的状态-⾏为值函数

到最优状态值函数和最优状态-⾏ 动值函数的⻉尔曼最优⽅程：
![](images/Pasted%20image%2020230727083247.png)


利⽤动态规划可以解决的问题需要满⾜ 两个条件：⼀是整个优化问题可以分解为多个⼦优化问题；⼆是⼦优化问 题的解可以被存储和重复利⽤。
由⻉尔曼最优⽅程可以知道，⻢尔科夫决策问题符合使⽤动态规划的两 个条件，因此可以利⽤动态规划解决⻢尔科夫决策过程的问题



状态s 处的值函数υ<sub>π </sub>（s），可以利⽤后继状态的值函数υ<sub>π</sub> （s′）来表 ⽰。可是有⼈会说，后继状态的值函数υ<sub>π</sub> （s′）也是未知的，那么怎么计 算当前状态的值函数，这不是⾃⼰抬⾃⼰吗？没错，这正是 bootstrapping算法（⾃举算法）！



![](images/Pasted%20image%2020230727163732.png)
每次迭代都需要对状态集进⾏⼀次遍历（扫描）以便 评估每个状态的值函数。
是当已知当前策略的值函数时，在每个状态采⽤贪 婪策略对当前策略进⾏改善，![](images/Pasted%20image%2020230727170545.png)
将策略评估算法和策略改善算法合起来便组成了策略迭代算法
![](images/Pasted%20image%2020230727170330.png)
策略迭代算法包括策略评估和策略改善两个步骤。在策略评估中，给 定策略，通过数值迭代算法不断计算该策略下每个状态的值函数，利⽤该 值函数和贪婪策略得到新的策略。如此循环下去，最终得到最优策略。这 是⼀个策略收敛的过程。
策略评估包括`两个循环`，第⼀个循环为 1000 次，保 证值函数收敛到该策略所对应的真实值函数。第⼆个循环为整个状态空间 的扫描，这样保证状态空间每⼀点的值函数都得到估计。然后基于当前的值函数得到贪 婪策略，将贪婪策略作为更新的策略，策略改善⽤公式表⽰为![](images/Pasted%20image%2020230727191126.png)


从策略迭代的伪代码我们看到，进⾏策略改善之前需要得到收敛 的值函数。值函数的收敛往往需要很多次迭代，现在的问题是进⾏策略改 善之前⼀定要等到策略值函数收敛吗？
我们的回答是不⼀定等到策略评估算法完全收敛。如果我们在评估⼀ 次之后就进⾏策略改善，则称为`值函数`迭代算法
![](images/Pasted%20image%2020230727175114.png)
需要注意的是在每次迭代过程中，需要对状态空间进⾏⼀次扫描，同 时在每个状态对动作空间进⾏扫描以便得到贪婪的策略
值迭代需要`三重循环`，第⼀重⼤循环⽤来保证值函数收敛，第⼆重中 间的循环⽤来遍历整个状态空间，对应着⼀次策略评估，第三重最⾥⾯的 循环则是遍历动作空间，⽤来选最优动作。





# 蒙特卡罗⽅法
第⼀次访问蒙特卡罗⽅法是指在计算状态 处的值函数时，只利⽤每次 试验中第⼀次访问到状态 时的返回值
![](images/Pasted%20image%2020230727200202.png)
每次访问蒙特卡罗⽅法是指在计算状态 处的值函数时，利⽤所有访问到状态时的回报返回值![](images/Pasted%20image%2020230727200231.png)

在动态规划⽅法中，为了保证值函数的收敛性，算法会逐个扫描状态 空间中的状态。⽆模型的⽅法充分评估策略值函数的前提是每个状态都能 被访问到，因此，在蒙特卡洛⽅法中必须采⽤⼀定的⽅法保证`每个状态都 能被访问到`，⽅法之⼀是探索性初始化。探索性初始化是指每个状态都有⼀定的⼏率作为初始状态

### 蒙特卡罗策略改善
蒙特卡罗⽅法利⽤经验平均估计策略值函数。估计出值函数后，对于 每个状态，它通过最⼤化动作值函数来进⾏策略的改善![](images/Pasted%20image%2020230727200756.png)


![](images/Pasted%20image%2020230727201043.png)


根据探索策略（⾏动策略）和评估的策略是否为同⼀个策略，蒙特卡 罗⽅法⼜分为on-policy和off-policy两种⽅法。
- 若⾏动策略和评估及改善的策略是同⼀个策略，我们称为on-policy， 可翻译为同策略。
同策略（on-policy）是指产⽣数据的策略与评估和要改善的策略是同 ⼀个策略。⽐如，要产⽣数据的策略和评估及要改善的策略都是 ε-soft策略
![](images/Pasted%20image%2020230727202920.png)

- 若⾏动策略和评估及改善的策略是不同的策略，我们称为off-policy， 可翻译为异策略。
例如⽤来评估和改善的策略$π$ 是贪婪 策略，⽤于产⽣数据的探索性策略$μ$ 为探索性策略，如  $ε-Soft$ 策略。
⽤于异策略的⽬标策略 和⾏动策略 并⾮任意选择的，⽽是必须满 ⾜⼀定的条件。这个条件是***覆盖性条件***，即⾏动策略 产⽣的⾏为覆盖或 包含⽬标策略 产⽣的⾏为。



此利⽤⾏动策略 所产⽣的累积函数返回值来评 估策略 时，需要在**累积函数**返回值前⾯**乘以**重要性权重![](images/Pasted%20image%2020230727210141.png)
因此重要性权重为
![](images/Pasted%20image%2020230727210213.png)
![](images/Pasted%20image%2020230727211950.png)
此处的软策略$μ$是   $ε-Soft$策略，需要改善的策略$π$  为贪婪策 略。



# 同策略Sarsa强化学习算法
代码表⽰同策略 中的⾏动策略和评估的策略都是ε-greedy策略
![](images/Pasted%20image%2020230727222959.png)


与Sarsa⽅法的不同之处在于， Qlearning⽅法是异策略的⽅法。即⾏动策略采⽤ε-greedy策略，⽽⽬标策略为贪婪策略
![](images/Pasted%20image%2020230727224042.png)















