- [[#计算方法|计算方法]]
- [[#状态-⾏为值函数|状态-⾏为值函数]]
- [[#状态-⾏为值函数#计算方法|计算方法]]
- [[#贝尔曼方程|贝尔曼方程]]
- [[#贝尔曼方程#状态值函数与状态-⾏为值函数的⻉尔曼⽅程|状态值函数与状态-⾏为值函数的⻉尔曼⽅程]]
- [[#确定性和随机性策略|确定性和随机性策略]]
- [[#动态规划|动态规划]]
- [[#动态规划#策略迭代|策略迭代]]
- [[#动态规划#价值迭代|价值迭代]]
- [[#基于价值的优化|基于价值的优化]]
- [[#基于策略的优化|基于策略的优化]]
- [[#价值迭代#二值化动作策略|二值化动作策略]]
- [[#价值迭代#对角高斯策略|对角高斯策略]]
- [[#价值迭代#基于策略的优化|基于策略的优化]]
- [[#价值迭代#策略梯度定理|策略梯度定理]]
- [[#神经网络的困境|神经网络的困境]]
- [[#蒙特卡罗⽅法|蒙特卡罗⽅法]]
- [[#价值迭代#蒙特卡罗策略改善|蒙特卡罗策略改善]]
- [[#价值迭代#增量蒙特卡罗|增量蒙特卡罗]]
- [[#时间差分学习|时间差分学习]]
- [[#动态规划、蒙特卡洛、TD|动态规划、蒙特卡洛、TD]]
- [[#TD(λ)|TD(λ)]]
- [[#同策略Sarsa强化学习算法|同策略Sarsa强化学习算法]]
- [[#Q-Learning：离线策略 TD 控制|Q-Learning：离线策略 TD 控制]]
- [[#DQN|DQN]]
- [[#价值迭代#Double DQN|Double DQN]]
- [[#计算智能体分数相对人类和 基准智能体分数的提升百分比|计算智能体分数相对人类和 基准智能体分数的提升百分比]]
- [[#价值迭代#Dueling DQN|Dueling DQN]]
- [[#价值迭代#优先经验回放|优先经验回放]]
- [[#价值迭代#其他改进内容：多步学习、噪声网络和值分布强化学习|其他改进内容：多步学习、噪声网络和值分布强化学习]]
- [[#策略梯度和价值优化和AC|策略梯度和价值优化和AC]]
- [[#策略梯度 Policy Gradient|策略梯度 Policy Gradient]]
- [[#策略梯度 Policy Gradient#策略搜索方法|策略搜索方法]]
- [[#TRPO|TRPO]]
- [[#基于确定性策略搜索的强化学习⽅法|基于确定性策略搜索的强化学习⽅法]]
- [[#GPS|GPS]]
- [[#逆向强化学习|逆向强化学习]]

# 23-10-21
值函数优化的目的是为了得到当前状态 $s$ 的最大的累计奖励值,
动作值函数优化问题的目的是在当前状态 $s$ 和当前动作 $a$ 的情况下求解出最大的累计奖励值，也就是说穷尽所有的策略当中的所有动作，然后找到一个最优的动作使
得 $Q$ 函数最大 
这二者具有很强的相关性，假设知道一个当前最好的动作（ Action ）$a^*$ , 那么显然$Q(s,a^*) \gt V^*(s)$ 
因为 V(s) 相当于最好策略下所有动作的均值函数
因此， V(s） 有 一 个 最优策略的上界，即为 $Q^*$函数
于是对于值函数的最优解策略问题求解转化为求出对应的 $Q^*$函数的动作，
![](images/Pasted%20image%2020231021094057.png)






#### 状态值函数
当智能体采⽤策略$π$时，累积回报服从⼀个分布，累积回报在状态s处 的期望值定义为状态-值函数
![](images/Pasted%20image%2020230727073505.png)

##### 计算方法
![](images/Pasted%20image%2020230811224844.png)
看一下这样的马尔可夫过程，我们求$t_2$的状态价值函数。
求出每种可能的轨迹
![](images/Pasted%20image%2020230811225127.png)
然后求期望
![](images/Pasted%20image%2020230811225141.png)




#### 状态-⾏为值函数
![](images/Pasted%20image%2020230727074110.png)


上面分别给出了状态值函数和状态-⾏为值函数的定 义计算式，但在实际真正计算和编程的时候并不会按照定义式编程。

$Q_π(s, a)$是基于策略 π 来估计的，因为对值的估计是策略 π 所决定的轨迹上的期望。也就是说，如果策略π改变了，$Q_π(s, a)$也会相应地跟着改变。

##### 计算方法
有两种简单方法来计算价值函数$v_π(s)$ 和动作价值函数 $q_π(s, a)$：第一种方法是**穷举法**，首先计算出从一个状态开始的所有可能轨迹的概率，然后 用公式计算出这个状态的 $V_π(s)$ 和 $Q_π(s, a)$。每个状态都用穷举法来单独计算。
然而实际中，可能的轨迹数量是非常大的，甚至是无穷个的。因此除了使用所有可能的轨迹，第 二种方法是使用之前介绍的**蒙特卡罗**方法通过采样大量的轨迹来估计 V π (s)。这两种方法都非常 简单，但都有各自的缺点。
而实际上，估计价值函数的公式可以根据马尔可夫性质做进一步的简 化，即下一小节要介绍的**贝尔曼方程****。


### 贝尔曼方程
贝尔曼方程（Bellman Equation），也称为贝尔曼期望方程，用于计算给定策略 π 时价值函 数在策略指引下所采轨迹上的期望。
我们可以利用递归关系得出在线状态价值函数的贝尔曼方程和在线动作价值函数的贝尔曼方程，如下面两种
下面的推导是基于最大长度为 T 的有限 MDP，然而，这些等式对无穷长度 MDP 也成立


#### 状态值函数与状态-⾏为值函数的⻉尔曼⽅程
![](images/Pasted%20image%2020230727074720.png)
![](images/Pasted%20image%2020230727074739.png)




可以参考状态值函数的计算示意图来看上面的贝尔曼方程：
空⼼圆圈表⽰状态，实⼼圆圈表⽰状态-⾏为对
![状态值函数的计算示意图](images/Pasted%20image%2020230727074814.png)
可以得到![](images/Pasted%20image%2020230727075651.png)，给出了状态值函数与状态-⾏为值函数的关系
第三张图计算状态- ⾏为值函数为![](images/Pasted%20image%2020230727080136.png)
然后根据这个图可以得到
![](images/Pasted%20image%2020230727080228.png)


下面这个图是状态-⾏为值函数的计算
![状态-⾏为值函数的计算示意图](images/Pasted%20image%2020230727080333.png)
由图三得到![](images/Pasted%20image%2020230727080405.png)
然后得到：
![](images/Pasted%20image%2020230727080421.png)



计算状态值函数的⽬的是为了构建学习算法从数据中得到最优策略。 每个策略对应着⼀个状态值函数，最优策略⾃然对应着最优状态值函数。
最优状态值函数![](images/Pasted%20image%2020230727083009.png)为在所有策略中值最⼤的值函数
最优状态-⾏为值函数![](images/Pasted%20image%2020230727083026.png)为在所有策略中 最⼤的状态-⾏为值函数

到最优状态值函数和最优状态-⾏ 动值函数的⻉尔曼最优⽅程：
![](images/Pasted%20image%2020230727083247.png)


### 确定性和随机性策略
基于策略的方法直接对策略进行优化，通过对策略迭代更新，实现累积奖励最大化
一个动作从概率分布中采样的策略称为随机性策略分布（Stochastic Policy Distribution），其动作为![](images/Pasted%20image%2020230811232725.png)
确定性策略 π(s) 也意味着给定一个状态，将得到唯一的动作，如下：![](images/Pasted%20image%2020230811232750.png)

确定性策略不再是从状态和动作到条件概率分布（Conditional Probability Distribution） 的映射，而是一个从状态到动作的直接映射。这点不同将导致随后介绍的策略梯度方法中的一些 推导过程的不同


# 动态规划

利⽤动态规划可以解决的问题需要满⾜ 两个条件：⼀是整个优化问题可以分解为多个⼦优化问题；⼆是⼦优化问 题的解可以被存储和重复利⽤。
由⻉尔曼最优⽅程可以知道，⻢尔科夫决策问题符合使⽤动态规划的两 个条件，因此可以利⽤动态规划解决⻢尔科夫决策过程的问题



状态s 处的值函数υ<sub>π </sub>（s），可以利⽤后继状态的值函数υ<sub>π</sub> （s′）来表 ⽰。可是有⼈会说，后继状态的值函数υ<sub>π</sub> （s′）也是未知的，那么怎么计 算当前状态的值函数，这不是⾃⼰抬⾃⼰吗？没错，这正是 bootstrapping算法（⾃举算法）！



![](images/Pasted%20image%2020230727163732.png)
每次迭代都需要对状态集进⾏⼀次遍历（扫描）以便 评估每个状态的值函数。
是当已知当前策略的值函数时，在每个状态采⽤贪 婪策略对当前策略进⾏改善，![](images/Pasted%20image%2020230727170545.png)
将策略评估算法和策略改善算法合起来便组成了策略迭代算法
![](images/Pasted%20image%2020230727170330.png)
策略迭代算法包括策略评估和策略改善两个步骤。在策略评估中，给 定策略，通过数值迭代算法不断计算该策略下每个状态的值函数，利⽤该 值函数和贪婪策略得到新的策略。如此循环下去，最终得到最优策略。这 是⼀个策略收敛的过程。
策略评估包括`两个循环`，第⼀个循环为 1000 次，保 证值函数收敛到该策略所对应的真实值函数。第⼆个循环为整个状态空间 的扫描，这样保证状态空间每⼀点的值函数都得到估计。然后基于当前的值函数得到贪 婪策略，将贪婪策略作为更新的策略，策略改善⽤公式表⽰为![](images/Pasted%20image%2020230727191126.png)


从策略迭代的伪代码我们看到，进⾏策略改善之前需要得到收敛 的值函数。值函数的收敛往往需要很多次迭代，现在的问题是进⾏策略改 善之前⼀定要等到策略值函数收敛吗？
我们的回答是不⼀定等到策略评估算法完全收敛。如果我们在评估⼀ 次之后就进⾏策略改善，则称为`值函数`迭代算法
![](images/Pasted%20image%2020230727175114.png)
需要注意的是在每次迭代过程中，需要对状态空间进⾏⼀次扫描，同 时在每个状态对动作空间进⾏扫描以便得到贪婪的策略
值迭代需要`三重循环`，第⼀重⼤循环⽤来保证值函数收敛，第⼆重中 间的循环⽤来遍历整个状态空间，对应着⼀次策略评估，第三重最⾥⾯的 循环则是遍历动作空间，⽤来选最优动作。


## 策略迭代
策略迭代（Policy Iteration）的目的在于直接操控策略。从任意策略 π 开始，我们可以通过**递 归**地调用贝尔曼方程来评估策略
![](images/Pasted%20image%2020230811234259.png)
一个获得更好策略的自然想法是根据 vπ 来贪心地执行动作
![](images/Pasted%20image%2020230811234318.png)
这样的提升可以由以下证明
![](images/Pasted%20image%2020230811234336.png)
接连地使用以上的策略评估和贪心提升，直到 π = π ′ 形成策略迭代。一般地，策略迭代的 过程可以总结如下：给定任意一个策略 $π_t$，对于每一次迭代 $t$ 中的每一个状态 s，我们首先评估 $v_{π_t} (s)$，然后找到一个更好的策略 $π_{t+1}$。我们把前一个阶段称为策略评估（Policy Evaluation），把 后一个阶段称为策略提升
策略迭代 会收敛到 $v_∗$


## 价值迭代
价值迭代（Value Iteration）的理论基础是最优性原则（Principle of Optimality）。这个原则告 诉我们当且仅当 π 取得了可以到达的任何后续状态上的最优价值时，π 是一个状态上的最优策 略。因此如果我们知道子问题 $v_∗(s^′ )$ 的解，就可以通过一步完全回溯（One-Step Full Backup）找 到任意一个初始状态 s 的解：
![](images/Pasted%20image%2020230811234832.png)
价值迭代会收敛到最优值 $v_∗$
何时停止价值迭代算法不是显而易见的。文献 (Williams et al., 1993) 在理论上给出了一个充 分的（Sufficient）停止标准：如果两个连续价值函数的最大差异小于 ϵ, 那么在任意状态下，贪心 策略的价值与最优策略的价值函数的差值不会超过![](images/Pasted%20image%2020230811235502.png)


# 基于价值的优化
基于价值的优化是强化学习中一类算法的范畴。这些算法主要关注于学习一个价值函数，它可以衡量在给定的状态下采取不同行动的预期回报
当状态空间或动作空间非常大时，可能需要使用神经网络或其他函数逼近方法来估计价值函数，**神经网络**因其 很好的可扩展性和对多样函数的综合能力而成为深度强化学习方法中最实用的拟合方法。神经网 络是一个可微分方法，因而可以基于梯度进行优化
对 于**无模型**方法，拟合器的参数 w 可以用蒙特卡罗（Monte-Carlo，MC）或时间差分（Temporal Difference，TD）学习来更新，可以对批量样本进行参数更新而非像基于表格的方法一样逐个 更新。这使得处理大规模问题时有较高的计算效率。
对**基于模型**的方法，参数可以用动态规划 （Dynamic Programming，DP）来更新
在强化学习中使用价值函数拟合对表征方式也有一些实际要求，而如果没有适当地考虑到这 些实际要求，将可能导致发散的情况的发生
总体来说，使用 策略梯度的基于策略的方法相比基于价值的方法有更好的收敛性保证。
优化目标被设置为估计函数 $V_π (s; w)$（或 $Q_π (s, a; w))$ 和真实价值 函数 $v_π(s)$（或 $q_π(s, a))$间的均方误差（Mean-Squared Error，MSE）
![](images/Pasted%20image%2020230812231336.png)
或者是
![](images/Pasted%20image%2020230812232333.png)
因此，用随机梯度下降（Stochastic Gradient Descent）法所得到的梯度为
![](images/Pasted%20image%2020230812232349.png)
和
![](images/Pasted%20image%2020230812232400.png)

对 MC 估计，目标值是用采样的回报 Gt 估计的。因此，价值函数参数的更新梯度为
![](images/Pasted%20image%2020230812232439.png)
或者
![](images/Pasted%20image%2020230812232451.png)

对TD(0)，根据式(2.84)表示的贝尔曼最优方程，目标值是时间差分的目标函数Rt+γVπ(St+1; wt)， 因此：
![](images/Pasted%20image%2020230812232508.png)
或者
![](images/Pasted%20image%2020230812232519.png)

对 TD(λ)，目标值是 λ-回报即 ${G^λ}_t$ ，因此更新规则是
![](images/Pasted%20image%2020230812233441.png)
或者
![](images/Pasted%20image%2020230812233458.png)


# 基于策略的优化
强化学习中的策略可以被分为确定性（Deterministic）和 随机性（Stochastic）策略。在深度强化学习中，我们使用神经网络来表示这两类策略，称为**参数 化策略**（Parameterized Policies）。具体来说，这里的参数化指抽象的策略用神经网络（包括单层 感知机）参数来，而非其他参量来表示。使用神经网络参数 θ，确定性和随机性策略可以分别写 作 $A_t = µ_θ(St)$ 和 $A_t ∼ π_θ(·|St)$
在深度强化学习领域，有一些常见的**具体分布**用来**表示**随机性策略中的**动作分布**：**伯努利 分布**（Bernoulli Distribution），**类别分布**（Categorical Distribution）和**对角高斯分布**（Diagonal Gaussian Distribution）。伯努利和类别分布可以用于**离散动作空间**，如二值的（Binary）或多类别 的（Multi-Category），而<u><b>对角高斯分布可以用于连续动作空间</b></u>。
### 二值化动作策略
一个以 θ 为参数的单变量 $x ∈ {\{0, 1\}}$ 的伯努利分布为 $P(s; θ) = θ^x(1 − θ)^{(1−x)}$。因而它可以 被用于表示二值化的动作，可以是单维，也可以是多维（对一个矢量中含多个变量的情况应用）， 它可以用作二值化动作策略（Binary-Action Policy）
类别型策略它将策略视为一个分类器（Classifier），以状态为条件（Conditioned on A State）而输 出在有限动作空间中每个动作的概率，所有概率和为 1，因此， 当将类别型策略参数化时，最后输出层（Output Layer）常用 Softmax 激活函数

### 对角高斯策略
对角高斯策略（Diagonal Gaussian Policy）输出一个对角高斯分布的均值和方差用于连续动 作空间。一个普通的多变量高斯分布包括一个均值矢量 $µ$ 和一个协方差（Covariance）矩阵$Σ$
而**对角高斯分布**是其特殊情况，即协方差矩阵只有对角元非零，因此我们可以用一个矢量 $σ$ 来表示它
当使用对角高斯分布来表示概率性动作时，它移除了不同动作维度间的协相关性。一个策 略被参数化时，如下所示的再参数化（Reparametrization）技巧可以被用来从均值和方差矢量表示的高斯分布中采样，同时保持操作的可微性。
再参数化技巧：从对角高斯分布中采样动作$a ∼ N (µ_θ,σ_θ)$，该分布的均值和方差矢量为 $µ_θ$ 和 $σ_θ$（参数化的），而这可以通过从正态分布中采样一个隐藏矢量 $z ∼ N (0, 1)$ 来得到动作 $a = µ_θ + σ_θ ⊙ z$
其中 ⊙ 是两个相同形状矢量的逐个元素乘积

### 基于策略的优化
基于策略的优化（Policy-Based Optimization）方法在强化学习情景下直接优化智能体的策略 而不估计或学习动作价值函数。采样得到的奖励值通常用于改进动作选择的优化过程，而优化过 程可以使用基于梯度或无梯度（Gradient-Free）的方法。其中，基于梯度的方法通常采用策略梯度（Policy Gradient），它在某种程度上代表了连续动作强化学习最受欢迎的一类算法，受益于对 高维情况的可扩展性。典型的基于梯度优化方法包括 REINFORCE 等。
无梯度方法对策略搜索中 相对简单的情况通常有更快的学习过程，无须有复杂计算的求导过程。典型的无梯度类方法包括 交叉熵（Cross-Entropy，CE）方法等。

我们在强化学习中智能体的目标是从期望或估计的角度去最大化从一个状态开始的累 计折扣奖励（Cumulative Discounted Reward），可以将其表示
![](images/Pasted%20image%2020230813000724.png)
![](images/Pasted%20image%2020230813000731.png)是有限步（适用于多数情形）的折扣期望奖励，而 $τ$ 是采样的轨迹。
基于梯度的优化方法是使用在期望回报（总的奖励）上的梯度估计来进行梯度下降（或上 升），以改进策略，而这个期望回报是从采样轨迹中得到的。这里我们把关于策略参数的梯度叫 作策略梯度（Policy Gradient），具体表达式如下
![](images/Pasted%20image%2020230813000810.png)
其中 $θ$ 表示策略参数，而 $α$ 是学习率。

### 策略梯度定理
![](images/Pasted%20image%2020230813000945.png)






# 神经网络的困境
绝大多数监督学习方法建立在这样一个假设之上，即训练数 据是从一个稳定的独立同分布 (Schmidhuber, 2015) 中采样得到的。然而，强化学习中的训练数据 通常包括高度相关的样本，它们是在智能体和环境交互中顺序得到的，而这违反了监督学习中的 **独立性条件**

# 蒙特卡罗⽅法
第⼀次访问蒙特卡罗⽅法是指在计算状态 处的值函数时，只利⽤每次 试验中第⼀次访问到状态 时的返回值
![](images/Pasted%20image%2020230727200202.png)
每次访问蒙特卡罗⽅法是指在计算状态 处的值函数时，利⽤所有访问到状态时的回报返回值![](images/Pasted%20image%2020230727200231.png)

在动态规划⽅法中，为了保证值函数的收敛性，算法会逐个扫描状态 空间中的状态。⽆模型的⽅法充分评估策略值函数的前提是每个状态都能 被访问到，因此，在蒙特卡洛⽅法中必须采⽤⼀定的⽅法保证`每个状态都 能被访问到`，⽅法之⼀是探索性初始化。探索性初始化是指每个状态都有⼀定的⼏率作为初始状态

### 蒙特卡罗策略改善
蒙特卡罗⽅法利⽤经验平均估计策略值函数。估计出值函数后，对于 每个状态，它通过最⼤化动作值函数来进⾏策略的改善![](images/Pasted%20image%2020230727200756.png)
对于每一次策略提升，我们都需要根据$q_{π_t}$来构造$π_{t+1}$。这里展示策略提升是怎么实现的：![](images/Pasted%20image%2020230812102357.png)
上面的式子证明了 $π_{t+1}$ 不会比 $π_t$ 差，而我们会在迭代策略提升后最终找到最优策略

### 增量蒙特卡罗
![](images/Pasted%20image%2020230812102538.png)
这个形式可以让我们在计算回报的时候更加容易操作。它的通用形式是：
**新估计值 ← 旧估计值 + 步伐大小 · (目标值 − 旧估计值)****



![](images/Pasted%20image%2020230727201043.png)


根据探索策略（⾏动策略）和评估的策略是否为同⼀个策略，蒙特卡 罗⽅法⼜分为on-policy和off-policy两种⽅法。
- 若⾏动策略和评估及改善的策略是同⼀个策略，我们称为on-policy， 可翻译为同策略。
同策略（on-policy）是指产⽣数据的策略与评估和要改善的策略是同 ⼀个策略。⽐如，要产⽣数据的策略和评估及要改善的策略都是 ε-soft策略
![](images/Pasted%20image%2020230727202920.png)

- 若⾏动策略和评估及改善的策略是不同的策略，我们称为off-policy， 可翻译为异策略。
例如⽤来评估和改善的策略$π$ 是贪婪 策略，⽤于产⽣数据的探索性策略$μ$ 为探索性策略，如  $ε-Soft$ 策略。
⽤于异策略的⽬标策略 和⾏动策略 并⾮任意选择的，⽽是必须满 ⾜⼀定的条件。这个条件是***覆盖性条件***，即⾏动策略 产⽣的⾏为覆盖或 包含⽬标策略 产⽣的⾏为。



此利⽤⾏动策略 所产⽣的累积函数返回值来评 估策略 时，需要在**累积函数**返回值前⾯**乘以**重要性权重![](images/Pasted%20image%2020230727210141.png)
因此重要性权重为
![](images/Pasted%20image%2020230727210213.png)
![](images/Pasted%20image%2020230727211950.png)
此处的软策略$μ$是   $ε-Soft$策略，需要改善的策略$π$  为贪婪策 略。

# 时间差分学习
时间差分在估算的过程中使用了自举（Bootstrapping）， 但是和蒙特卡罗一样，它不需要在学习过程中了解环境的全部信息
它使用自举法的原因是它需要从观察到的回报和对下个状态的估值中来构造它的目 标。具体来说，最基本的时间差分使用以下的更新方式：
![](images/Pasted%20image%2020230812102708.png)
这个方法也被叫作 TD(0), 或者是**单步 TD**。也可以通过将目标值改为在 N 步未来中的折扣 回报和 N 步过后的估计状态价值（Estimated State Value）来实现 **N 步 TD**


# 动态规划、蒙特卡洛、TD
![](images/Pasted%20image%2020230812111532.png)
这是同一个式子
公式 (2.58) 是蒙特卡罗方法的状态价值估计方式，公式 (2.60) 是动态规划的。它们都不是真 正的状态值而是估计值。
时间差分则把蒙特卡罗的采样过程和动态规划的自举法结合了起来

蒙特卡罗法直接估算到一个回合结束累计的回报。这也正是状态值的定义， 它是没有偏差的。而时间差分法会有一定的偏差，因为它的目标值是由自举法估计得到的，如 $R_{t+1} + γv_π(S_{t+1})$。
方差：  蒙特卡罗法，所以在不同回合中积累到最后的回报会有较大的方 差由于不同回合的经过和结果都不同。时间差分法通过关注局部估计的目标值来解决这个问题， 只依赖当前的奖励和下一个状态或动作价值的估计。自然地，时间差分法方差更小。


# TD(λ)
λ-回报是之后 n 步中的估计回报值。λ-回报是 n 个已经折扣化的回报和一个在最后一步状态 下的估计值相加得到的。我们可以把它写作：
![](images/Pasted%20image%2020230812123913.png)

TD 误差 $δ_t$ 可以被定义为
![](images/Pasted%20image%2020230812124222.png)





# 同策略Sarsa强化学习算法
首先在一个状态（S）下， 选择了一个动作（A），同时也观察到了回报（R），然后我们就到了另外一个状态（S）下，需要 选择一个新的动作（A）。这样的过程让我们可以做一个简单的更新步骤。对于每一个转移，状态 价值都得到更新，**更新后的状态价值会影响决定动作的策略****，即在线策略法。在线策略法一般用 来描述这样一类算法，它们的**更新策略和行动策略**（Behavior Policy）同样。
![](images/Pasted%20image%2020230812163649.png)
# Q-Learning：离线策略 TD 控制
Q-Learning 是一种离线策略方法，与 Saras 很类似，在深度学习应用中有很重要的作用，如 深度 Q 网络（Deep Q-Networks）。Q-Learning 和 Sarsa 主要的区别是，它的 目标值现在不再依赖于所使用的策略，而只依赖于状态-动作价值函数。
![](images/Pasted%20image%2020230812163617.png)








代码表⽰同策略 中的⾏动策略和评估的策略都是ε-greedy策略
![](images/Pasted%20image%2020230727222959.png)


与Sarsa⽅法的不同之处在于， Qlearning⽅法是异策略的⽅法。即⾏动策略采⽤ε-greedy策略，⽽⽬标策略为贪婪策略
![](images/Pasted%20image%2020230727224042.png)

![](images/Pasted%20image%2020230727225136.png)

# DQN
第一个关键技术被称为回放缓存（Replay Buffer）。这是一种被称为经验重演的生物学启发机 制。在每个时间步 t 中，DQN 先将智能体获得 的经验 (St, At, Rt, St+1) 存入回放缓存中，然后从该缓存中均匀采样小批量样本用于 Q-Learning 更新。回放缓存相较于拟合 Q 迭代有几个优势。
- 首先，它可以重用每个时间步的经验来学习 Q 函数，这样可以提高数据使用效率。
- 其次，如果像拟合 Q 迭代那样没有回放缓存，那么一个批次 中的样本将会是连续采集的，即样本高度相关。这样会增加更新的方差。
- 最后，经验回放防止用 于训练的样本只来自上一个策略，这样能平滑学习过程并减少参数的震荡或发散。
在实践中，为 了节省内存，我们往往只将最后 N 个经验存入回放缓存（FIFO 缓存）。
第二个关键技术是目标网络。它作为一个独立的网络，用来代替所需的 Q 网络来生成 Q-Learning 的目标，进一步提高神经网络的稳定性。此外，目标网络**每 C 步****将通过直接**复制**（硬 更新）或者指数衰减平均（软更新）的方式与主 Q 网络同步。目标网络通过使用旧参数生成 Q-Learning 目标，使目标值的产生不受最新参数的影响，从而大大***减少发散和震荡***的情况。例如， 在动作 (St, At) 上的更新使得 Q 值增加，此时 St 和 St+1 的相似性可能会导致所有动作 a 的 Q(St+1, a) 值增加，从而使得由 Q 网络产生的训练目标值被过估计。但是如果使用目标网络产生 训练目标，就能避免过估计的问题

### Double DQN
Double DQN 是对 DQN 在减少过拟合方面的改进
Q-Learning 目标 $R_t + γ max_a Q(S_{t+1}, a)$ 包含一个最大化算子 max 的操作。而 Q 又由于环境、非稳态、函数近似 或者其他原因，可能带有噪声
标准 DQN 的学习目标可以被重写为如下式子
$R_t + γ\hat Q(S_{t+1}, \underset{a}{\arg\max} \hat Q(S_{t+1}, a; \hat θ);  \hat θ)$
在式子中可以注意到一个问题：$\hatθ$ 既用于估计 Q 值，又用于对估计过程中的下一个动作 a 进 行选择

Double DQN 的核心思想是在这两个阶段使用两个不同的网络，以去除选择和评价中 噪声的相关性。因此，需要一个额外的网络完成这项工作，而 DQN 结构中的 Q 网络则是一个很 自然能想到的选择。
因此，Double DQN 中使用的 Q 学习目标是
$R_t + γ\hat Q(S_{t+1}, \underset{a}{\arg\max} \hat Q(S_{t+1}, a;  θ);  \hat θ)$

# 计算智能体分数相对人类和 基准智能体分数的提升百分比
![](images/Pasted%20image%2020230814191345.png)

### Dueling DQN
对于某些状态来说，不同的动作与预期值无关，因此我们不需要学习各个动作对该状态的影 响。例如，假想我们在山上看日出，美丽的景色令人陶醉，这是一个很高的奖励。此时，你即使 在这里继续做不同的动作也不会对 Q 值产生影响。因此，将动作无关的状态值与 Q 值进行解耦， 可以获得更加鲁棒的学习效果。

Dueling DQN 提出了一种新的网络结构来实现这一思想。更准确地说，Q 值可以被分为**状态值**和**动作优势**这两部分
$Q_π (s, a) = V_π (s) + A_π (s, a)$
然后，Dueling DQN 通过如下方法将这两部分的表示分开：
$Q(s, a; θ, θ_v, θ_a) = V (s; θ, θ_v) + (A(s, a; θ, θ_a) − \underset{a′}{max}  A(s, a^′ ; θ, θ_a))$


### 优先经验回放
优先经验回放 （Prioritized Experience Replay，PER）是一种将经验进行优先排序的技术。通过该技术可以使重要 的状态转移经验被更加频繁地回放。
PER 的核心思想是通过 TD 误差 $δ$ 来考虑 不同状态转移数据的重要性。


### 其他改进内容：多步学习、噪声网络和值分布强化学习
多步学习（Multi-Step Learning）使用 n 步回报将使估计更加准确，也被证明可 以通过适当调整 n 值来加快学习速度。然而，在离线策略学习过程中，目标策略和行为策略在多个步骤中的行为选择可能并不匹配。Rainbow 直接使用了来自给定状态 St 的截断的 n 步回报${R_t}^k$，其中${R_t}^k$ 由以下公式定义
![](images/Pasted%20image%2020230814194238.png)

Q-Learning 多步学习变体的**目标**的定义就变成：
![](images/Pasted%20image%2020230814194439.png)

噪声网络是另一种 ϵ 贪心的探索算法，对于像《蒙 特祖玛的复仇》这样需要大量探索的游戏十分有效。我们使用一个额外的噪声流将噪声加入线性 层 y = (W x + b) 中
![](images/Pasted%20image%2020230814194525.png)
⊙ 表示元素间的乘积， $W_{noisy}$ 和 $b_{noisy}$ 都是可训练的参数，而 $ϵ_w$ 和 $ϵ_b$ 是将退火到 0 的随机 的标量。实验表明，噪声网络相比于许多基线算法，使得众多雅达利游戏的得分有了大幅提升。






# 策略梯度和价值优化和AC
策略优化算法往往分为两大类：（1）基于价值的优化（Value-Based Optimization）方法，如 Q-Learning、DQN 等，通过优化动作价值函数（Action-Value Function）来获得对 动作选择的偏好；（2）基于策略的优化（Policy-Based Optimization）方法，如 REINFORCE、交 叉熵算法等，通过根据采样的奖励值来直接优化策略。这两类的结合被人们 (Kalashnikov et al., 2018; Peters et al., 2008; Sutton et al., 2000) 发现是一种更加有效的方式，而这构成了一种在无模型 （Model-Free）强化学习中应用最广的结构，称为 Actor-Critic。Actor-Critic 方法通过对价值函数 的优化来引导策略改进

# 策略梯度 Policy Gradient
使用参数化策略 $π_θ$ 和参数化 Q 值函数 ${Q_π}^w(St, At)$ 的 MDP 概率图模型。一般通过在强化学习术语中 被称为策略梯度（Policy Gradient）的方法改进参数化策略。
![](images/Pasted%20image%2020230812172336.png)
也可以使用参数化策略 $π_θ$ 和参数化价值函数 ${V_π}^w(St)$ 的 MDP 概率图模型，它们的参数化过程分别使用了参数 θ 和 w。
![](images/Pasted%20image%2020230812172440.png)




## 策略搜索方法
正是因为直接策略搜索⽅法⽐值函数⽅法拥有 更多的优点，我们才有理由或动机去研究和学习并改进直接策略搜索⽅ 法


策略梯度公式
![](images/Pasted%20image%2020230727233440.png)

![](images/Pasted%20image%2020230727233506.png)
![](images/Pasted%20image%2020230727233535.png)

因此，从直观上理解策略梯度时，我们发现策略梯度会增加⾼回报路 径的概率，减⼩低回报路径的概率。⾼回报区域的轨迹概率 被增⼤，低回报区域的轨迹概率被减⼩。




# TRPO
合适的步⻓是指当策略更新后，回报函数的值不能更差。那么如何选 择步⻓？或者说，如何找到新的策略使新的回报函数的值单调增⻓，或单 调不减？这就是TRPO要解决的问题。
⼀个⾃ 然的想法是能否将新的策略所对应的回报函数分解成旧的策略所对应的回 报函数加其他项。这样，只要新的策略所对应的其他项⼤于等于零，那么 新的策略就能保证回报函数单调不减。这样的等式其实是存在的，它是 2002年由Sham Kakade提出来的。TRPO的起点便是这样⼀个等式：
![](images/Pasted%20image%2020230728200132.png)是强化学习的回报函数
![](images/Pasted%20image%2020230728193932.png)
![](images/Pasted%20image%2020230728194301.png)
是优势函数


这个优势到底是指和谁相⽐的优势？
![](images/Pasted%20image%2020230728195039.png)
值函数$V_π(S)$ 是 该状态下所有动作值函数关于动作概率的平均值
动作值函数 $Q_π(s,a)$是单个动作所对应的值函数， $Q_π(s,a)-V_π(S)$能评价当前动作值函数相 对于平均值的⼤⼩。
这⾥的优势指的是动作值函数相⽐于当前状态 的值函数的优势。如果优势函数⼤于零，则说明该动作⽐平均动作好，如 果优势函数⼩于零，则说明当前动作不如平均动作好。
TRPO中最重要的等式：
![](images/Pasted%20image%2020230729104934.png)

优势函数的期望可以写成下⾯这样：
![](images/Pasted%20image%2020230729104956.png)


# 基于确定性策略搜索的强化学习⽅法

$π_θ​(a∣s)=P[a∣s;θ]$ 这个是随机策略的公式，其含义是，在状态为s时，动作符合参数为$θ$ 的概率分布
$a=μ_θ​(s)$这个是确定性策略的公式，和随机策略不同，相同的策略（即$θ$  相同时），在状态为s时，动作是 唯⼀确定的
确定性策略⽅法的效率⽐随机策略的效率⾼⼗倍，这也是 确定性策略⽅法最主要的优点
当初试状态已知时，⽤确定性策略 所产⽣的轨迹永远都是固定的，智能体⽆法探索其他轨迹或访问其他状 态，从这个层⾯来说，智能体⽆法学习。我们知道，强化学习算法是通过 智能体与环境交互来学习的。这⾥的交互是指探索性交互，即智能体会尝 试很多动作，然后在这些动作中学到好的动作

***既然确定性策略⽆法探索环境，那么它如何学习呢？***
答案是利⽤异策略学习⽅法（off-policy）。异策略是指⾏动策略和评 估策略不是同⼀个策略。我们此处所说的⾏动策略是随机策略，以保证充 ⾜的探索；评估策略是确定性策略，。整个确定性策略的学 习框架采⽤AC（Actor-Critic Algorithm）的⽅法。
AC 算法包括两个同等地位的元素，⼀个元素是 Actor 即⾏动策略，另 ⼀个元素是Critic策略即评估，这⾥是指利⽤函数逼近⽅法估计值函数

随机策略的梯度是![](images/Pasted%20image%2020230729114803.png)
如图 所⽰，其中 Actor ⽅法⽤来调整 值；Critic ⽅法逼近值函数![](images/Pasted%20image%2020230729114833.png)
其中 $w$ 为待逼近的参数，可⽤TD学习的⽅法评估 值函数。
![](images/Pasted%20image%2020230729114900.png)

为了得到确定性策略AC的⽅法，我们⾸先给出确定性策略梯度如下
![](images/Pasted%20image%2020230729115020.png)
异策略确定性策略梯度为
![](images/Pasted%20image%2020230729115401.png)
有了上面这个式子，我们便可以得到确定性策略异策略AC算法的更新过 程了，如下
![](images/Pasted%20image%2020230729115442.png)


# GPS

TRPO⽅法和DDPG⽅法的局限性：这两种⽅法都是⽆模型的强化学习算法。 ⽆模型的强化学习算法有很多优点，⽐如可以不⽤对外界建模，尤其当外 界环境⾮常复杂，很难建模或根本⽆法建模时，该⽅法是唯⼀的⽅法。但 是，⽆模型的强化学习算法也有其固有的缺点：因为没有模型，所以⽆模 型的强化学习算法只能通过不断尝试来探索环境，这些算法只能处理参数 最多为数百个的策略⽹络，对于更⼤的策略⽹络，这些⽅法学习效率不 ⾼。原因很简单：当随机初始化有数千个或上万个参数的⽹络时，随机尝 试根本⽆法产⽣好的数据。因此，对于复杂的任务，随机探索⼏乎找不到 成功的解或者好的解，⽽没有成功的解或好的解，智能体就⽆法从中学到 好的动作，也就⽆法形成良性循环。综上，⽆模型的强化学习算法最⼤的 缺点是数据效率低


解决⽆模型随机探索问题的⽅法是利⽤模型探索。有了模型，我们可以做哪些事呢？ 第⼀，利⽤模型和基于模型的优化算法，可以得到回报⾼的数据，也 就是好数据。有了好数据，就可以稳定训练策略⽹络。 第⼆，有了模型，我们可以充分地利⽤⽰例（demonstration）学习。 ⼈的⽰例可以当成模型的初值。
以GPS⽅法将策略搜索⽅法分成控制相和监督相。其中控制相通过 轨迹最优、传统控制器或随机最优等⽅法控制产⽣好的数据；监督相利⽤ 从控制相产⽣的好数据进⾏监督学习。

强化学习是智能体通过与环境交互产⽣数据，从交互数据中学习；GPS是策略⽹络通过与控制相的交互产⽣数据，从控制 相产⽣的数据中学习。换句话说，在⽆模型的情况下，智能体（策略⽹ 络）是通过试错向环境学习；⽽GPS的⽅法是向逐渐迭代优化的控制器学 习。
![](images/Pasted%20image%2020230729123314.png)




# 逆向强化学习

我们介绍了强化学习算法，包括如何策略评估，如 何策略迭代。但是有⼀个很关键的问题还没有提及，那就是回报函数。因 为在利⽤强化学习算法时，我们都假设回报函数是⼈为给定的。那么回报 函数如何确定呢？这其实有很强的主观性和经验性。由于回报函数的不同 会导致最优策略的不同，因此它⾮常重要。但是当任务很复杂时，我们很 难⼈为给出回报函数。⽐如在⾃动驾驶中，回报函数可能是信号灯、前⾯ 的汽⻋，周边环境等各种因素的函数，我们很难⼈为判断给定这个回报函 数。⽽且，在执⾏不同的任务时，回报函数也不同。所以，回报函数的设 定是推⼴强化学习算法应⽤的⼀⼤障碍。逆向强化学习就是为了解决回报 函数的问题⽽提出来的——只有解决该问题，强化学习算法才能得到⼤规 模应⽤。

















