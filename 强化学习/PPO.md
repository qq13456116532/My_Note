Proximal Policy Optimization

PPO算法的基本思想是通过最小化相邻策略之间的差异来保证训练的安全性，同时使用一种限制策略变化的方法来防止策略变化过大。在训练过程中，PPO算法使用经验数据和**优势函数**来调整智能体的策略函数。经验数据是指智能体在与环境交互时所获得的状态、动作和奖励等信息，而优势函数表示在某个状态下采取某个动作相对于执行当前策略的性能提升。

0点时：我与环境进行互动，收集了很多数据。然后利用数据更新我的策略，此时我成为1点的我。当我被更新后，理论上，1点的我再次与环境互动，收集数据，然后把我更新到2点，然后这样往复迭代。
但是如果我仍然想继续0点的我收集的数据来进行更新。因为这些数据是0点的我（而不是1点的我）所收集的。所以，我要对这些数据做一些重要性重采样，让这些数据看起来像是1点的我所收集的。当然这里仅仅是看起来像而已，所以我们要对这个“不像”的程度加以更新时的惩罚（KL）

所以PPO可以使用经验回放，但是也是on-policy算法，因为使用了重要性采样，结合一些off-policy(政策)修正技术，例如重要性采样，以使旧样本与新策略相适应。

**优势函数**可以使用GAE来获得，也可以不使用，那么就直接求：
![](images/Pasted%20image%2020230823225214.png)
Generalized Advantage Estimation 是一种估计强化学习中的优势函数（advantage function）的技术
GAE通过结合多个时间步的奖励和值估计来计算优势，从而提供了一种偏差和方差之间的权衡。
GAE的计算：
![](images/Pasted%20image%2020230823224241.png)



















