
PG的损失函数是： 
![](images/Pasted%20image%2020230820171157.png)

这个的代码是：
logits通常是一个两元素的数组。每个元素对应于一个可能的动作的未归一化概率
self.action_buffer包含的是实际采取的动作的索引，这些索引值通常是0或1。其中0表示第一个动作，1表示第二个动作。
```python
loss = F.cross_entropy(logits, torch.tensor(self.action_buffer, dtype=torch.long), reduction='none') * torch.tensor(  
discounted_reward_buffer_norm)  
loss = loss.mean() # 计算平均损失
```
那么

```python
F.cross_entropy(logits, torch.tensor(self.action_buffer, dtype=torch.long), reduction='none')
```
对应的就是![](images/Pasted%20image%2020230820171643.png)
后面的
```python
discounted_reward_buffer_norm
```
是把整个的reward记录下来，然后重新对每个位置的奖励值重新加权计算，如下：

```python
 def _discount_and_norm_rewards(self):  # 奖励折扣和标准化
        discounted_reward_buffer = np.zeros_like(self.reward_buffer)  # 初始化折扣后的奖励缓冲区
        running_add = 0  # 用于累加折扣后的奖励
        for t in reversed(range(0, len(self.reward_buffer))):  # 反向遍历奖励缓冲区
            running_add = running_add * self.gamma + self.reward_buffer[t]  # 计算折扣后的奖励
            discounted_reward_buffer[t] = running_add  # 存储折扣后的奖励

        # 将折扣后的奖励缓冲区进行标准化处理
        discounted_reward_buffer -= np.mean(discounted_reward_buffer)  # 减去均值
        discounted_reward_buffer /= np.std(discounted_reward_buffer)  # 除以标准差
        return discounted_reward_buffer  # 返回处理后的折扣奖励缓冲区

```

















