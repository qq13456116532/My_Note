-- [[#REINFORCE|REINFORCE]]
	- [[#REINFORCE#训练流程|训练流程]]
- [[#Actor-Critic|Actor-Critic]]
	- [[#Actor-Critic#训练流程|训练流程]]
- [[#带基线的策略梯度方法|带基线的策略梯度方法]]
	- [[#带基线的策略梯度方法#带基线的REINFORCE|带基线的REINFORCE]]
	- [[#带基线的策略梯度方法#训练流程|训练流程]]
- [[#Advantage Actor-Critic (A2C)|Advantage Actor-Critic (A2C)]]
	- [[#Advantage Actor-Critic (A2C)#训练流程|训练流程]]
- [[#置信域策略优化TRPO|置信域策略优化TRPO]]
	- [[#置信域策略优化TRPO#训练流程|训练流程]]
- [[#熵正则 (Entropy Regularization）SAC算法|熵正则 (Entropy Regularization）SAC算法]]
- [[#连续控制|连续控制]]
	- [[#连续控制#确定策略梯度 (DPG)|确定策略梯度 (DPG)]]
	- [[#连续控制#随机策略和确定策略的区别|随机策略和确定策略的区别]]
- [[#多智能体|多智能体]]



![](images/Pasted%20image%2020230715174409.png)
在深度Q学习（DQN）中，神经网络的输出通常是一个向量，其每个元素对应于每个可能的离散动作的预期回报。因此，DQN通常用于离散动作空间的问题。
在连续动作空间中，可能的动作数量是无限的，因此我们不能为每个可能的动作分配一个神经网络的输出。这就是为什么DQN不适用于连续动作空间的问题。
所以这就需要 策略梯度policy Gradients 来解决连续动作空间的问题了。

策略梯度方法通过优化策略参数来选择动作。在这种情况下，策略是一个函数，它将每个状态映射到一个动作或一组动作的概率分布。这个函数可以是任何形式的，包括可以处理连续动作空间的函数。例如，策略可以是一个神经网络，其输出是一个连续动作的概率密度函数。
因此，通过直接优化策略参数，策略梯度方法可以处理连续的动作空间。

这里既然要优化策略参数，那肯定要有一个目标，这里就是设计了一个目标函数：
![[Pasted image 20230714111516.png]]
这里的![[Pasted image 20230714111543.png]]是状态价值函数，定义如下：
![[Pasted image 20230714111609.png]]
![[Pasted image 20230714113721.png]]又是动作价值函数，定义如下：
![[Pasted image 20230714113738.png]]
状态价值![[Pasted image 20230714111543.png]]既依赖于当前状态 st，也依赖于策略网络 π 的参数 θ，然后我们只想优化θ一个函数，所以使用期望去掉S，得到了![[Pasted image 20230714114003.png]]

策略 π 越好（即参数 θ 越好），那么Vπ(st) 也会越大。例如，从同一起点出发打游 戏，高手（好的策略）的期望回报远高于初学者（差的策略）。所以策略学习就是为了最大化 J(θ)
想要求解最大化问题，显然可以用梯度上升更新 θ，使得J(θ) 增大。设 当前策略网络的参数为 θ<sub>now</sub>，做梯度上升更新参数，得到新的参数 θ<sub>new</sub>这样来做梯度的上升：
![[Pasted image 20230714114421.png]]
β是学习率，![[Pasted image 20230714114626.png]]是指当前的梯度，也叫做策略梯度。
![[Pasted image 20230714114741.png]]
这个梯度使用公式计算是：
![[Pasted image 20230714114930.png]]
这个很重要！！！！！，记住即可。


现在的问题梯度的期望想直接求出是不可能的，所以这里使用蒙特卡洛近似，
根据当前S,随机抽样一个a，策略网络的参数必须是最新的
![[Pasted image 20230714120857.png]]
计算一个随机梯度
![[Pasted image 20230714120939.png]]
这个就是策略梯度![[Pasted image 20230714121030.png]]的无偏估计
应用上述结论，我们可以做随机梯度上升来更新 θ，使得目标函数 J(θ) 逐渐增长：
![[Pasted image 20230714122015.png]]

但是这里还有缺陷，就是 动作价值函数![[Pasted image 20230714122046.png]]未知，所以还需要对动作价值函数进行近似，就出现了如下两种： 
`REINFORCE`（使用实际观测的汇报 u 近似动作价值函数![[Pasted image 20230714122046.png]]） 
`actor-critic`，用神经网络![[Pasted image 20230714122507.png]]近似![[Pasted image 20230714122046.png]]




# REINFORCE
REINFORCE进一步对Q<sub>π</sub> 做蒙特卡洛近似，把它替换成回报 u
从时刻t开始，智能体完成一局游戏，观测到全部奖励r<sub>t</sub>，....，r<sub>n</sub>,然后可以计算出![[Pasted image 20230714124216.png]]的u<sub>t</sub>就是![[Pasted image 20230714140133.png]]，因为u<sub>t</sub>是U<sub>t</sub>的实际观测值，所以![[Pasted image 20230714151421.png]]
所以更新策略变成：
![[Pasted image 20230714152725.png]]

## 训练流程
![[Pasted image 20230714154658.png]]
![[Pasted image 20230714154729.png]]


# Actor-Critic
上一节的 REINFORCE 用实际观测的回报近似Q<sub>π</sub>，本节的 actor-critic 方法用神经网络近似Q<sub>π</sub>。

![[Pasted image 20230714155344.png]]

![[Pasted image 20230714155424.png]]

![[Pasted image 20230714155526.png]]
![[Pasted image 20230714155536.png]]
训练价值网络（评委）：通过以上分析，我们不难发现上述训练策略网络（演员）的 方法不是真正让演员表现更好，只是让演员更迎合评委的喜好而已。因此，评委的水平 也很重要，只有当评委的打分 q真正反映出动作价值Qπ，演员的水平才能真正提高。初 始的时候，价值网络的参数 w 是随机的，也就是说评委的打分是瞎猜。可以用 SARSA 算法更新w，提高评委的水平。每次从环境中观测到一个奖励 r，把 r 看做是真相，用 r来校准评委的打分。

![[Pasted image 20230714155627.png]]


## 训练流程
![](images/Pasted%20image%2020230714160539.png)
![](images/Pasted%20image%2020230714160623.png)


上面没有用到目标网络来进行计算，下面是使用了目标网络，![](images/Pasted%20image%2020230714162302.png)是目标网络
![](images/Pasted%20image%2020230714162321.png)



# 带基线的策略梯度方法
带基线 的策略梯度（policy gradient with baseline）可以大幅提升策略梯度方法的表现。使用基线（baseline）之后，REINFORCE变成REINFORCE with baseline，actor-critic 变成 advantage actor-critic（A2C）。

只需 对策略梯度公式![](images/Pasted%20image%2020230714163521.png)做一个微小的改动，就能大幅提升表现：把 b 作为动作价值函数 Qπ(S, A) 的基线（baseline），用Q<sub>π</sub>(S, A) − b 替换掉Q<sub>π</sub>。设 b 是任意的函数，只要不依
赖于动作A就可以，例如 b 可以是状态价值函数Vπ(S)。
带基线的策略梯度定理
![](images/Pasted%20image%2020230714163918.png)
![](images/Pasted%20image%2020230714164005.png)
基本都使用V<sub>π</sub>来当作b
![](images/Pasted%20image%2020230714164143.png)



## 带基线的REINFORCE
此处我们同样用u代替Q<sub>π</sub>(s, a)。此外，我们还用一个神经网络 v<sub>π</sub>(s;w) 近似状态价值函 数V<sub>π</sub>(s)。这样一来，g(s, a; θ) 就被近似成了：
![](images/Pasted%20image%2020230714164313.png)

带基线的 REINFORCE 需要两个神经网络：策略网络 π(a|s; θ) 和价值网络 v(s;w)

更新就是这样了：
![](images/Pasted%20image%2020230714171933.png)
训练价值网络就变成如下这样:
![](images/Pasted%20image%2020230714172016.png)


## 训练流程
![](images/Pasted%20image%2020230714172220.png)
还是需要一条轨迹
![](images/Pasted%20image%2020230714172236.png)




# Advantage Actor-Critic (A2C)
![](images/Pasted%20image%2020230714172911.png)
训练价值网络和TD目标一样的
![](images/Pasted%20image%2020230714173157.png)
训练策略网络：
![](images/Pasted%20image%2020230714173310.png)

## 训练流程
![](images/Pasted%20image%2020230714173649.png)
使用目标网络：
![](images/Pasted%20image%2020230714173739.png)
![](images/Pasted%20image%2020230714173753.png)


# 置信域策略优化TRPO

置信域策略优化（trust region policy optimization，TRPO）
学习TRPO的关键在于理解置信域方法（trust region methods）。置信域方法不是TRPO 的论文提出的，而是数值最优化领域中一类经典的算法，历史至少可以追溯到 1970 年。
TRPO 论文的贡献在于巧妙地把置信域方法应用到强化学习中，取得非常好的效果。

与一般的策略优化方法（如梯度上升）不同，TRPO的关键思想是在更新策略时，只在一个“信任区域”（trust region）内进行搜索和优化，这样可以确保新的策略不会与旧的策略相差太远，从而降低策略优化的风险。
TRPO的核心是使用了一种称为Kullback-Leibler (KL) 散度的度量，来限制新策略和旧策略的差异。KL散度可以衡量两个概率分布的相似性。在优化过程中，TRPO通过限制新旧策略之间的KL散度，来定义信任区域，并确保**优化过程只在这个区域内进行。**

用一个一元函数的例 子解释 J(θ) 和 L(θ | θnow) 的关 系。图中横轴是优化变量 θ，纵轴 是函数值。如图 9.2(a) 所示，函 数 L(θ | θnow) 未必在整个定义域 上都接近 J(θ)，而只是在 θnow 的 领域里接近 J(θ)。θnow 的邻域就 叫做**置信域**。
![](images/Pasted%20image%2020230714223148.png)
J 是个很复杂的函 数，我们甚至可能不知道 J 的解 析表达式（比如 J 是某个函数的 期望）。而我们人为构造出的函数 L相对较为简单，比如L是J 的蒙 特卡洛近似，或者是 J 在 θnow 这 个点的二阶泰勒展开。既然可以 信任 L，那么不妨用 L 代替复杂 的函数 J，然后对 L 做最大化。这 样比直接优化 J 要容易得多。这 就是置信域方法的思想。具体来 说，置信域方法做下面这两个步 骤，一直重复下去，当无法让 J 的值增大的时候终止算法。
![](images/Pasted%20image%2020230714223235.png)
![](images/Pasted%20image%2020230714223247.png)
![](images/Pasted%20image%2020230714223254.png)

## 训练流程
![](images/Pasted%20image%2020230714224453.png)
![](images/Pasted%20image%2020230714225537.png)
TRPO 算法真正实现起来并不容易，主要难点在于第二步——最大化。不建议读者 自己去实现 TRPO



# 熵正则 (Entropy Regularization）SAC算法

策略学习的目的是学出一个策略网络 π(a|s; θ) 用于控制智能体。每当智能体观测到 当前状态 s，策略网络输出一个概率分布，智能体依据概率分布抽样一个动作，并执行这个动作。
当输出的几个动作是左、右、上三者中的任何一个，概率分别是 0.03, 0.96, 0.01。 概率都集中在“向右”的动作上，接近**确定性**的决策。确定性大的好处在于不容易选中很 差的动作，比较安全。但是确定性大也有缺点。假如策略网络的输出总是这样确定性很大的概率分布，那么智能体就会**安于现状**，不去尝试没做过的动作，不去探索更多的状态，**无法找到更好**的策略。
我们希望策略网络的输出的概率不要集中在一个动作上，至少要给其他的动作一些 非零的概率，让这些动作能被探索到。可以用熵 (Entropy) 来衡量概率分布的不确定性。 对于上述离散概率分布 p = [p1, p2, p3]，熵等于
![](images/Pasted%20image%2020230714231629.png)
熵小说明概率质量很集中，熵大说明随机性很大

![](images/Pasted%20image%2020230714233735.png)
熵 H(s; θ) 只依赖于状态 s 与策略网络参数 θ。我们希望对于大多数的状态 s，熵都会比较大，也就是让 ES[H(S; θ)] 比较大。
策略学习的目标函数是 J(θ) = ES[Vπ(S)]。策略学习的目的是寻找参数 θ 使得 J(θ) 最大化。同时，我们还希望让熵比较大，所以把熵作为正则项，放到目标函数里。使 用熵正则的策略学习可以写作这样的最大化问题
![](images/Pasted%20image%2020230714233833.png)
此处的 λ 是个超参数，需要手动调。



# 连续控制 

## 确定策略梯度 (DPG)

确定策略梯度（deterministic policy gradient, DPG）是最常用的连续控制方法。DPG 是一种 actor-critic 方法，它有一个**策略网络**（演员），一个**价值网络**（评委）。策略网络 控制智能体做运动，它基于状态 s 做出动作 a。价值网络不控制智能体，只是基于状态 s 给动作 a 打分，从而指导策略网络做出改进

![](images/Pasted%20image%2020230715000257.png)
在之前章节里，策略网络 π(a|s; θ) 是 一个概率质量函数，它输出的是概率值。本节的确定策略网络 µ(s; θ) 的输出是 d 维的向量 a，是一个具体的动作，例如`（转动10°，转动20°）`这个向量，所以叫做 Deterministic。两种策略网络一个是随机的，一个是**确定性的**
这里的价值网络 q(s, a; w) 是对动作价值函数 Q<sub>π</sub>(s, a) 的近似
价值网络可以评价策略网络 的表现。在训练的过程中，价值网络帮助训练策略网络；在训练结束之后，价值网络就 被丢弃，由策略网络控制智能体。
更新价值网络可以使用TD Target
输出的价值q与θ有关，需要的q越大，要对θ进行梯度上升
![](images/Pasted%20image%2020230715154059.png)
![](images/Pasted%20image%2020230715154200.png)

为了防止自举还可以使用target networks。
目标网络是主Q网络的一个复制品，但它的参数不是每个步骤都更新，而是每隔一段时间从主Q网络复制过来。这样，目标网络在计算更新目标时，使用的是过去的、固定的Q值，这使得训练过程更加稳定。


## 随机策略和确定策略的区别
![](images/Pasted%20image%2020230715160902.png)



# 多智能体
![](images/Pasted%20image%2020230715174515.png)
纳什均衡当每个智能体都找不到更好的策略时收敛

多智能体强化学习有三种架构
![](images/Pasted%20image%2020230715180251.png)

第一种是去中心化：
![](images/Pasted%20image%2020230715180327.png)

第二种是中心控制，决策都由Controller来做
![](images/Pasted%20image%2020230715180621.png)


第三种是集中训练和分离执行
![](images/Pasted%20image%2020230715191749.png)
![](images/Pasted%20image%2020230715191724.png)

三个方法区别
![](images/Pasted%20image%2020230715195443.png)


















