强化学习框架还不完全完善的一个重要原因是它假设奖励是由环境给出的。
![](images/Pasted%20image%2020231128215510.png)
agent发送一个action，环境则将观察和奖励发送回来。
现实中的情况不是这样，因为我们自己有一个框架来从观察中找出奖励是什么
**我们奖励自己**we rewards ourselves。
环境并没有说，哪里有负面奖励，这其实就是感知，可以让我们确定什么是奖励。

那么这里agent应该是什么呢？这里就是神经网络，每当我们想做某事时，答案是 一个神经网络，并且我们想要agent将观察映射到行动。因此，我们就可以使用神经网络对其进行参数化，然后应用学习算法。

一句话说强化学习， try something new，add randomness to your actions，and compare the result to your expectation. If the result surprises you，if you find that the result exceeded your expectation, then change your parameters to take those actions in the future.That's it. Try it out ,see if you like it, and if you do ,do more of that in the future .That's the core idea.

在强化学习中，你运行一个神经网络，为我的行动添加了一点随机性，然后如果你喜欢这个结果，实际上，随机性变成了想要的目标。

![](images/Pasted%20image%2020231129102248.png)
在不解释这里方程的意义的情况下，重点不是推导，而是证明它们存在。强化学习有两类，一类就是策略梯度。
我们做的就是将这个期望总和、奖励的总和 $\mathbb{E}\left[\sum_tr_t\right]$ 的表达式，计算其衍生品即可，扩展、计算、代数替换，然后就得到了一个导数。奇迹般的是，导数的形式恰好是 **尝试一些行动，如果你喜欢它们，增加对数的概率**，这是从数学上得出的。从直观上的解释在等式中得到的内容具有一一对应的关系。















