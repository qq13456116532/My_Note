
强化学习的两大类算法族是“直接法”和“间接法”：1. 间接型RL是求解最优性条件（如[Bellman方程](https://www.zhihu.com/search?q=Bellman%E6%96%B9%E7%A8%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3287915850%7D)，HJB方程），本质上就是求解一个等式方程，[策略迭代法](https://www.zhihu.com/search?q=%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E6%B3%95&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3287915850%7D)等价于牛顿迭代，值迭代法等价于[不动点迭代](https://www.zhihu.com/search?q=%E4%B8%8D%E5%8A%A8%E7%82%B9%E8%BF%AD%E4%BB%A3&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3287915850%7D)（这个等价本身就是美感）。2. 直接型RL就是直接找原问题的梯度，即policy gradient；而梯度的估计过程，需要Q或者V的估计，即[critic](https://www.zhihu.com/search?q=critic&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3287915850%7D)。3. 十分巧妙的，这两者都能导出[Actor-Critic](https://www.zhihu.com/search?q=Actor-Critic&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3287915850%7D)框架，且二者的结果恰好是一致的，颇有殊途同归的美感。

等价的美是可以给我们一些基础研究方向的启发。比如：值迭代法等价于不动点迭代，以此为观点，可以发现Q-learning就是一类不动点迭代Mann fixed-point iteration。因此，使用全新的[fixed-point iteration](https://www.zhihu.com/search?q=fixed-point%20iteration&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3287915850%7D)，就能获得全新的RL迭代。

再例如：以直接法为观点，Actor-Critic框架就是策略梯度加上它内部的Q或者V的估计，它的算法收敛性取决于梯度优化法本身。[凸优化](https://www.zhihu.com/search?q=%E5%87%B8%E4%BC%98%E5%8C%96&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3287915850%7D)领域有众多新颖的算法，设法套一套，试一试，就可产生新的强化学习算法，[TRPO](https://www.zhihu.com/search?q=TRPO&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3287915850%7D)，CPO就是这么产生的。另外，为什么只估计Q或者V呢？Baseline技巧告诉我们，估计Q-V更好，这就是advantage 函数。而advantage 函数实际上又等价于[时序差分算法](https://www.zhihu.com/search?q=%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E7%AE%97%E6%B3%95&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3287915850%7D)，这让我们明白前者的学习效果好原来是因为[bootstrapping机制](https://www.zhihu.com/search?q=bootstrapping%E6%9C%BA%E5%88%B6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3287915850%7D)的参与。

 
GPI由两个过程组成，其中一个驱使价值函数去准确地预测当前策略的回报，这就是所谓的“预测问题”。而另一个过程则驱使策略根据当前的价值函数来进行局部改善(例如可以使用 e-贪心策略)，这就是所谓的“控制问题”。


即使所有动作的回报都是正的，策略梯度方法仍然可以区分出哪些动作带来更高的回报。这是因为它关注的是动作之间回报的相对差异，而非绝对值。动作概率的调整基于对各个动作带来的回报进行比较。动作带来的回报越高，增加其概率的幅度就越大；反之亦然。

然后用基线有下面这些好处：
**减少方差**：策略梯度估计通常具有高方差，这意味着需要很多样本才能得到稳定和可靠的梯度估计。基线的使用可以减少这种方差，使得学习过程更稳定，需要的样本数量更少。
**加速收敛**：通过减少方差，基线可以帮助策略梯度算法更快地收敛到最优策略。这是因为它提供了一个参照点，帮助算法更有效地区分更好和更差的动作。






