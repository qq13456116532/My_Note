
- [[#K臂老虎机|K臂老虎机]]
		- [[#Upper-Confidence-Bound 上置信界|Upper-Confidence-Bound 上置信界]]
- [[#Gradient Bandit Algorithms|Gradient Bandit Algorithms]]
- [[#MDPs|MDPs]]
- [[#最优化|最优化]]
- [[#动态规划|动态规划]]
- [[#值迭代|值迭代]]
		- [[#策略迭代|策略迭代]]
		- [[#价值迭代|价值迭代]]
- [[#异步动态规划|异步动态规划]]
- [[#蒙特卡洛|蒙特卡洛]]
		- [[#蒙特卡洛预测|蒙特卡洛预测]]
		- [[#蒙特卡洛ES|蒙特卡洛ES]]
- [[#重要性采样|重要性采样]]
	- [[#重要性采样#off-policy蒙特卡洛控制|off-policy蒙特卡洛控制]]
- [[#TD差分|TD差分]]
	- [[#TD差分#TD预测方法的优势|TD预测方法的优势]]
	- [[#TD差分#TD（0）的优化|TD（0）的优化]]
		- [[#TD（0）的优化#SARSA|SARSA]]
		- [[#TD（0）的优化#Q-learning|Q-learning]]
		- [[#TD（0）的优化#expected Sarsa|expected Sarsa]]
		- [[#TD（0）的优化#最大化偏差与双学习|最大化偏差与双学习]]
		- [[#TD（0）的优化#n步自举法|n步自举法]]
		- [[#TD（0）的优化#n步Sarsa|n步Sarsa]]
- [[#表格型近似求解方法|表格型近似求解方法]]
		- [[#TD（0）的优化#线性方法|线性方法]]
		- [[#TD（0）的优化#基于函数逼近的 on-policy Control|基于函数逼近的 on-policy Control]]
		- [[#TD（0）的优化#平均收益|平均收益]]
		- [[#TD（0）的优化#off-policy methods with approximation|off-policy methods with approximation]]
		- [[#TD（0）的优化#致命三要素|致命三要素]]
		- [[#TD（0）的优化#策略梯度|策略梯度]]
		- [[#TD（0）的优化#策略梯度定理|策略梯度定理]]
		- [[#TD（0）的优化#REINFORCE|REINFORCE]]
		- [[#TD（0）的优化#Actor-Critic|Actor-Critic]]
		- [[#TD（0）的优化#预测与控制|预测与控制]]

agent并不是被告诉去采取什么动作，而是自己去尝试，然后发现能获得最大reward的动作。
因为动作既可以影响immediate reward，也影响下个状态和下面的reward，所以 trial-and-error和延迟奖励是很重要的两个强化学习特点。

- policy： 从可感知的状态到动作的一个映射
- reward signal： 指示什么在即时意义上是好的，可能是一个关于状态与动作的随机函数
- value function：指定什么从长远来看是好的。
我们通常选择一个行动，这个行动可以带来最大的奖励总和在长期活动中，而不是最大的即时奖励。
奖励基本上是直接给出的，但是价值必须被评估，所以强化学习中最重要的部分就是那些有效评估价值的方法。

evolutionary methods：  获得最多奖励的策略以及它们的随机变体被保留到下一代策略中，然后这个过程重复进行。

我们想要更准确地估计赢得游戏的概率，这可以通过将较早状态的值向后来状态的值移动一部分来完成，如果我们将 $S_t$ 表示贪婪移动前的状态， $S_{t+1}$表示贪婪移动后的状态，那么可以更新可以被写成：
$$V(S_t)\leftarrow V(S_t)+\alpha[V(S_{t+1})-V(S_t)]$$
通过这种方式，价值函数的估计逐渐改进，反映出通过实际游戏经验学习到的胜率



### K臂老虎机
反复在k个不同的选项中做出选择，每次选择后得到一个数值奖励（来自一个固定的概率分布），目标是在一定时间周期内最大化预期总奖励。
经过重复的选择我们为了最大化奖励，肯定需要把动作集中到能得到最大奖励值的那根杆子
k个动作每一个被选中时都有预期或者平均奖励，这称为行动的价值，任意一个动作$a$的价值，记为 $q_*(a)$ ,就是选中$a$的期望reward
$$q_*(a)\doteq\mathbb{E}[R_t\mid A_t{=}a]$$

但是一开始并不知道动作的价值，那么我们只能估计，于是就把动作$a$在时间$t$的价值估计当作 $Q_t(a)$我们想让这个估计趋向于  $q_*(a)$

ε-贪婪方法相对于贪婪方法的优势取决于任务本身，假设奖励的方差更大，在奖励更加不确定的情况下，需要更多的探索来找到最优行动，ε-贪婪方法相对于贪婪方法应该会表现得更好。另一方面，如果奖励方差为零，那么贪婪方法在尝试每个行动一次后就会知道它们的真实价值。在这种情况下，贪婪方法可能实际上表现得最好，因为它很快就会找到最优行动，然后就不再进行探索

这里action-value的值是 平均，所以可以使用增量的方式计算。
![](images/Pasted%20image%2020231025203954.png)



在老虎机这种静态环境中，步长$StepSize$可以随着n增加而变小，但是在非静态环境中，必须更有效的适应环境的变化，恒定的步长参数可以帮助算法更快地忘记过时的或不再相关的旧信息，更加重视最近的经验和信息
![](images/Pasted%20image%2020231025204908.png)
所以此时这里的  $\alpha$就变成了恒定的，可以得到：
![](images/Pasted%20image%2020231025204954.png)
也就是 $Q_{n+1}$是 $R_i$和$Q_1$的加权和。


当 $\alpha$是 $1/n$时，此时是在样本平均的方法中，由于大数定律，可以保证 action-value一定会收敛。
但是对于其他的  $\alpha$序列就不一定能保证了，这里有两个能保证收敛的条件：
![](images/Pasted%20image%2020231025210429.png)
第一个条件是为了确保步骤足够大，最终能够克服任何初始条件或随机波动。第二个条件保证了最终步骤会变得足够小，以确保收敛

探索是必要的，因为对动作价值估计的准确性总是存在不确定性。

##### Upper-Confidence-Bound 上置信界
上置信界限（UCB）动作选择的思想是，平方根项是对动作 $a$ 的价值估计的不确定性或方差的度量。因此，被取最大值的量是动作 a 可能的真实价值的一种上界，其中 $c$ 决定了置信水平
![](images/Pasted%20image%2020231026083622.png)
- 每次选择 $a$ 时，不确定性假定会降低：$N_t(a)$ 增加，且由于它出现在分母中，不确定性项减少。
- 另一方面，每次选择 $a$ 以外的动作时，$t$ 增加但 $N_t(a)$ 不增加；因为 $t$ 出现在分子中，不确定性估计增加
所有动作最终都会被选中，但是那些估值较低或已经被频繁选择的动作，随着时间的推移被选择的频率会降低

很明显，工作的效果很好：![](images/Pasted%20image%2020231026084053.png)


### Gradient Bandit Algorithms
梯度赌博机算法
前面介绍了通过估计action-value，然后使用这个估计来选择动作。
这里我们给每个动作添加一个偏好preference，$H_t(a)$，这个偏好越大，动作$a$越可能被选择。
动作选择的概率是通过 soft-max分布来确定的：
$$\Pr\{A_t{=}a\}\doteq\frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}}\doteq\pi_t(a),$$

这里引入了一个新符号， $\pi_t(a)$，是指在时间$t$，采取a的概率。

基于随机梯度上升的思想，很容易想到这样的更新方程：
    在每一步，选择 $A_t$然后接收到奖励 $R_t$，动作的偏好preference可以被更新为： ![](images/Pasted%20image%2020231026085725.png)
    $\alpha$是step-size 参数， ![](images/Pasted%20image%2020231026090010.png)是平均的奖励值，它作为一个基线来和当前获得的reward比较，那么如果新reward比基线大，那么提高采取$A_t$的概率，如果小，那么减少概率
上面乘 $1-\pi_t(A_t)$的理由是： 如 $A_t$已经是一个很大概率的动作，那么我们就不需要大幅度的增加概率了，如果还是一个小概率的动作，那么对其偏好的增加更显著。
下面乘$\pi_t(a)$的理由是： 为了表示更新的程度与行为 $a$被选中的原始概率成比例，如果某个未选中的行为 $a$ 本身就很不可能被选择，那么它对偏好的负面影响也应该较小
很明显，动作更好了：
![](images/Pasted%20image%2020231026091610.png)


总结一下上面的所有算法，![](images/Pasted%20image%2020231026094550.png)
其中UCB差不多是最好的



# MDPs
MDPs 的动作影响的不只是即时奖励，也有接下来的状态，因此MDPs需要去权衡即时奖励和延迟奖励。
在多臂赌博机里面，我们评估每个动作的价值，记为$q_*(a)$，在MDPs里面我们评估在状态$s$下选择动作$a$的价值，记为 $q_*(a)$ ，或者我们估计在最优行动选择下每个状态的价值$v_*(s)$

在有限MDP中，状态states、动作actions、奖励rewards集合都有一个有限的元素数量
对于所有的 $s,a,r,s'$ ，都有
$$p(s^{\prime},r|s,a)~\doteq~\Pr\{S_t{=}s^{\prime},R_t{=}r\mid S_{t-1}{=}s,A_{t-1}{=}a\}$$
这个被称为MDP的动态性。
等号上面的点说明这是一个定义式子，而不是由先前的定义推导出的式子。
容易得到：
$$\sum_{s^{\prime}\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s^{\prime},r|s,a)=1,\text{ for all }s\in\mathcal{S},a\in\mathcal{A}(s).$$

在RL中，一个agent的目标是通过一个叫 **reward**的信号来形式化的，它是从环境传递给智能体。非正式地说，智能体的目标是最大化其接收到的奖励总量。这意味着不仅要最大化即时奖励，而且要长期最大化累积奖励，
可以理解为期望值的最大化，这个期望值是接收到的标量信号（称为奖励）的累积和的预期值

 奖励信号是你用来向代理传达你想要实现什么，而不是你想要如何实现的方式，例如，一个下棋的代理应该只因为实际获胜而得到奖励，而不是因为实现了诸如夺走对手棋子或控制棋盘中心等子目标

前面我们说了，代理的目标是要最大化它长期所接收到的累积奖励。这如何被正式定义呢？
如果在时间步骤 $t$ 之后接收到的奖励序列被表示为$R_{t+1}, R_{t+2}, R_{t+3}, ...，$那么我们希望最大化这个序列的哪个确切方面呢？
一般来说，我们寻求最大化预期回报，其中回报，记作 $G_t$，被定义为奖励序列的某个具体函数，下面是它的最简单形式
$$G_t\doteq R_{t+1}+R_{t+2}+R_{t+3}+\cdots+R_T,$$
这里的定义有一个结束时间$T$，通过这里也得到episode的定义，当代理与环境的互动自然地分解为子序列时，我们称之为episode，T时刻的状态是 terminal state。
在许多情况下，代理与环境的互动是持续不断地进行，那么前面这个方程就有一点问题，因为 $T=\infty$  , 因此使用下面这种方程来表示 $G_t$
$$G_t~\doteq~R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots~=~\sum_{k=0}^\infty\gamma^kR_{t+k+1}$$
这里有个额外的概念，是折扣discounting，$0 \le \gamma \le 1$ 叫做discount rate
目标就变成了选择$A_t$去最大化期望的折扣回报

连续时间步骤的回报彼此之间以一种重要的方式相关联：
$$\begin{aligned}
G_{t}& \begin{aligned}&\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\gamma^3R_{t+4}+\cdots\end{aligned}  \\
&=R_{t+1}+\gamma\big(R_{t+2}+\gamma R_{t+3}+\gamma^2R_{t+4}+\cdots\big) \\
&=R_{t+1}+\gamma G_{t+1}
\end{aligned}$$


前面可以看到连续动作和周期动作的描述方法一般是不同的，所以建立一种符号法以便我们能同时准确讨论两种情况是非常有用的
简单来说就是把周期动作也看成连续动作，![](images/Pasted%20image%2020231026163203.png)在达到terminated state后，一直在自己的状态循环并得到0的奖励值。
$$G_t\doteq\sum_{k=t+1}^T\gamma^{k-t-1}R_k,$$
于是这个方程就是通用的方程


value function： 用来评估agent在某个状态的好坏，“好坏”又是根据期望的未来奖励值确定，也是期望回报。很明显期望得到的未来的奖励是取决于自己采用的动作，所以，值函数是根据特定的行为方式（称为策略）来定义的。
policy： 一个策略是状态到动作概率的映射，如果一个agent在时间 t 时遵循策略 π，那么 $\pi(a|s)$就是在 $S_t = s$ 时 $A_t = a$ 的概率

在策略 $\pi$下的一个状态 $s$的价值函数，叫作 $v_\pi(s)$，就是从$s$开始，然后遵循策略 $\pi$的期望回报。对于MDPs，可以用这样的形式定义：
$$v_\pi(s)~\doteq~\mathbb{E}_\pi[G_t\mid S_t=s]~=~\mathbb{E}_\pi\biggl[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\biggr|~S_t=s\biggr],~\text{for all}~s\in\mathcal{S}$$
相似的，我们也可以定义在策略 $\pi$下，在状态 $s$采取动作$a$的价值，记作$q_\pi(s,a)$，就是从$s$开始，采取了动作$a$，之后遵循策略$\pi$，
$$q_\pi(s,a)\doteq\mathbb{E}_\pi[G_t\mid S_t{=}s,A_t=a]\quad=\mathbb{E}_\pi\bigg[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\bigg|S_t{=}s,A_t{=}a\bigg]$$
这个叫做策略$\pi$的动作价值函数


在强化学习中使用到价值函数的一个重要性质，就是：$s$ 的值与其可能的后继状态的值之间存在递归性关系：
$$\begin{aligned}
v_{\pi}(s)& \doteq\mathbb{E}_\pi[G_t\mid S_t{=}s]  \\
&=\mathbb{E}_\pi[R_{t+1}+\gamma G_{t+1}\mid S_t{=}s] \\
&=\sum_a\pi(a|s)\sum_{s'}\sum_rp(s',r|s,a)\Big[r+\gamma\mathbb{E}_\pi[G_{t+1}|S_{t+1}=s']\Big] \\
&=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_\pi(s')\Big],\quad\text{for all }s\in\mathcal{S},
\end{aligned}$$
在最后的等式中，我们也注意到如何将两个求和，一个是关于所有 $s'$ 的值，另一个是关于所有 $r$ 的值，合并为一个求和，包括两者的所有可能值。我们经常使用这种合并求和来简化公式。注意最终表达式如何可以轻松地读作一个期望值。它实际上是对三个变量 $a$, $s'$ 和 $r$ 的所有值的总和。对于每个三元组，我们计算其概率，$π(a|s)p(s0, r|s, a)$，按该概率对括号中的数量进行加权，然后对所有可能性求和以得到期望值。

像上面这个最后等式，也叫做$v_\pi$的贝尔曼等式，表达了当前状态和后续状态的关系。贝尔曼方程对所有可能性进行平均，按其发生的概率进行加权。它表明，起始状态的值必须等于预期下一个状态的（折扣后）值，加上沿途预期的奖励。对其理解，可以看这个图
![](images/Pasted%20image%2020231026170641.png)
像这样的图也成为backup diagram ，即备份图。因为它们图解了构成更新或备份操作基础的关系，这些操作是强化学习方法的核心。



# 最优化
解决一个强化学习任务，也就是说，找到一个策略，在长期内能达到很多奖励。对于有限的MDPs，我们可以用下面的方式定义最优策略，
**对于所有的状态，如果策略$π$的预期回报大于或等于策略$π'$的预期回报，那么策略$π$就被定义为优于或等于策略$π'$**,换句话说，$\pi \ge \pi'$ 当且仅当对于任意状态$s$，$v_{\pi(s)}\ge v_{\pi'}(s)$
我们把所有的最优策略（可能不止一个）记为 $\pi_*$，它们都有相同的状态价值函数，叫做 **最优状态价值函数**，记为 $v_*$,
$$v_*(s)\doteq\max_\pi v_\pi(s).$$

最优策略同时也共享相同的最优动作价值函数，记为 $q_*$，公式为
$$q_*(s,a)\doteq\max_\pi q_\pi(s,a)$$
这个函数给出了一个期望回报，当你在状态$s$执行了动作$a$然后遵循一个最优策略，那么很容易得到它遵循这样的方程：
$$q_*(s,a)=\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t=s,A_t=a]$$
因为 $v_*$是一个策略的价值函数，它必须满足由贝尔曼方程式给出的状态值的自洽条件，但是又因为它是最优的价值函数，所以它的自洽性可以被写成一种特殊的形式，不需要引用任何特殊的策略，这就是 $v_*$的最优方程，也可以叫做 **贝尔曼最优 方程**。直觉上，贝尔曼最优方程中，在一个状态下，最优策略的值必须要等于它选择的最好的动作的期望值，用方程来说是这样：$v_*(s)=\max_{a\in\mathcal{A}(s)}q_{\pi_*}(s,a)$推导一下，
$$\begin{aligned}
v_{*}(s)& =\max_{a\in\mathcal{A}(s)}q_{\pi_*}(s,a)  \\
&=\max_a\mathbb{E}_{\pi_*}[G_t\mid S_t=s,A_t=a] \\
&=\max_a\mathbb{E}_{\pi_*}[R_{t+1}+\gamma G_{t+1}\mid S_t{=}s,A_t{=}a] \\
&=\max_a\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t=s,A_t=a] \\
&\begin{aligned}=\max_a\sum_{s^{\prime},r}p(s^{\prime},r|s,a)\big[r+\gamma v_*(s^{\prime})\big].\end{aligned}
\end{aligned}$$
这最后两个等式就是 $v_*$的贝尔曼最优方程，类似的，对于 $q_*(s,a)$，
$$\begin{gathered}
q_{*}(s,a) \begin{aligned}=\quad\mathbb{E}\Big[R_{t+1}+\gamma\max_{a'}q_*(S_{t+1},a')\Big|S_t=s,A_t=a\Big]\end{aligned} \\
=\sum_{s',r}p(s',r|s,a)\Big[r+\gamma\max_{a'}q_*(s',a')\Big]. 
\end{gathered}$$
下面是他们的备份图，除了在代理的选择点增加了弧来表示选择该点的最大值而不是给定某个策略的预期值之外，与之前的没区别
![](images/Pasted%20image%2020231027154325.png)
对于有限MDPs来说，贝尔曼最优方程对 $v_*$有一个唯一解，因为它实际上是一组方程组，有n个未知数，n个方程，如果动态性$p$已知，那么$v_*$容易通过解非线性方程组的方式求解出来。

只要有了 $v_*$，很容易去得到最优策略，一步搜索后看起来最好的动作将是最优动作。要注意 $v_*$是已经考虑到了长期的回报，通过 $v_*$，最优的期望长期回报被当作一个数值可以被现在的状态立即使用。

有了 $q_*$会让选择最优的动作变得更轻松，agent甚至不需要去做一个 “一步搜索”，“一步搜索”在这里也可以是这样： 对于任意状态$s$，他都意味着找到一个动作，这个动作最大化$q_*(s,a)$，这个动作价值函数相当于是缓存了一步搜索的结果。它为每个状态-动作对提供了最优的预期长期回报，作为一个在本地和立即可用的值。  因此，以代表状态-动作对函数为代价，而不仅仅是状态，最优动作-价值函数允许在不需要了解任何可能的后续状态及其值，即不需要了解环境动态的情况下，选择最优动作。

明确的解决贝尔曼最优方程提供的是一种找到最优策略的途径，从而解决强化学习问题。但是它类似于一种穷举搜索，所以很少直接有用。而且要使用它也需要有下面的三条必要条件：
- (1) 环境的动态特性被准确知晓；
- (2) 计算资源足够完成计算；
- (3) 状态具有马尔可夫性质
于是，在强化学习中，通常需要接受近似解决方案。
许多强化学习方法可以理解为通过使用实际经验的转换来近似解决贝尔曼最优性方程，而不是使用对预期转换的了解

先前定义了最优质函数和最优策略，但问题是生成最优策略需要极高的计算资源，所以它只是一个智能体只能近似达到的理想目标。

在强化学习中，我们非常关注那些无法找到最优解，而必须以某种方式近似的情况。



# 动态规划
传统的DP算法在强化学习中的实用性有限，原因是它们假设了一个完美的模型，并且计算成本很高，但从理论上讲它们仍然很重要。
动态规划（DP）的核心思想，以及强化学习的通用思想，是使用价值函数来组织和构造寻找良好策略的搜索过程。
就像前面已经讨论过的，如果我们直到了最优价值函数 $v_*$或者$q_*$，那么可以很容易地得到最优策略，而且这两函数也满足贝尔曼方程：
$$\begin{gathered}
v_{*}(s) \begin{aligned}&=\max_a\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t=s,A_t=a]\end{aligned} \\
=\max_a\sum_{s^{\prime},r}p(s^{\prime},r|s,a)\Big[r+\gamma v_*(s^{\prime})\Big], 
\end{gathered}$$
和
$$\begin{gathered}
q_{*}(s,a) =\mathbb{E}\bigg[R_{t+1}+\gamma\max_{a'}q_*(S_{t+1},a')\bigg|S_t=s,A_t=a\bigg] \\
=\sum_{s',r}p(s',r|s,a)\Big[r+\gamma\max_{a'}q_*(s',a')\Big], 
\end{gathered}$$

首先我们考虑怎么计算在一个任意策略$\pi$下的状态价值函数 $v_\pi$，这也叫做策略评估，有时也叫做 **预测问题**。
回想之前的状态价值计算公式：
$$\begin{aligned}
v_{\pi}(s)& \doteq\mathbb{E}_{\pi}[G_{t}\mid S_{t}=s]  \\
&=\mathbb{E}_\pi[R_{t+1}+\gamma G_{t+1}\mid S_t{=}s] \\
&=\mathbb{E}_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s] \\
&=\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_\pi(s')\Big],
\end{aligned}$$
然后只需要保证 $\gamma$小于1或者有终止状态，那么 $v_\pi$存在且唯一。

如果说环境的动态性是完全已知的，那么上面这个关于 $v_\pi$的方程就是就是相当于 $|S|$个线性方程组，且有 $|S|$个未知数，所以原则上是可以直接计算出来的，但是考虑到我们的目的，迭代方法是最合适的。考虑一个估计价值函数的序列： $v_0,v_1,v_2...$，初始的$v_0$是随机选择的，然后每个接下来的估计都是通过贝尔曼方程来得到
$$\begin{aligned}v_{k+1}(s)\quad&\doteq\quad\mathbb{E}_{\pi}[R_{t+1}+\gamma v_{k}(S_{t+1})\mid S_{t}=s]\\&=\quad\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_{k}(s')\Big],\end{aligned}$$
实际上，可以证明，在保证 $v_\pi$存在的情况下，$v_k$会随着 $k->\infty$ 而收敛到 $v_\pi$
上面这种算法就是**策略评估**。

根据更新的是状态（如本例）还是状态-动作对，以及后继状态的估计值是如何组合的，有几种不同类型的期望更新。在动态规划（DP）算法中进行的所有更新都称为期望更新，因为它们是基于所有可能的下一个状态的期望，而不是基于一个样本下一个状态。
如果要编写一个顺序的计算机程序，那么可能需要使用两个数组，一个用于旧值，vk(s)，另一个用于新值，vk+1(s)。通过两个数组，新值可以一个接一个地从旧值中计算出来，而不会改变旧值。或者，你也可以使用一个数组并“原地”更新值，即每个新值立即覆盖旧值。实际上 ，`原地版本`通常比两个数组的版本收敛更快，因为它一旦数据可用就立即使用新数据。所以通常都是考虑in-place的方式编写代码。
虽然说是 $k->\infty$ 才会收敛，但是实际上需要提前中断，只需要判断一个更新的量足够小就行：
$$\begin{aligned}\max_{s\in\mathcal{S}}|v_{k+1}(s)-v_{k}(s)|\end{aligned}$$


### 值迭代
我们计算策略的值函数的目的是为了找到更好的策略，那么假设我们现在已经确定了一个策略$\pi$的值函数$v_\pi$，那么对于一个状态$s$，我们怎么知道是否选择新的动作 $a\neq \pi(s)$呢？因为通过$v_\pi$我们知道从$s$开始遵循当前策略会有多好，但是新策略是会更好还是更差怎么看呢？
回答这个问题的一种方法是考虑在状态$s$中选择动作$a$，然后在之后遵循现有的策略，$π$。这个值的公式可以写成：
$$\begin{aligned}q_{\pi}(s,a)\quad&\doteq\quad\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s,A_{t}=a]\\&=\quad\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_{\pi}(s')\Big].\end{aligned}$$
关键的标准就是这个值是大于 $v_\pi(s)$还是小于，如果它大于，也就是说在 状态$s$选择动作$a$然后再遵循$\pi$会比一直遵循 $\pi$更加好。那么我们会期望每次遇到状态s时选择动作a会更好，实际上新的策略会是一个整体上更好的策略。

这里刚刚讲的就是 **策略改进**的一个特殊例子，对于任意 $s∈S$，如果存在
$$q_\pi(s,\pi^{\prime}(s))\geq v_\pi(s)$$
那么策略 $\pi'$一定好于或者是等于策略 $\pi$，也就是说， 策略 $\pi'$一定会获得更大或者相等的期望回报（对于任意 $s∈S$），
$$v_{\pi^{\prime}}(s)\geq v_\pi(s)$$
而且，如果前者是严格大于，那么后者也是严格大于的。

策略改进理论是应用到了我们一开始的假设： 一个初始的确定的策略$\pi$和一个改变后的策略 $\pi'$，它和$\pi$相同，除了 $\pi'(s)=a\neq\pi(s)$ 。对于除了$s$的其他状态，$q_\pi(s,\pi'(s)) ==  v_\pi(s)$ ，因此，如果$q_\pi(s,a)>v_\pi(s)$，那么这个改变后的策略确实是大于原来的策略的。
策略改进定理证明背后的思想很容易理解，从前面的公式$q_\pi(s,\pi^{\prime}(s))\geq v_\pi(s)$开始，我们不断扩展 $q_\pi$
$$\begin{aligned}
&v_{\pi}(s) \leq q_\pi(s,\pi^{\prime}(s))  \\
&=\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s,A_{t}=\pi'(s)] \\
&=\mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s] \\
&\leq\mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma q_{\pi}(S_{t+1},\pi^{\prime}(S_{t+1}))\mid S_{t}=s] \\
&=\mathbb{E}_{\pi^{\prime}}[R_{t+1}+\gamma\mathbb{E}[R_{t+2}+\gamma\upsilon_{\pi}(S_{t+2})|S_{t+1},A_{t+1}=\pi^{\prime}(S_{t+1})]\mid S \\
&=\mathbb{E}_{\pi^{\prime}}\big[R_{t+1}+\gamma R_{t+2}+\gamma^2v_\pi(S_{t+2})\big|S_t=s\big] \\
&\leq\mathbb{E}_{\pi^{\prime}}\big[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\gamma^3v_\pi(S_{t+3})\big|S_t=s\big] \\
&\leq\mathbb{E}_{\pi^{\prime}}\big[R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\gamma^3R_{t+4}+\cdots\big|S_t{=}s\big] \\
&=v_{\pi^{\prime}}(s).
\end{aligned}$$


到目前为止，我们已经看到了如何在给定一个策略及其价值函数的情况下，轻松地评估在单个状态下策略的改变。自然地，我们可以考虑在所有状态下的改变，根据 $q_π​(s,a)$ 在每个状态选择看起来最好的动作
$$\begin{aligned}
\pi^{\prime}(s)& \begin{aligned}\dot{=}\quad\arg\max_aq_\pi(s,a)\end{aligned}  \\
&\begin{aligned}=\quad\underset{a}{\operatorname*{argmax}}\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s,A_{t}=a]\end{aligned} \\
&\begin{aligned}=\quad\arg\max_a\sum_{s^{\prime},r}p(s^{\prime},r|s,a)\Big[r+\gamma v_\pi(s^{\prime})\Big],\end{aligned}
\end{aligned}$$
其中 $\arg\max_a$ 表示使后面的表达式最大化的 a 的值，很容易知道$\arg\max_a$是满足$q_\pi(s,\pi^{\prime}(s))\geq v_\pi(s)$这个条件的，所以我们知道它至少与原始策略一样好，或者更好

通过根据原始策略的价值函数使其变得贪婪，从而创建一个比原始策略更好的新策略的过程，称为**策略改进**。
然后从上面的 $\pi'$的公式我们任意得到$v_\pi$的公式
$$\begin{aligned}v_{\pi'}(s)\quad&=\quad\max_a\mathbb{E}[R_{t+1}+\gamma v_{\pi'}(S_{t+1})\mid S_t=s,A_t=a]\\&=\quad\max_a\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_{\pi'}(s')\Big].\end{aligned}$$
这里发现，他是和贝尔曼最优方程是一模一样的，
> 贝尔曼最优方程：
> $$\begin{aligned}
v_{*}(s)& =\max_{a\in\mathcal{A}(s)}q_{\pi_*}(s,a)  \\
&=\max_a\mathbb{E}_{\pi_*}[G_t\mid S_t=s,A_t=a] \\
&=\max_a\mathbb{E}_{\pi_*}[R_{t+1}+\gamma G_{t+1}\mid S_t{=}s,A_t{=}a] \\
&=\max_a\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t=s,A_t=a] \\
&\begin{aligned}=\max_a\sum_{s^{\prime},r}p(s^{\prime},r|s,a)\big[r+\gamma v_*(s^{\prime})\big].\end{aligned}
\end{aligned}$$

所以可以知道，$v_\pi$必须是 $v_*$，而且 $\pi$和$\pi'$必须都是最优策略。策略改进必须给我们一个严格变好的策略触发它已经是最优了。



##### 策略迭代
从一个策略$\pi$开始，使用 $v_\pi$来改进获得更好的策略 $\pi'$，然后计算 $v_\pi'$来改进获得更好的策略 $\pi''$，我们因此可以得到这样的一个单调改进的策略和价值函数序列：
![](images/Pasted%20image%2020231031162838.png)
每个策略保证是前一个策略的严格改进。因为有限MDP只有有限个确定策略，这个过程最终肯定会收敛到最优策略和最优价值函数。

策略迭代通常会在出人意料地少的迭代次数内收敛，策略改进定理向我们保证，这些策略比原始的随机策略要好。然而，有时候，这些策略不仅仅是更好，而且是最优的，以最少的步数前进到终止状态。


##### 价值迭代
策略迭代的一个缺点就是每次迭代都涉及到策略迭代，但这又是非常花时间的内容。那么我们能否缩减策略评估呢？
事实上，策略评估可以以很多方式缩减而不会丧失收敛保证，一个重要的例子就是只对所有状态更新一次，这样的算法是价值迭代，可以被写成一个简单的更新方程，虽然简单但是结合了策略改进和缩减的策略评估：
$$\begin{aligned}v_{\pi'}(s)\quad&=\quad\max_a\mathbb{E}[R_{t+1}+\gamma v_{\pi'}(S_{t+1})\mid S_t=s,A_t=a]\\&=\quad\max_a\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_{\pi'}(s')\Big].\end{aligned}$$
理解价值迭代的另一种方式是参考贝尔曼最优性方程(在上面的引用)。注意，价值迭代仅仅是通过将贝尔曼最优性方程转化为一个更新规则而得到的。还要注意，价值迭代更新与策略评估更新（4.5）是相同的，除了它要求在所有行动上取最大值

价值迭代在其每次扫描中有效地结合了一次策略评估扫描和一次策略改进扫描。

# 异步动态规划
DP的一个主要缺点就是它涉及到整个状态集合的操作。如果状态集合特别大，单次横扫也花费很大。

异步动态规划（DP）算法是一种就地迭代DP算法，它们不是按照系统性的状态集扫描来组织的。这些算法以任何顺序更新状态的值，使用其他状态当前可用的任何值。在更新其他状态的值一次之前，某些状态的值可能被更新了多次。然而，为了正确收敛，一个异步算法必须继续更新所有状态的值：在计算的某个点之后，它不能忽略任何状态。

当然，避免扫描并不一定意味着我们可以减少计算量。这只是意味着算法不需要在一个长时间的扫描中停滞不前就可以改进策略。我们可以尝试利用这种灵活性，选择我们应用更新的状态，以便提高算法的进展速度。我们可以尝试安排更新的顺序，让价值信息以高效的方式从一个状态传播到另一个状态。有些状态可能不需要像其他状态那样频繁地更新它们的值。如果某些状态对最优行为不相关，我们甚至可以尝试完全跳过更新它们。

我们使用术语“广义策略迭代”（GPI）来指代让策略评估和策略改进过程相互作用的一般思想，与这两个过程的粒度和其他细节无关。几乎所有的强化学习方法都可以很好地描述为 GPI。也就是说，所有这些方法都有可识别的策略和价值函数，策略总是相对于价值函数进行改进，价值函数总是被推动向着该策略的价值函数，如下图
![](images/Pasted%20image%2020231101171751.png)
如果评估过程和改进过程都稳定下来，即不再产生变化，那么价值函数和策略必须是**最优的**。价值函数只有在与当前策略一致时才会稳定，而策略只有在对当前价值函数贪婪时才会稳定。因此，当找到一个相对于其自身评估函数是贪婪的策略时，两个过程才会同时稳定下来。这意味着贝尔曼最优性方程成立，因此该策略和价值函数是最优的。
在GPI中，评估和改进过程既相互竞争又相互合作。它们之间的竞争体现在它们互相拉扯于不同方向。使策略对价值函数变得贪婪通常会使价值函数对于改变后的策略不再正确，而使价值函数与策略保持一致通常会导致该策略不再贪婪。然而，从长远来看，这两个过程相互作用，以找到一个单一的联合解决方案：最优价值函数和一个最优策略。
也可以看图理解
![](images/Pasted%20image%2020231101173843.png)

**重点：**
动态规划方法的一个特殊属性。所有这些方法都是基于对后继状态价值的估计来更新对状态价值的估计。也就是说，它们基于其他估计来更新估计。我们称这个一般性的思想为**引导法（bootstrapping）**。许多强化学习方法都执行引导法，即使它们不像动态规划那样要求完整且准确的环境模型。

在后面，我们将探索不需要模型且不使用引导法的强化学习方法，但下面的下面将探索不需要模型但需要使用引导法的强化学习方法。


### 蒙特卡洛
蒙特卡洛需要的只有经验----来自真实环境交互得到的 状态、动作、奖励的样本序列。
从实际经验中学习之所以引人注目，是因为它不需要预先了解环境动态，却仍然能够达到最优行为。

在很多情况下，我们可以比较容易地生成符合特定概率分布的样本数据（即通过模拟或实验来获取数据，这些数据遵循我们希望研究的概率分布）。但是，要直接以显式的方式（数学公式或完整数据集的形式）获取这些概率分布本身却很困难或不可行。这也是蒙特卡洛对于动态规划的一点优势，获得不了概率分布本身就用不了动态规划。简而言之，通过实验或模拟来"模拟"这些分布是可行的，但要完全详细地描述这些分布（例如，通过精确的数学公式）则通常是难以实现的。

蒙特卡洛方法是基于平均样本的回报来解决强化学习的。
为了确保能够得到明确的回报，我们在这里仅将蒙特卡洛方法定义用于情节性任务。也就是说，我们假设体验被分为多个情节，并且不管选择了什么动作，所有情节最终都会结束。只有在一个情节完成后，价值估计和策略才会发生改变。因此，蒙特卡洛方法可以在逐情节的基础上逐步增加，但并不是逐步地（在线）增加。

回想一下，一个状态的价值是预期回报——从那个状态开始的预期累积未来折现奖励。那么，一个明显的从经验中估算它的方法就是简单地平均在访问那个状态后观察到的回报。随着观察到更多的回报，平均值应该收敛到预期值。这个想法是所有蒙特卡洛方法的基础。

##### 蒙特卡洛预测
假设我们希望估计$v^\pi(s)$，即在策略$\pi$下状态$s$的价值，给定一组通过遵循$\pi$并通过$s$获得的episode，在episode中状态$s$的每次出现称为对$s$的一次访问。首次访问蒙特卡洛方法将 $v_\pi(s)$估计为首次访问 $s$后的回报平均值，而每次访问蒙特卡洛方法则平均所有访问$s$后的回报。
下面这就是首次访问的伪代码：
![](images/Pasted%20image%2020231102112256.png)


首次访问和每次访问的MC都会在访问次数趋向于无穷时，收敛到$v_\pi(s)$。
对于首次访问MC，这很容易理解，因为每个回报都是对$v_\pi(s)$的独立同分布的估计，而且具有有限的方差。根据大数定律，这些估计的平均数序列将收敛于它们的期望值


我们能否将备份图的概念推广到蒙特卡洛算法中？备份图的一般概念是在**顶部显示要更新的根节点，并在下面显示所有对更新有贡献的转换和叶节点**，包括其奖励和估计值。
那么对于 $v_\pi$的蒙特卡洛估计，根节点是一个状态节点，下面是沿着特定单一剧集的整个转换轨迹，在终止状态结束，就像右边的图![](images/Pasted%20image%2020231102165117.png)

尽管动态规划（DP）图（第59页）显示了所有可能的转换，但蒙特卡洛图仅显示了在一个剧集中采样的转换。动态规划图只包括一步转换，而蒙特卡洛图则一直到剧集的结尾。这些图中的差异准确地反映了这两种算法之间的基本区别。
![](images/Pasted%20image%2020231102165736.png)和![](images/Pasted%20image%2020231102165117.png)


蒙特卡洛一个很重要的特点就是对于每个状态的估计值都是独立的，这些估计并不需要建立在任何其他状态上，用其他话说，蒙特卡洛方法并没有 bootstrap(自举)

估计单个状态的值的计算成本与状态的数量无关。当只需要**一个或一些状态的值**时，这使得蒙特卡洛方法特别有吸引力。可以从感兴趣的状态开始生成许多样本剧集，仅对这些状态的回报进行平均，而忽略所有其他状态

如果**没有模型**，那么估计动作值（状态-动作对的值）而非状态值尤为有用。有了模型，仅通过状态值就足以确定策略；人们只需向前看一步，选择会导致最佳奖励和下一个状态的组合的动作，就像我们在关于动态规划（DP）的章节中所做的那样。没有模型，仅有状态值是不够的。为了使值在推荐策略方面有用，必须明确估计每个动作的值。因此，我们对蒙特卡洛方法的主要目标之一是估计$q_*$，为了实现这个，我们首先考虑动作价值的评估问题。
动作值的策略评估问题是估计$q_\pi(s,a)$，即从状态$s$开始，采取动作$a$，并在之后遵循策略$\pi$。对于这个问题，蒙特卡洛方法本质上与刚刚为状态值呈现的方法相同，只是现在我们讨论的是访问状态-动作对而非访问状态。与以前一样，随着每个状态-动作对的访问次数趋向于无限，这些方法将二次收敛于真正的期望值。

唯一的问题是，许多状态-动作对可能永远不会被访问。如果$\pi$是一个确定性策略，那么在遵循它的时候，只会观察到每个状态的一个动作的返回值。然后由于没有返回值可以平均，所以其他动作的蒙特卡罗估计不会随着经验而改善。这是一个严重的问题。因为学习动作值的目的是为了帮助在每个状态中选择可用的动作。为了比较替代方案，我们需要估算每个状态的所有动作的值，而不仅仅是我们当前倾向的那一个。

上面这个问题就是 如何保持探索。为了使策略评估对动作值有效，我们必须确保持续的探索。
1. 实现这一点的一种方法是规定，情节是从一个状态-动作对开始的，每个对都有一个非零的概率被选为开始。这保证了所有的状态-动作对将在无限次数的情节的极限下被访问无限次。我们称这种假设为探索开始的假设。
2. 确保遇到所有状态-动作对的最常见的替代方法是只考虑那些在每个状态中选择所有动作的非零概率的随机策略

但是现在，我们还是保留探索开始（第一个）的假设，并去完成一个完整的蒙特卡洛控制方法的呈现。

现在就是考虑如何使用蒙特卡洛估计来解决控制问题，即如何近似最优的策略。
现在就要去学习蒙特卡洛估计，去近似最优策略。总的想法是按照与动态规划（DP）章节相同的模式进行，即按照**广义策略迭代GPI**的想法进行。价值函数会不断迭代使其更加精确地近似对应当前策略的真实价值函数，而当前的策略也会根据当前的价值函数不断调优。它们整体会趋向最优解。
![](images/Pasted%20image%2020231117215656.png)
在广义策略迭代的框架下，策略评估和策略改进交替进行。在策略评估阶段，使用蒙特卡洛方法来估计给定策略下状态-动作对的值。然后，在策略改进阶段，基于这些估算值来改进策略。这个过程反复进行，直到策略收敛到最优策略

在开始，让我们考虑蒙特卡洛版本的经典策略迭代。我们用一个随机策略$\pi_0$开始，交替完成策略评估和策略改进，，以一个最优策略和最优动作价值函数结束：
![](images/Pasted%20image%2020231102203003.png)
那么蒙特卡洛版本的策略评估完全按照前一节所述的方法进行，经历了很多episodes后，近似的动作价值函数会渐进地趋向真实的动作价值函数。然后，现在让我们假设我们能观察到无限个episodes，每个episode都是用探索开始的方法生成的，在这些假设下，蒙特卡洛方法会准确的计算每一个 $q_{\pi_k}$，对于任意的策略$\pi_k$ 。
然后策略改进是在当前价值函数上贪心地选择动作。在这种情况下，我们有一个动作-价值函数，因此不需要模型来构建贪婪策略。对于任何动作-价值函数 q，相应的贪婪策略是确定性地为每个 s ∈ S 选择具有最大动作价值的动作的策略
$\pi(s)\doteq\arg\max_aq(s,a)$
策略改进可以通过构造每一个$\pi_{k+1}$ 作为相对于 $q_{\pi_k}$ 的贪婪策略来完成，然后又通过前面学习知道 $\pi_{k+1}$一定是好于 $\pi_k$的，或者是至少一样好。
通过这种方式蒙特卡洛方法可以被用来寻找最优策略而不需要给出环境的动态性。

为了容易获得蒙特卡洛方法的收敛保证，我们在上面做了两个**不太可能**的假设。一个是假设情节有探索性的开始，另一个是假设策略评估可以通过无限数量的情节来完成。为   了获得实用的算法，我们将不得不消除这两个假设。我们将推迟对第一个假设的考虑，直到本章稍后。

目前我们关注的假设是策略评估基于无限数量的情节。这个假设相对容易移除： 
这就需要做一些假设并定义一些测度，来分析逼近误差的幅度和出现概率的上下界，然后采取足够多的步数来保证这些界足够小。这种方法可能完全可以在保证正确收敛到某种近似水平的意义上令人满意。然而，它可能也需要太多的情节，除了最小的问题之外，在实践中可能不太实用。
第二种方法去避免无限个episode，就是在返回策略改进之前放弃尝试完成策略评估，在每次评估中，我们将价值函数朝着$q_{\pi_k}$改变，但是除了经过许多步骤之外，我们不期望实际上接近它。   这个想法的一种极端形式是值迭代，在其中每个策略改进步骤之间只执行一次迭代策略评估的迭代。值迭代的原地版本更为极端；在那里，我们对单个状态的改进和评估步骤进行交替。


##### 蒙特卡洛ES
对于蒙特卡洛策略迭代，很自然地会在每个情节的基础上交替进行评估和改进。在每个情节之后，观察到的返回用于策略评估，然后在情节中访问的所有状态处改进策略。沿着这些线路，我们有一个完整简单的算法，我们称之为**蒙特卡洛ES**，即Monte Carlo with Exploring Starts，其伪代码： 
![](images/Pasted%20image%2020231102210133.png)

在蒙特卡洛ES中，对于每个状态-动作对都被累积和平均，而不管它们被观察到时的策略是什么。
很容易看出，蒙特卡洛ES不能收敛到任何次优策略。如果它做到了，那么值函数最终会收敛到该策略的值函数，而这反过来又会导致策略发生变化。只有当策略和值函数都是最优时，才能实现稳定性。随着时间的推移，动作-值函数的变化减少，收敛到这个最优固定点似乎是不可避免的，但尚未得到正式证明。在我们看来，这是强化学习中最基本的开放理论问题之一


  
避免探索开始的不切实际的假设的方法是什么呢？确保所有的动作都被无限次地选中的唯一一般方法是让代理持续不断地选择它们。为了确保这一点，有两种方法，这也造成了我们所说的 on-policy 的方法和 off-policy 的方法。
on-policy 方法试图去评估或改进 用来进行做决定的策略，但是off-policy就是评估和改进一个与生成数据策略不一样的策略。
蒙特卡洛ES方法就是一种on-policy方法。在下面我们讲一下不使用探索开始这一不切实际假设，而且是on-policy的蒙特卡罗控制方法
在on-policy控制方法中，策略是 soft的，也就是说对于 所有$a∈A , s∈S$,都有 $\pi(s|a)>0$  ，但是会慢慢地朝着确定性的策略转移。
在这里讲的 on-policy方法是使用$\varepsilon\operatorname{-}greedy$策略，也就是说大部分时间都选择最大动作，就是有最大的估计动作价值，但小概率$\varepsilon$随机选择。所以，非贪婪动作的概率都是： $\frac\varepsilon{|\mathcal{A}(s)|}$ ，贪婪动作的概率是： $1-\varepsilon+\frac\varepsilon{|\mathcal{A}(s)|}$ 


现在没有探索开始的假设，我们不能简单地通过使其相对于当前的价值函数变得贪婪来改进策略，因为这会阻止对非贪婪动作的进一步探索。幸运的是，GPI 不要求策略完全变成贪婪策略，只要求它朝着贪婪策略的方向移动。在我们的基于策略的方法中，我们只会将其移动到 "$ε$-贪婪" 策略 。对于任何 $\epsilon-soft$策略$\pi$，任何$\epsilon-greedy$来对$q_\pi$的策略都保证好于或是等于策略$\pi$ ,伪代码如下：
![](images/Pasted%20image%2020231102214148.png)

任何相对于 $q_\pi$的 "$ε$-贪婪" 策略相比任何 "ε-软" 策略 $π$ 都有所改善,这是由策略改进定理所保证的。让$\pi'$作为$\epsilon-greedy$策略，那么对于任意 $s∈S$，都有：
$$\begin{aligned}
q_{\pi}(s,\pi'(s))& \begin{aligned}=\sum_a\pi'(a|s)q_\pi(s,a)\end{aligned}  \\
&=\frac\varepsilon{|\mathcal{A}(s)|}\sum_aq_\pi(s,a)+(1-\varepsilon)\max_aq_\pi(s,a) \\
&\begin{aligned}\geq&\frac\varepsilon{|\mathcal{A}(s)|}\sum_aq_\pi(s,a)~+~(1-\varepsilon)\sum_a\frac{\pi(a|s)-\frac\varepsilon{|\mathcal{A}(s)|}}{1-\varepsilon}q_\pi(s,a)\end{aligned} \\
&=\frac{\varepsilon}{|\mathcal{A}(s)|}\sum_{a}q_{\pi}(s,a)-\frac{\varepsilon}{|\mathcal{A}(s)|}\sum_{a}q_{\pi}(s,a)+\sum_{a}\pi(a|s)q_{\pi}(s,a) \\
&=v_{\pi}(s).
\end{aligned}$$
因此，通过上面这个式子，我们可以得到 $\pi' \ge \pi$，因为对于所有的$s∈S$，都有$v_\pi'(s)\ge v_\pi(s)$ 。


考虑一个新的环境，它与原始环境完全相同，除了将 "$ε$-软" 策略的要求“移入”环境中。新环境具有与原始环境相同的动作和状态集，并按照以下方式行为。如果处于状态 $s$ 并采取动作 $a$，那么以概率 $1−ε$，新环境的行为完全像旧环境一样。以概率 $ε$，它会随机重新选择动作，每个动作的概率相等，然后用新的随机动作像旧环境一样行为。在这个新环境中，使用一般策略所能做到的最好情况与在原始环境中使用 "$ε$-软" 策略所能做到的最好情况相同。
然后让 $\widetilde{v}_{*}$和$\widetilde{q}_{*}$ 都作为这个新环境的最优价值函数，那么一个策略 $\pi$ 属于$\epsilon-soft$时当且仅当 $v_\pi=\widetilde{v}_{*}$ 时，策略$\pi$是最优的,我们知道$\widetilde{v}_{*}$是会更改状态转移概率的贝尔曼最优方程的唯一解
$$\begin{aligned}
\widetilde{v}_{*}(s)& \begin{aligned}&=\max_a\sum_{s',r}\bigg[(1-\varepsilon)p(s',r|s,a)+\sum_{a'}\frac{\varepsilon}{|\mathcal{A}(s)|}p(s',r|s,a')\bigg]\bigg[r+\gamma\widetilde{v}_*(s')\bigg]\end{aligned}  \\
&=(1-\varepsilon)\max_a\sum_{s',r}p(s',r|s,a)\Big[r+\gamma\widetilde{v}_*(s')\Big] \\
&\begin{array}{ccc}&+&\frac{\varepsilon}{\left|\mathcal{A}(s)\right|}\sum_{a}\sum_{s^{\prime},r}p(s^{\prime},r|s,a)\Big[r+\gamma\widetilde{v}_{*}(s^{\prime})\Big].\end{array}
\end{aligned}$$
当等式成立且$\epsilon-soft$策略$\pi$不再改进时，我们也可以知道$v_\pi$和$q_\pi$的关系式：
$$\begin{aligned}
v_{\pi}(s) \begin{aligned}&=\quad(1-\varepsilon)\max_aq_\pi(s,a)+\frac{\varepsilon}{|\mathcal{A}(s)|}\sum_aq_\pi(s,a)\end{aligned}  \\
\begin{aligned}=\quad(1-\varepsilon)\max_a\sum_{s^{\prime},r}p(s^{\prime},r|s,a)\Big[r+\gamma v_\pi(s^{\prime})\Big]\end{aligned} \\
+\quad\frac\varepsilon{|\mathcal{A}(s)|}\sum_{a}\sum_{s^{\prime},r}p(s^{\prime},r|s,a)\Big[r+\gamma v_{\pi}(s^{\prime})\Big]
\end{aligned}$$
然而，这个等式是和前面那个等式是相同的，这是因为$\widetilde{v}_{*}$是唯一的解，必须有 $v_{\pi}= \widetilde{v}_{*}$


在本质上，上面几段中我们已经展示了策略迭代对$\epsilon-soft$策略是有效的。使用对$\epsilon-soft$的贪婪策略的自然概念，人们可以确保在每一步都得到改进，除非已经在$\epsilon-soft$中找到了最佳策略
现在我们只能在"$\epsilon-soft$策略"中实现最佳策略，但另一方面，我们消除了探索起点的假设。



### 重要性采样
所有的学习控制方法都面临一个困境，它们试图学习基于后续最优行为的动作值，但为了探索所有动作（以找到最优动作）它们需要非最优地行动。它们如何在按照探索策略行动时学习到关于最优策略的信息呢？
on-policy策略只是做了一个妥协，它不是为最优策略学习动作值，而是为仍然具有探索性的近乎最优策略学习动作值
一个更直接的方法是使用两个策略，一个是被学习并成为最优策略的，另一个则更具探索性并用于生成行为。被学习的策略称为目标策略，用于生成行为的策略称为行为策略。在这种情况下，我们说学习是来自“偏离”目标策略的数据，整个过程被称为偏离策略（off-policy）学习。

on-policy方法通常更简单，因此首先被考虑。off-policy方法需要额外的概念和符号表示，由于数据来自不同的策略，off-policy 方法通常具有更大的方差，收敛速度较慢。另一方面，off-policy方法更强大和通用。


现在考虑一个预测问题来学习off-policy方法，在这里目标策略和行为策略都是固定的。也就是说，假设我们要估计 $v_\pi$或者是$q_\pi$ ，但是我们有的episodes都是遵循着另一个策略b得到的，且$b!\neq\pi$ ，那么$\pi$就是目标策略，$b$就是行为策略，两者都是固定的，也给定了。

为了使用来自$b$的episodes去估计$\pi$的价值，我们需要让$\pi$策略会执行的动作在策略$b$下也被执行。那就是说，如果$\pi(a|s)>0$，那么$b(a|s)>0$ ,这个叫做**覆盖假设**。
从覆盖中我们可以得出，对于那些与 $π$ 不完全相同的状态，$b$ 必须是随机的。另一方面，目标策略 $π$ 可以是确定性的。
在控制中，目标策略通常是相对于当前动作值函数估计的确定性贪婪策略。这个策略变成一个确定性的最优策略，而行为策略保持随机并且更具探索性。然而，在这一部分中，我们考虑预测问题，其中 $π$ 是不变的和给定的。

几乎所有的偏离策略（off-policy）方法都利用了重要性采样（importance sampling），这是一种在给定来自**另一个分布的样本**时**估计**一个分布下**期望值**的通用技术。我们通过根据目标策略和行为策略下轨迹发生的相对概率来对返回值进行加权，应用重要性采样于偏离策略学习，这被称为重要性采样比率（importance-sampling ratio）。通过这种方式，重要性采样允许我们从行为策略中采集的数据来估计目标策略的期望值。
给定一个开始状态 $S_t$，接下来的状态-动作轨迹 $A_t,S_{t+1},A_{t+1},\ldots,S_T$在任何策略$\pi$下发生的概率是： 
$$\begin{aligned}\Pr\{A_t,S_{t+1},A_{t+1},\ldots,S_T\mid S_t,A_{t:T-1}\sim\pi\}\\&=\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\pi(A_{t+1}|S_{t+1})\cdots p(S_T|S_{T-1},A_{T-1})\\&=\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k),\end{aligned}$$
这里的$p$就是状态转移概率函数。因此，在目标策略和行为策略下轨迹的相对概率（重要性抽样比率）是
$$\rho_{t:T-1}\doteq\frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k)}=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}.$$
  虽然轨迹概率依赖于MDP（马尔可夫决策过程）的转移概率，这些通常是未知的，但它们在分子和分母中都以相同的方式出现，因此会**相互抵消**。最终，重要性抽样比率仅依赖于两个策略和序列，而不依赖于MDP。

回想一下我们希望去估计目标策略的期望回报（即values），但是我们拥有的是根据行为策略得到的回报 $G_t$ ，这些回报有错误的期望值 $\mathbb{E}[G_t|S_t{=}s]=v_b(s)$，也无法平均去得到 $v_\pi$ 。
这里就是重要性采样发挥作用的地方了， 这个比率$\rho_{t:T-1}$把回报转化成有正确的期望值： 
$$\mathbb{E}[\rho_{t:T-1}G_t\mid S_t=s]=v_\pi(s)$$
现在我们准备使用蒙特卡洛算法去通过平均一批按策略b观察到的情节的回报来估算$v_\pi$，在这里，以一种能够跨越情节边界递增的方式对时间步进行编号是方便的。也就是说，如果这批次的第一个情节在时间100结束于一个终止状态，那么下一个情节将在时间 t = 101 开始。这使我们能够使用时间步编号来引用特定情节中的特定步骤。
具体来说，
我们可以定义访问状态$s$的所有时间步的集合，表示为![](images/Pasted%20image%2020231103110826.png)，这是每次访问方法，如果是首次访问方法，$\mathcal{T}(s)$只会包含那些在一个episode中首次访问状态$s$的时间步。然后，
让 $T(t)$表示在时间 $t$ 之后的第一次终止时间，
让 $G_t$ 表示从 $t$ 到 $T(t)$的回报
![](images/Pasted%20image%2020231103110845.png)表示所有与状态 $s$ 相关的回报
![](images/Pasted%20image%2020231103110835.png)包含了相应的重要性抽样比率。


为了去估计 $v_\pi(s)$，我们简单地按比率缩放回报，然后对结果求平均
![](images/Pasted%20image%2020231103111059.png)
当以这种简单平均的方式进行重要性抽样时，它被称为普通重要性抽样。

一个重要的替代方法是加权重要性抽样，它使用加权平均值，被定义为
![](images/Pasted%20image%2020231103111430.png)

为了理解重要性抽样的这两种类型，考虑在观察到状态s的单个回报后，它们的首次访问方法的估计。在加权平均估计中，单个回报的比率$\rho_{t:T(t)-1}$在分子和分母中抵消，使得估计值等于观察到的回报，而不依赖于比率（假设比率非零）。鉴于这个回报是唯一观察到的，所以这是个合理的估计，但是它的期望值是等于 $v_b(s)$而不是$v_\pi(s)$ ，也就是说在统计学的意义上是有偏差的。
对比而言，普通重要性抽样估计器总是把 $v_\pi(s)$当作期望值（它是无偏的），但是他却可能会很极端。假设比率是十，这表明在目标策略下观察到的轨迹的可能性是在行为策略下的十倍。在这种情况下，普通重要性抽样估计将是观察到的回报的十倍。也就是说，尽管该情节的轨迹被认为非常能代表目标策略，但它会与观察到的回报相差很远。

这两种重要性抽样的首次访问方法之间的区别体现在它们的偏差和方差上。普通重要性抽样是无偏的，而加权重要性抽样是有偏的（尽管偏差会渐进地收敛到零）
另一方面，普通重要性抽样的方差通常是无界的，因为比率的方差可能是无界的，而在加权估计器中，任何单个回报上的最大权重是一
实际上，在假设回报有界的情况下，即使比率本身的方差是无限的，加权重要性抽样估计器的方差也会收敛到零。在实践中，加权估计器通常具有明显较低的方差，因此是强烈推荐的。

普通和加权重要性抽样的每次访问方法都是有偏的，尽管再次强调，随着样本数量的增加，偏差会渐进地收敛到零。在实践中，每次访问方法通常更受欢迎，因为它们消除了记录哪些状态已被访问的需求，并且它们更容易扩展到近似方法。

我们可以先看一下对于一个随机变量的方差公式：
$$\operatorname{Var}[X]\doteq\mathbb{E}\Big[\left(X-\bar{X}\right)^2\Big]=\mathbb{E}\Big[X^2-2X\bar{X}+\bar{X}^2\Big]=\mathbb{E}\Big[X^2\Big]-\bar{X}^2$$
因此，如果均值是有限的，那么方差如果是无限的情况，那就只能是前者无限了。所以把前者列出来：
$$\mathbb{E}\left[\left(\prod_{t=0}^{T-1}\frac{\pi(A_t|S_t)}{b(A_t|S_t)}G_0\right)^2\right]$$
也就是他是无限的。


  
在普通的重要性抽样中，回报按照重要性抽样比率$\rho_{t:T-1}$进行缩放，然后进行简单地求平均。对于这些方法，我们可以再次使用第2章的增量方法，但是用缩放后的回报来代替那一章的奖励。在这里，我们必须形成回报的加权平均，而且需要一个略有不同的增量算法。
假设我们有一个回报的序列 $G_1,G_2,...G_{n-1}$，这些都开始于同一个状态而且每个都与一个随机权重$W_i$相对应，比如$W_i=\rho_{t_i:T(t_i)-1}$，我们希望把估计形式化为：
$$V_n\doteq\frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k},\quad n\geq2,$$
然后为了让它在我们得到单个额外的回报$G_n$之后更新，根据增量更新的原理，变成：
$$V_{n+1}\doteq V_n+\frac{W_n}{C_n}\Big[G_n-V_n\Big],\quad n\ge1,$$
其中$$C_{n+1}\doteq C_n+W_{n+1},$$
且$C_{0}= 0$ 
下面的伪代码是一个用于蒙特卡洛策略评估的完整的逐回合增量算法。这个算法名义上适用于off-policy情况，使用加权重要性采样，但是通过选择目标策略和行为策略为相同的情况（即$\pi=b$，$W$总是1)，这个近似$Q$会收敛到 $q_\pi$，（对于所有遇到的状态-动作对而言），即使动作是根据不同的策略$b$选择的




#### off-policy蒙特卡洛控制
在off-policy中，生成行为的策略，叫行为策略（behavior policy），可能和用于评估和改进的目标策略（target policy）是没有关系的。这样的一种优势就是目标策略可以是确定性的（例如，贪婪），但是行为策略还是可以继续去采样任何可能的动作。

离线策略蒙特卡洛控制方法使用前面提出的技术之一。它们在学习和改进目标策略的同时，遵循行为策略。这些技术要求行为策略必须有选择目标策略可能选择的所有行动的非零概率（覆盖）。为了探索所有可能性，我们要求行为策略是soft的（即它在所有状态下选择所有行动的概率都是非零的）

下面框中的代码就是off-policy的蒙特卡洛控制方法，基于GPI、加权重要性采样，来评估 $\pi_*$和$q_*$。目标策略$\pi\approx\pi_*$是一个和$Q$值相关的贪心策略，估计$q_\pi$。这个行为策略$b$可以是任意的，但是为了确保$\pi$一定可以收敛到最优策略，每个状态-动作对都必须有无限个数的回报。这个可以通过让策略$b$变成$\varepsilon-soft$. 即使根据可能在各个剧集之间甚至在剧集内部变化的不同软策略$b$选择行动，策略$π$也会在所有遇到的状态下收敛到最优。
![](images/Pasted%20image%2020231108110017.png)

一个潜在的问题就是这个方法只能在episodes尾部学习，而且是当剧集中剩余的所有行为都是贪婪的时候。但是如果非贪婪的动作很常见，那么学习就会很慢，特别是对于在长剧集早期部分出现的状态。对于离线策略蒙特卡洛方法的经验不足以评估这个问题有多严重。如果问题确实严重，解决它的最重要方法可能是结合时间差分学习，这是在后面中发展的算法思想。

在设计蒙特卡洛控制方法时，我们遵循了广义策略迭代（GPI）的总体框架。GPI涉及策略评估和策略改进的相互作用过程。蒙特卡洛方法提供了一种替代的策略评估过程。它们不是使用模型来计算每个状态的价值，而是简单地平均许多从该状态开始的回报。因为一个状态的价值是预期回报，这个平均值可以成为价值的好近似。在控制方法中，我们特别感兴趣的是近似动作-价值函数，因为这些可以用来改进策略，而不需要环境转移动态的模型。
在蒙特卡洛控制方法中，保持足够的探索是一个问题。仅仅选择当前估计为最佳的行动是不够的，因为这样将不会为其他选择获得回报，可能永远也无法了解到这些选择实际上可能更好。一种方法是忽略这个问题，假设情节以随机选择的状态-行动对开始，以覆盖所有可能性。这种探索起点在具有模拟情节的应用中有时可以安排，但在从真实经验中学习时不太可能。在在线策略方法中，代理承诺始终进行探索，并试图找到仍然进行探索的最佳策略。在离线策略方法中，代理同样进行探索，但学习一个确定性的最优策略，这可能与遵循的策略无关。
离线策略预测指的是从由不同行为策略生成的数据中学习目标策略的价值函数。这种学习方法基于某种形式的重要性抽样，即通过取观察到的行动在两个策略下的概率之比来加权回报，从而将它们的期望值从行为策略转换为目标策略。普通重要性抽样使用加权回报的简单平均值，而加权重要性抽样使用加权平均值。普通重要性抽样产生无偏估计，但具有更大的、可能是无限的方差，而加权重要性抽样始终具有有限的方差，在实践中更受青睐。尽管它们在概念上很简单，离线策略蒙特卡洛方法在预测和控制方面仍然未定，是持续研究的主题。
本章讨论的蒙特卡洛方法与前面的动态规划方法有两个主要区别。首先，它们作用于样本经验，因此可以直接学习，无需模型。其次，它们不自举。也就是说，它们不基于其他价值估计来更新自己的价值估计。这两个区别并不紧密相连，可以分开。



### TD差分
如果要确定强化学习中一个核心且新颖的概念，那无疑是时序差分（Temporal Difference, TD）
TD学习是蒙特卡洛方法和动态规划（DP）方法的结合。像蒙特卡洛方法一样，TD方法可以直接从原始经验中学习，无需对环境动态的模型。也像DP一样，TD方法基于部分学习到的估计进行更新，而不用等待最终结果（它们使用自举bootstrap）。

像之前一样，我们首先关注策略评估或者说预测问题，也就是估计给定策略$\pi$的价值函数$v_\pi$的问题。**对于控制问题（找到最优策略），DP、TD和蒙特卡洛方法都使用某种通用策略迭代（GPI）的变体。方法之间的主要区别是它们对预测问题的处理方式。**

TD和蒙特卡洛方法都使用经验来解决预测问题，给定一些遵循策略$\pi$的经验，两种方法都更新他们对于$v_\pi$的估计$V$,只要这些状态$S_t$都发生在这个经验中。粗略的说，蒙特卡洛方法会一直等，直到这个某个状态的回报被知道为止，然后就会使用这个回报作为 $V(S_t)$的目标。一个适用于不平稳的环境的简单的每次访问蒙特卡洛方法是：$$V(S_t)\leftarrow V(S_t)+\alpha\Big[G_t-V(S_t)\Big]$$
这里的$G_t$是在时间$t$真实回报，$\alpha$是一个步长参数的常量。这个方法就叫做常数$\alpha$-MC
尽管蒙特卡洛方法必须等到一个episode的结束才能决定$V(S_t)$的增量，因为只有那时$G_t$才知道，
TD方法则只需要等到下一个time step就可以了。在时间步$t+1$,他们就立刻形成一个目标并且做一个有效更新，通过使用观察到的回报$R_{t+1}$和估计$V(S_{t+1})$，最简单的TD方法使用这个更新：$$$V(S_t)\leftarrow V(S_t)+\alpha\Big[R_{t+1}+\gamma V(S_{t+1})-V(S_t)\Big]$$
只要转换到$S_{t+1}$和接收到$R_{t+1}$，就进行更新。对于蒙特卡洛更新来说，目标是$G_t$，但是这里TD的更新是 $R_{t+1}+\gamma V(S_{t+1})$，这种TD方法叫做 $TD(0)$, 也叫做一步TD。流程图如下：
![](images/Pasted%20image%2020231108150350.png)

因为TD（0）的更新部分基于现有的估计，所以我们说它是一种自举方法，就像DP一样。

我们在之前学到了 $v_\pi$的定义式：
$$\begin{aligned}
v_{\pi}(s)& \doteq\mathbb{E}_\pi[G_t\mid S_t{=}s]  \\
&=\mathbb{E}_\pi[R_{t+1}+\gamma G_{t+1}\mid S_t{=}s] \\
&=\mathbb{E}_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s].
\end{aligned}$$
粗略地说，蒙特卡洛方法使用第一个等式估计作为目标，DP则是使用第三个等式作为目标。蒙特卡洛目标是一个估计值，因为第一个等式中的预期值未知；使用样本回报代替实际预期回报。DP目标也是估计，但是不是因为假设完全由环境模型提供的期望值而进行的估计，而是因为 $v_\pi(S_{t+1})$并不知道，从而使用了当前的估计$V(S_{t+1})$  。
TD的目标也是一个估计值因为下面这两个原因，它采样了期望的值，而且使用了当前的估计$V$而不是真实的$v_\pi$ ， 因此，TD方法结合了蒙特卡洛的采样和DP的自举bootstraping。
![](images/Pasted%20image%2020231108152236.png)这个图就是表格型TD(0)的备份图。在备份图中顶部的状态的价值估计是通过从它到紧接着的状态的一个样本转换来更新的。
我们把TD和蒙特卡洛更新叫做样本更新，因为他们涉及到向前去看一个采样的继任状态（或是状态-动作对），使用继任状态的值和跟着这条路得到奖励去计算一个备份的值，然后更新这个初始状态的值。
采样更新和DP方法的期望更新不同之处在于，它们基于单个采样继任者，而不是基于所有可能继任者的完整分布。

最后，要注意TD(0)更新的数量代表一种错误，衡量了$S_t$的估计值与和更好的估计$R_{t+1}+\gamma\bar{V}(S_{t+1})$之间的差距。这个数量叫做TD误差，在整个过程中以各种形式出现
在强化学习中
$$\delta_t\doteq R_{t+1}+\gamma V(S_{t+1})-V(S_t)$$
每次的TD误差都是当时估计的误差。因为TD错误取决于下一个状态和下一个奖励，所以直到一个时间步长后它才真正可用。
也要注意如果数组$V$在episode期间并没有改变，那么蒙特卡洛错误可以被写成TD误差的总和：
![](images/Pasted%20image%2020231108161133.png)
不过这个东西在$V$在episode期间更新时是不准确的，比如TD(0)。但是如果这个步长足够小的话，最后的等式还是可以保持近似的。该恒等式的推广在时差学习的理论和算法中起着重要作用


#### TD预测方法的优势
TD方法更新估计是部分基于其他的估计的。他们从一个猜测学习到另一个猜测，这也叫做bootstrap自举。
***那么这是一个好事吗？
TD方法相比MC和DP方法又好在哪里呢？***
我们将简要介绍一些答案。

很明显，TD方法比DP好的优势就是它不需要完整的环境模型，即reward和下个状态的概率分布。
TD方法相比蒙特卡洛最明显的优势就是，时间差分（TD）方法相对于蒙特卡罗方法可以自然地在线实现，完全增量式地执行。使用蒙特卡罗方法，必须等到一个剧情的结束，因为只有到那时才知道回报是多少，而使用TD方法，只需等待一个时间步长。这一点出人意料地经常变得至关重要

TD方法真的可靠吗？当然，不用等待实际结果就从下一个猜测中学习是很方便的，但我们还能保证收敛到正确答案吗？令人高兴的是，答案是肯定的。如果步长参数按照通常的随机逼近条件（下图）递减，那么收敛的概率为1。如果步长参数是常数且足够小的话，在平均意义上会收敛。
![](images/Pasted%20image%2020231109091948.png)

如果TD方法和蒙特卡罗方法都能渐进地收敛到正确的预测上，那么一个自然的下一个问题是“哪一个到达得更快？”换句话说，哪种方法学习得更快？哪一种对有限数据的利用更高效？目前，这是一个开放性问题，因为没有人能够数学上证明一种方法比另一种收敛得更快。


#### TD（0）的优化
假设这里只有有限的经验能被利用，比如说10个或100个时间步。在这种情况下，增量学习方法的一个常见做法是反复呈现经验，直到方法收敛于一个答案。
给定一个近似价值函数$V$，按照$V(S_t)\leftarrow V(S_t)+\alpha\Big[R_{t+1}+\gamma V(S_{t+1})-V(S_t)\Big]$指定的增量在每个访问非终止状态的时间步$t$处都会被计算出来，但只通过所有增量的总和更新值函数一次。
然后，所有可用的经验再次用新的值函数处理，以产生一个新的总增量，依此类推，直到值函数收敛。我们称这种更新为**批量更新**，因为只有在处理完每一批训练数据之后才进行更新。
如果使用了批量更新，那么步长$\alpha$只有足够小的情况下TD(0)才能收敛到一个与$\alpha$无关的一个值。MC也可以收敛，但是不是同一个值。能够理解两个值的区别也就能理解两种方法的区别了。在正常更新下，方法不会一直移动到各自的批量答案，但在某种意义上，它们会朝着这些方向采取步骤。
现在给个例子来分析下这两种方法，假设我们观察到8个这样的episodes：
![](images/Pasted%20image%2020231109172052.png)
第一个episode是从状态A开始，转移到B获得0，然后从B到终止状态，且获得了0的奖励。其他的episode都是从状态B开始然后就立即结束了。
那么给出这样的episodes，我们可以很容易判断出$V(B)$是$\frac{3}{4}$ ，因为有6次立即终止且得到了奖励1，2次立即终止得到奖励0。
但是根据这些数据，估计 V (A) 的最佳值是什么？这里有两个合理的回答。一个是观察到每次状态A都会立即转变到B，而且reward是0，又因为已知B的值是$\frac{3}{4}$，所以A也应该是$\frac{3}{4}$。这个回答首先是基于对马尔可夫过程的建模，就像下面展示的那样，然后根据模型计算出正确的估计值。确实，在这个案例中，这样计算得出$V(A)=\frac{3}{4}$。这同样也是TD(0)会给出的答案。![](images/Pasted%20image%2020231110141204.png)

另一个合理的答案是简单地观察我们已经见过A一次，其后的回报是0，因此我们估计V(A)为0。这是批量蒙特卡罗方法给出的答案，注意，这也是在训练数据上给出最小平方误差的答案。事实上，它在数据上给出了零误差。但我们仍然期望第一个答案会更好。如果这个过程是马尔可夫的，我们期望第一个答案在未来数据上产生的误差会更低，尽管蒙特卡罗答案在现有数据上表现更好。


上面这个例子展示了批量TD(0)和批量蒙特卡罗方法找到的估计之间的一般差异。
批量蒙特卡洛方法总是找出最小化训练集上均方误差的估计，而批量 TD(0) 总是找出完全符合马尔可夫过程模型的最大似然估计参数。通常，一个参数的最大似然估计是使得生成训练数据的概率最大的参数值。在这个例子中，马尔可夫过程模型参数的最大似然估计可以很直观地从观察到的多幕序列中得到。从i到的转移概率估计值，就是观察数据中从i出发转移到，的次数占从出发的所有转移次数的比例。而相应的期望收益则是在这些转移中观察到的收益的平均值。我们可以据此来估计价值函数，并且如果模型是正确的，则我们的估计也就完全正确。这种估计被称为确定性等价估计，因为它等价于假设潜在过程参数的估计是确定性的而不是近似的。批量 TD(0) 通常收敛到的就是确定性等价估计。
这一点也有助于解释为什么TD 方法比蒙特卡洛方法更快地收敛。在以**批量的形式**学习的时候，TD(0)比蒙特卡洛方法更快是因为它计算的是真正的确定性等价估计。这就解释了在随机游走任务(见图 6.2)的批量学习结果中TD(0) 为什么显示出优势。与确定性等价估计的关系也可以在一定程度上解释非批量 TD(0) 的速度优势。尽管非批量TD(0)并不能达到确定性等价估计或最小平方误差估计，但它大致朝着这些方向在更新，因此它可能比常数  MC 更快。目前对于在线 TD和蒙特卡洛方法的效率的比较还没有明确的结论。

但是，值得注意的是，虽然确定性等价估计从某种角度来说是一个最优答案，但是不可能直接计算，如果 $n=|S|$是状态数，那么至少建立最大似然估计就需要 $n^2$的内存，相比之下，TD方法可以使用不超过n的内存，并且通过再训练集上反复计算来逼近同样的答案。对于状态空间巨大的任务,TD方法可能是唯一可行的逼近确定性等价解的方法。



##### SARSA
我们现在转去使用TD预测方法来解决控制问题，就像之前一样，我们遵循GPI的模式，只在评估或者说预测部分使用TD方法。就像在MC方法中，我们也面临着试探和利用的权衡，那么这样又区分成了两类：on-policy和off-policy。这里先讲一下同策略TD控制方法。

第一步是去学习一个动作价值而不是一个状态价值函数。而且，对于一个on-policy的方法来说，我们必须对于当前的策略$\pi$中的所有状态$s$和动作$a$估计他们的 $q_\pi(s,a)$。这个估计可以使用之前估计$v_\pi$同样的方法。回想一下，一个episode包含了一个动作和状态-动作对的交替：
![](images/Pasted%20image%2020231110151727.png)
在前面我们考虑状态和状态的转移，然后从状态的值里面学习，这里我们考虑状态-动作对的转移，并从状态-动作对里面学习。虽然在形式上是相同的，都是一个带有reward的马尔科夫链。保证TD(0)的状态价值可以收敛的定理同样也可以应用到动作价值上：
$$Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\Big[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\Big]$$
从一个$S_t$转移状态后就会进行一次更新。如果 $S_{t+1}$是终止状态，那么 $Q(S_{t+1},A_{t+1})$被定义成0，这个更新规则使用到了五元组： $(S_t,A_t,R_{t+1},S_{t+1},A_{t+1})$，组成了一个转移状态，从一个状态-动作对到下一个。这个五元组也是SARSA名字的由来，SARSA的备份图如下
![](images/Pasted%20image%2020231110155951.png)

基于TD方法设计一个on-policy的控制算法是很直接的，就像在所有的on-policy方法中，我们持续地估计一个策略$\pi$的 $q_\pi$，同时让$\pi$根据$q_\pi$朝着贪心优化的方向改变。SARSA控制算法的通常形式在下面的框中给出
![](images/Pasted%20image%2020231110160218.png)
Sarsa算法的收敛性质是取决于策略对价值Q的依赖程度。例如，可以使用ε-贪婪或ε-软策略。在通常的步长条件（如下）下
![](images/Pasted%20image%2020231110160951.png)
只要所有的状态-动作对都被无限次访问，并且策略最终收敛到贪婪策略（例如，通过将ε设置为1/t来安排ε-贪婪策略），Sarsa算法就有概率1收敛到最优策略和动作价值函数。


##### Q-learning
强化学习早期的一大突破就是Q学习的发展，被定义为：
$$Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\Big[R_{t+1}+\gamma\max_aQ(S_{t+1},a)-Q(S_t,A_t)\Big]$$
在这里，被学习的动作价值函数，$Q$,是直接近似到$q_*$，也就是最优动作价值函数，与被遵循的策略$\pi$没有关系。这大大简化了对算法的分析，并且使得早期的收敛证明成为可能。策略实际上仍然有影响，因为它决定了哪些状态-动作对被访问和更新。
然而，能够正确收敛，需要的只有所有的状态-动作对都持续地更新。就像我们在第五章看到的，这是任何方法保证找到最优智能体的最低要求。 基于这种假设以及步长参数的某个常用的随机近似条件，就可以证明Q 一定能以1的概率收敛到 $q_*$，Q学习算法如下
![](images/Pasted%20image%2020231110163125.png)
那么，Q学习的备份图是什么呢？根据上面的更新规则，更新的根节点，一定是一个动作节点，然后更新也是来源于动作节点，最大化在下个状态的所有可能的动作，因此，底部的节点肯定也是动作节点，然后最大化这个动作，需要在每条线直接有一条弧。于是备份图如下：
![](images/Pasted%20image%2020231110163303.png)


##### expected Sarsa
考虑一个学习算法，和Q学习很像，但是除了最大化下一个状态的动作，而是去使用一个期望值，考虑一下当前策略下每个动作是怎么做的。这样的算法的更新规则如下：
$$\begin{aligned}Q(S_t,A_t)&\leftarrow Q(S_t,A_t)+\alpha\Big[R_{t+1}+\gamma\mathbb{E}_\pi[Q(S_{t+1},A_{t+1})\mid S_{t+1}]-Q(S_t,A_t)\Big]\\&=Q(S_t,A_t)+\alpha\Big[R_{t+1}+\gamma\sum_a\pi(a|S_{t+1})Q(S_{t+1},a)-Q(S_t,A_t)\Big],\end{aligned}$$
但是其他部分都和Q学习一样。给定了下一个状态$S_{t+1}$，这个算法可以确定性地向期望意义上的Sarsa算法所决定地方向上移动，所以它叫做 expected Sarsa。它的备份图：
![](images/Pasted%20image%2020231110165137.png)

期望Sarsa在计算上比Sarsa更加复杂，但是作为回报，它消除了因为随机选择$A_{t+1}$而产生的方差，在相同数量的经验上，它的表现比Sarsa更好一点。
在一般情况下，期望Sarsa可以采用与目标策略不同的策略来生成行为。在这种情况下期望 Sarsa 就成了离轨策略的算法。举个例子，假设目标策略是一个贪心策略，而行动策略却更注重于试探，那么此时期望 Sarsa与Q学习完全相同。从这个角度来看，期望Sarsa推广了Q学习，可以将Q学习视作期望Sarsa 的一种特例，同时期望Sarsa比起Sarsa 也稳定地提升了性能。除了增加少许的计算量之外，期望Sarsa 应该完全优于这两种更知名的TD差分控制算法。

##### 最大化偏差与双学习
我们学习到的所有控制算法都是为了最大化它们的目标策略。比如，在Q学习中，目标策略是在当前动作价值下取贪心的策略，那么在Sarsa中，这个策略总是  $\varepsilon-greedy$，也会涉及到最大化操作。在这些算法中，在估计值的基础上最大化也可以看作是隐式地对最大值进行估计，这就可能导致很大的正偏差。为了说明为什么，考虑一个状态$s$，它可选择的动作的真实价值$q(s,a)$全为0，但是它们的估计值$Q(s,a)$是不确定的，可能有些大于0，有些小于0。真实值的最大值是0，但是估计值的最大值是正数，这句产生了正偏差，我们将其称作最大化偏差（maximization bias）

看个例子，一个MDP中有两个非终止节点A和B，每个episode都**从A开始**选择向左或向右的动作，选择向右会立刻转移到终止状态并得到值为0的收益和回报。选择向左的动作则会使状态转移到B，得到的收益也为0。而在B的状态下就有很多可能的动作，每种动作被选择后就立刻终止并得到一个从 均值为-0.1，方差为1.0的正态分布中采样一个收益。
![](images/Pasted%20image%2020231110185656.png)
因此，任何一个以向左开始的轨迹的期望回报均为-0.1，因为reward的均值为 -0.1，这意味着长期平均来看，每次从 B 状态结束的 episode 的预期收益是 -0.1，所以在A这个状态中根本不应该选择向左。 尽管如此，我们的控制方法都会偏好向左，因为最大化偏差会让B呈现出正的价值。即使 B 状态的平均收益是负值（-0.1），由于它的高方差，控制算法可能会将其视为具有潜在的正价值。这是因为高方差意味着有机会获得高于平均值的正回报，尽管平均回报是负的。控制方法可能会重视这种潜在的高回报机会，并因此偏好向左的选择。
下图就是显示了使用  $\varepsilon-greedy$策略来选择动作的Q学习算法会在开始阶段非常明显地偏好向左这个动作。即使在算法收敛到稳态时，它选择向左这个动作的概率也比最优值高了5%，![](images/Pasted%20image%2020231110184439.png)

那么有没有算法能够避免最大化偏差呢？
再来考虑一个赌博机的例子，在这个例子中，我们对每个动作的价值做一个带噪声的估计，这是通过对该动作产生的所有收益进行简单平均得到的。正如前文所述，如果我们将估计值的最大值视为对真实估计的最大值的估计，那么就会产生正的最大化偏差。对于这个问题，有一种看法是，其根源在于确定价值最大的动作和估计它的价值这两个过程采用了同样的样本。假如我们将这些样本划分为两个集合，并用它们学习两个独立的对真实价值$q(a)$的估计$Q_1(A)$和$Q_2(a)$，那么我们接下来就可以使用其中一个估计，比如$Q_1(A)$来确定最大的动作$A^*=\operatorname{argmax}_aQ_1(a)$ ，再用另一个$Q_2$来计算其价值的估计$Q_2(A^*)=Q_2(\operatorname{argmax}_aQ_1(a))$。然后由于$\begin{aligned}\mathbb{E}[Q_2(A^*)]=q(A^*).\end{aligned}$ ，我们可以知道这个估计是无偏差的。这就是 **双学习**的思想，这里我们虽然学习了两个估计值，但是对每个样本集合只更新一个估计值。
这样，双学习看上去消除了最大化偏差所带来的性能损失。



GPI由两个过程组成，其中一个驱使价值函数去准确地预测当前策略的回报，这就是所谓的“预测问题”。而另一个过程则驱使策略根据当前的价值函数来进行局部改善(例如可以使用 e-贪心策略)，这就是所谓的“控制问题”。当第一个过程需要使用经验时，如何维持足够的试探就成为一个难题。在解决这个难题时，可以分成同轨策略方法和离轨策略方法




##### n步自举法
这里我们将把MC方法和一步差分方法结合起来。n步时序差分方法是两种方法的更一般的推广，在这个框架下可以更平滑地切换这两种方法。MC和时序差分是这个框架中地两个极端特例。
n步时序差分法解决了之前更新时刻的不灵活问题，在单步时序差分方法中，时间间隔（即时刻步长）总是一样的，n步方法使自举法在一个较长的时间段内进行，从而解决了单步时序差分的不灵活问题。
我们这里也首先考虑预测问题，再考虑控制问题，即我们先讨论n步方法如何能够更好地对一个固定的策略预测其状态价值函数$v_\pi$，然后再将其扩展到控制问题。

蒙特卡洛方法根据从某一状态开始到终止状态的收益序列，对这个状态的价值进行更新。而时序差分方法则只根据后面的单个即时收益，在下一个后继状态的价值估计值的基础上进行自举更新。在这里，用于自举的后继状态价值代表了后面所有剩余时刻的累积收益。因此，一种介于两者之间的方法根据多个中间时刻的收益来进行更新。例如，两步更新基于紧接着的两步收益和两步之后的价值函数的估计值。
几种方法的备份图如下，![](images/Pasted%20image%2020231110204431.png)
n步更新方法依然属于时序差分方法，因为在这些方法中，前面状态的估计值会根据它与后继状态的估计值的差异进行更新，不同的地方是，这里的后继状态是n步后的状态，而不是紧接着当前状态的下一个时刻的状态。

考虑一个state-reward序列，$S_t,R_{t+1},S_{t+1},R_{t+2},\ldots,R_T,S_T$，
我们知道蒙特卡洛更新$v_\pi(S_t)$的值是使用完整的回报：
$G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots+\gamma^{T-t-1}R_T$
这个量就叫做更新的目标，但是在一步时序差分中，就是立即的奖励加上折扣的下个状态值的估计，就是：
$G_{t:t+1}\doteq R_{t+1}+\gamma V_t(S_{t+1})$
其中$\gamma V_t(S_{t+1})$代替了完整回报中 的$\gamma R_{t+2}+\gamma^2R_{t+3}+\ldots+\gamma^{T-t-1}R_T$

类似地，这种想法也能扩展到两步的情况，两步更新的目标是两步回报：
$$G_{t:t+2}\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2V_{t+1}(S_{t+2})$$
其中$\gamma^2V_{t+1}(S_{t+2})$代替了$$\gamma R_{t+3}+\gamma^2R_{t+4}+\cdots+\gamma^{T-t-1}R_T$$
类似地，任意n步更新的目标是 n步回报
$$G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^nV_{t+n-1}(S_{t+n})$$

n步时序差分的伪代码如下
![](images/Pasted%20image%2020231110210157.png)



##### n步Sarsa
那么怎么让n步方法不单单用来预测，也可以用作控制呢？接下来就来看一下n步方法是怎么和Sarsa直接结合来产生on-policy的时序差分学习控制方法的。

核心是将状态替换为 **状态-动作**二元组，然后使用 $\epsilon-greedy$策略。 n步Sarsa的备份图和n步时序差分的备份图相似，![](images/Pasted%20image%2020231110211519.png)![](images/Pasted%20image%2020231110211547.png)
唯一不同的就是Sarsa首末两端都是动作而不是状态，我们重新根据动作的价值估计定义如下的n步方法的回报（更新目标）：
$$G_{t:t+n}=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^{n}Q_{t+n-1}(S_{t+n},A_{t+n}),n\geqslant1,0\leqslant t<T-n,$$

那么n步方法怎么用在off-policy上呢？ off-policy是在学习策略$\pi$的时候，智能体却遵循另一个策略$b$的学习方法，通常$\pi$是针对当前动作价值函数的贪心策略，而$b$是一个更具试探性的策略，例如$\epsilon-greedy$ ，在n步方法中，回报根据n步来建立，所以我们只对这n步的相对概率感兴趣，例如，实现一个简单的off-policy策略版本的n步时序差分学习，对于t时刻的更新（实际在t+n时刻），可以简单地用 $\rho_{t:t+n-1}$来加权
$$V_{t+n}(S_t)\doteq V_{t+n-1}(S_t)+\alpha\rho_{t:t+n-1}\left[G_{t:t+n}-V_{t+n-1}(S_t)\right],0\leqslant t<T,$$
 $\rho_{t:t+n-1}$就叫做重要度采样率，是两种策略采取 $A_t~A_{t+n}$这n个动作的相对概率
 $$\rho_{t:h}\doteq\prod_{k=t}^{\min(h,T-1)}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}.$$
 例如，假定策略$\pi$永远都不会采取某个特定动作$(\pi(A_k|S_k)=0)$则n步的权重应为0，即完全忽略。另一方面，假如碰巧某个动作在策略$\pi$下被采取的概率远远大于行动策略6，那么将增加对应回报的权重。这是有意义的，因为该动作是策略$\pi$的特性(因此我们需要学习它)，但是该动作很少被选择，因此也很少出现在数据中。为了弥补这个缺陷，当该动作发生时，我们不得不提高它的权重。注意，如果这两种策略实际上是一样的，那么重要度采样率总是 1。所以，更新公式(7.9)是之前的n步时序差分学习方法的推广，并且可以完整代替它。同样，之前的n步Sarsa更新方法可以完整地被如下简单的off-policy版的方法代替
$$Q_{t+n}(S_t,A_t)\doteq Q_{t+n-1}(S_t,A_t)+\alpha\rho_{t+1:t+n}\left[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)\right],$$
这里的重要度采样率，其起点和终点比n步时序差分学习方法都要晚一步，这是因为这里更新的是 **状态-动作**二元组，我们并不需要关系这些动作有多大概率被选择，既然我们已经确定了这个动作，那么我们想要的是充分地学习发生的事情，这个学习过程会使用基于后继动作计算出的重要度采样加权系数。这个算法的完整伪代码展示在下面的框里。
![](images/Pasted%20image%2020231110221121.png)



强化学习方法分为基于模型和无模型。基于模型的方法将**规划**作为其主要组成部分，而无模型的方法则主要依赖于**学习**。虽然这两种方法之间存在着很大的差异，但它们也有很多相似之处，特别是这两类方法的核心都是价值函数的计算。

在人工智能中，根据我们的定义，有两种不同的规划方法。第一种是**状态空间规划**，包括我们在本书中所采用的方法，可以将它们视为一种“搜索”方法，即在状态空间中寻找最优策略或实现目标的最优决策路径。每个动作都会引发状态之间的转移，价值函数的定义和计算都是基于状态的。而在所谓的**方案空间规划**中，规划是在“方案空间”中进行的。我们需要定义一些操作将一个“方案”(即一个具体的决策序列)转换成另一个，并且如果价值函数存在，它是被定义在不同“方案”所构成的空间中的。

我们在这里提出一个统一的视角，即所有的状态空间规划算法都有一个通用的结构，在之前所讨论的学习方法中也体现了这个结构。本章的目标就是阐述这个观点。这里有两个基本的思想:
- (1) 所有的状态空间规划算法都会利用计算价值函数作为改善策略的关键中间步骤;
- (2)它们通过基于仿真经验的回溯操作来计算价值函数。这种通用的结构如下所示
通用的结构如下：
![](images/Pasted%20image%2020231111093348.png)

显然，动态规划就是这样的结构，它们先遍历状态空间，为每个状态生成可能的转移，然后利用概率分布来计算回溯值（更新目标），并更新状态的价值函数估计值。
其他方法也适用于这种结构，不同的方法只在它们所作的回溯操作、它们执行操作的顺序和回溯的信息被保留的时间长短上有所不同。

对于在线完成的规划，智能体与环境交互时会出现一些有趣的问题，从交互中获得的新信息可能会改变模型，从而与规划过程产生相互作用。所以我们可能需要对规划过程进行定制化调整。

有一种简单的架构 Dyna-Q, 继承了在线规划的智能体所需要的主要功能。对一个规划智能体来说，实际经验有两个作用： 
- 可以用来改进模型（使模型与显示环境更精确地匹配）
- 可以用于直接改善前面说的价值函数和策略。
前者称为模型学习，后者称为直接强化学习。下面这个图表明了 经验、模型、价值和策略之间的关系
![](images/Pasted%20image%2020231111095121.png)
注意看，“经验”是如何直接或通过模型间接地改善价值函数和策略的。

无论是直接还是间接的方法都有各自的优缺点，间接方法往往能更充分地利用有限的经验，从而获得更好的策略，减少与环境的相互作用。直接方法就简单得多，它不受模型的设计偏差的影响。

Dyna-Q包括了上面的所有过程： 规划、动作、模型学习、直接强化学习。
![](images/Pasted%20image%2020231111102627.png)
中间一列是智能体与环境之间的基本交互关系，产生真实经验，左边的箭头表示直接强化学习，通过实际经验改善价值函数和策略。右边部分是基于模型的 过程，模型从实际经验中学习，并进行仿真产生模拟经验，然后将强化学习方法应用于模拟经验得到规划，就像它们真的发生过一样。

从上面这个图就可以看出 **学习**和 **规划**是紧密地结合在一起的，它们分享几乎所有相同的计算资源，不同的只是它们经验的来源。


在本书中目前提到的所有方法都包含三个重要的通用思想:
- 首先，它们都需要估计价值函数;
- 第二，它们都需要沿着真实或模拟的状态轨迹进行回溯操作来更新价值估计;
- 第三，它们都遵循广义策略迭代(GPI)的通用流程，也就是说它们会维护一个近似价值函数和一个近似策略，并且持续地基于一方的结果来改善另一方。
这三个思想也是贯穿全书的核心内容。我们认为价值函数、基于回溯的价值更新和广义策略迭代(GPI)几乎是任何一种智能模型都用到的有效组织原理，无论是人工智能还是自然智能都不例外。

# 表格型近似求解方法
很多我们想要应用强化学习的任务都具有组合性、巨大的状态空间，在这种情况下，即使有近乎无限的时间与数据，我们也不期望找到最优的策略或最优价值函数，我们目标转而为使用有限的计算资源找到一个比较好的近似解



首先看一下这么使用on-policy策略数据来估计状态价值函数，也就是说，从已知的策略$\pi$生成的经验来近似一个价值函数$v_\pi$，但是近似的价值函数不再是一个表格，而是一个具有权值向量$\mathbf{w}\in\mathbb{R}^d$的参数化函数，即 $\hat{v}(s,\mathbf{w})\approx v_{\pi}(s)$， 通常来说，权值的数量（w的维度）远远小于状态的数量（$(d\ll|\mathcal{S}|)$）。
改变一个权值将改变许多状态的估计值。因此，一个状态被更新时，许多其他状态的价值函数也会被更改，这样的泛化能力会使学习能力更加强大，但也可能更加难以控制与理解。

强化学习中，所有预测方法都可以表示为对一个待估计的价值函数的更新，这种更新使得某个特定状态下的价值移向一个“回溯值”（back-up value），或称为这个状态下的更新目标。我们使用符号 $s\mapsto u$表示一次单独的更新，这里$s$表示更新的状态，而 $u$表示$s$的估计价值朝向的更新目标。比如，蒙特卡洛的价值函数预测的更新是 $S_t\mapsto G_t$ , TD(0)的更新是$S_t\mapsto R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)$ ，然后n步TD更新是$S_t\mapsto G_{t:t+n}$ ， 在DP的策略评估更新中，则是$\begin{aligned}s\mapsto\mathbb{E}_\pi[R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)|S_t=s]\end{aligned}$ 。DP的任意一个状态s都会被更新，而其他的方法只会更新在经验中遇到的状态 $S_t$

我们很自然可以将每一次更新解释为给价值函数指定一个理想的“输入-输出”范例样本。更新$s\mapsto u$意味着状态 s的估计价值需要更接近更新目标 u。实际使用的更新是简单的：$s$的估计价值只是简单地向$u$地方向进行了一定比例地移动，而其他状态地估计价值保持不变。现在我们允许使用任意复杂地方法来进行输出，在$s$上进行的更新会泛化，使得其他状态的估计价值同样发生变化。
学习拟合“输入-输出”样例的机器学习方法叫作 **有监督学习** ， 当输出$u$为数字时，这个过程通常被称为 **函数逼近**，函数逼近需要获取它所逼近的函数的理想“输入-输出”样本，我们简单地把每次更新使用的$s\mapsto u$作为使用函数逼近的价值函数预测的**训练样本**，然后把产生的近似函数作为**估计价值函数**。

我们把每一次更新都视为一个训练样本，这就让我们可以使用几乎所有现存的函数逼近方法来进行价值函数预测。原则上我们可以使用任何有监督方法，比如人工神经网络，决策树和各种多元回归。 
- 但是 在强化学习中需要在线学习，即智能体需要和环境交互，要做到这一点就需要算法能够从逐步得到的数据中有效地学习。
- 此外，强化学习通常需要能够处理非平稳目标函数（即随时间变化的目标函数）的函数逼近方法。例如，在基于GPI（通用策略迭代）的控制方法中，我们经常需要在变化时学习 $q_\pi$。而且即使策略保持不变，使用自举法产生的训练样例也是 非平稳的。
那么最为复杂的神经网络和统计方法都假设存在一个静态训练集，并在此集合上可以进行多次训练，然后难以处理上面两种情况的也不太适合强化学习。


那么到现在我们还没有确定一个清晰明确的预测目标，在表格型情况下，学习到的价值函数完全可以与真实的价值函数精确相等，而且每个每个状态下学习的价值函数都是解耦的-----一个状态的更新不会影响其他状态，但是在函数逼近中，一个状态的更新会影响到许多其他状态，而且不可能让所有状态的价值函数完全正确。因为状态的数量远多于权值的数量，所以一个状态的估计价值越准确就意味着别的状态的估计价值变得不准确。
所以我们需要给出那些状态是我们最关心的，一个状态$s$的误差表示近似价值函数$\hat{v}(s,\mathbf{w})$和真实价值函数$v_\pi(s)$的差的平方，我们再给他加上一些权值，就得到一个自然的目标函数，即均方价值误差，$$\overline{\mathrm{VE}}(\mathbf{w})\doteq\sum_{s\in\mathcal{S}}\mu(s)\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2$$
它的平方根给出了一个粗略的对近似价值函数与真实价值差异的大小的度量。
目前还不确定$\overline{\mathrm{VE}}$是不是强化学习正确的性能目标，但是不要忘记我们的终极目标，学习价值函数 的目的是为了寻找更好的策略，达到这个目的的最优价值函数不一定是满足最小化$\overline{\mathrm{VE}}$的价值函数，然而，对于价值函数预测来说，我们还不清楚是否存在一个更清晰的目标，所以现在主要关注$\overline{\mathrm{VE}}$
我们想要的是找到一个最优解$w^*$，让$\overline{\mathrm{VE}}$达到最小,基本上都是使用梯度下降法。

一个好的策略就是尽量减少观测到的样本的误差，随机梯度下降的方法对于每一个样本，将权值向量朝着能够减小这个样本的误差的方向移动一点点
$$\begin{aligned}
\mathbf{W}_{t+1}& \doteq\mathbf{w}_{t}-\frac{1}{2}\alpha\nabla\Big[v_{\pi}(S_{t})-\hat{v}(S_{t},\mathbf{w}_{t})\Big]^{2}  \\
&=\mathbf{w}_{t}+\alpha\Big[v_{\pi}(S_{t})-\hat{v}(S_{t},\mathbf{w}_{t})\Big]\nabla\hat{v}(S_{t},\mathbf{w}_{t}),
\end{aligned}$$
这里$\alpha$是一个正的步长参数。对于任意一个关于向量的标量函数 $f(w)$，$\nabla f(\mathbf{w})$是一个列向量，其中每个分量是该函数对输入向量对应分量的偏导数：
$$\nabla f(\mathbf{w})\doteq\left(\frac{\partial f(\mathbf{w})}{\partial w_1},\frac{\partial f(\mathbf{w})}{\partial w_2},\ldots,\frac{\partial f(\mathbf{w})}{\partial w_d}\right)^\top.$$
这个 导数向量是$f$关于$w$的梯度，之所以说SGD是一种 **梯度下降** 的方法，是因为 $w_t$在整个时刻中按比例向样本的平均误差的负梯度移动。这就是误差下降最快速的方法。梯度下降方法被称为“随机”是因为更新仅仅依赖于一个样本来完成，而且这个样本是随机选择的。 通过小步长的更新，总体的效果就是最小化一个平均的性能度量，比如$\overline{\mathrm{VE}}$
SGD的收敛性是建立在$\alpha$随时间减小的假设上，如果满足标准随机近似条件（如下图）的方式减少，那么SGD方法能保证收敛到局部最优解。
![](images/Pasted%20image%2020231111162038.png)


现在讨论一种情况，第$t$个训练样本 $S_t\mapsto U_t$的目标输出 $U_t$不是真实的价值 $v_\pi(S_t)$，而是它的一个随机近似，例如， $U_t$可能是 $v_\pi(S_t)$的一个带噪声的版本。在这种情况下，由于  $v_\pi(S_t)$不知道，因此我们不能直接使用式子
$$\begin{aligned}
\mathbf{W}_{t+1}& \doteq\mathbf{w}_{t}-\frac{1}{2}\alpha\nabla\Big[v_{\pi}(S_{t})-\hat{v}(S_{t},\mathbf{w}_{t})\Big]^{2}  \\
&=\mathbf{w}_{t}+\alpha\Big[v_{\pi}(S_{t})-\hat{v}(S_{t},\mathbf{w}_{t})\Big]\nabla\hat{v}(S_{t},\mathbf{w}_{t}),
\end{aligned}$$
进行更新
但是，我们可以用$U_t$近似取代$v_\pi(S_t)$ ，这就产生了以下由于进行状态价值函数预测的通用SGD方法：
$$\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\Big[U_t-\hat{v}(S_t,\mathbf{w}_t)\Big]\nabla\hat{v}(S_t,\mathbf{w}_t)$$
如果 $U_t$是一个无偏估计，即满足对任意的 $t$，都有$\mathbb{E}[U_t|S_t=s]=v_\pi(S_t)$，那么在$\alpha$满足随机近似条件下，$w_t$一定会收敛到一个局部最优解。

蒙特卡洛目标$U_t\doteq G_t$根据定义就是 $v_\pi(S_t)$的无偏估计。在这种选择下，通用SGD方法会收敛到 $v_\pi(S_t)$的一个局部最优近似。下面就是使用蒙特卡洛的伪代码：
![](images/Pasted%20image%2020231111163949.png)

如果使用 $v_\pi(S_t)$的自举估计值作为目标$U_t$，则无法得到相同的收敛性保证。自举目标，比如n步回报的$G_{t:t+n}$或是DP目标$\sum_{as',r}\pi(a|S_t)p(s',r|S_t,a)[r+\gamma\hat{v}(s',\mathbf{w}_t)]$都取决于权值向量$w_t$的当前值，这就意味着有偏，所以它们无法实现真正的梯度下降法。由于只包含了一部分梯度，所以这也叫做半梯度方法。
尽管半梯度（自举法）不像梯度方法那样稳健地收敛，但它们在一些重要情况下依然可以可靠地收敛。它们学习速度通常比较快，而且支持持续地和在线地学习，而不需要等待一个episode结束，这就让它们能够用于持续性地问题，所以半梯度通常是首选的方案。半梯度TD(0)是一个在原型意义上的半梯度方法，使用$\begin{aligned}U_t\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w})\end{aligned}$作为其目标，下面是其伪代码：
![](images/Pasted%20image%2020231111164720.png)


##### 线性方法
函数近似最特殊的例子就是当 近似函数$\hat{v}(\cdot,\mathbf{w})$是权重向量 $w$的线性函数。对于每个状态s，都有一个实数向量$$\mathbf{x}(s)\doteq(x_1(s),x_2(s),\ldots,x_d(s))^\top $$
与$w$有相同的维数。
线性方法通过内积这两个向量来对状态价值函数近似：
$$\hat{v}(s,\mathbf{w})\doteq\mathbf{w}^\top\mathbf{x}(s)\doteq\sum_{i=1}^dw_ix_i(s)$$
这个向量 $x(s)$表示 $s$的特征向量，每个分量$x_i(s)$都是一个 状态->实数 的函数，即$\begin{aligned}x_i:\mathcal{S}\to\mathbb{R}\end{aligned}$ 。 一个特征就是一个完整的函数，状态$s$对应的函数值称作 $s$的特征。对于线性方法，特征被称作基函数，这是因为它们构成了可能的近似函数集合的线性基。构造d维特征向量来表示一个状态与选择一组 d 个基函数是相同的。

使用SGD方法来更新线性函数近似是很自然地，这个近似价值函数关于w的梯度就是
$$\nabla\hat{v}(s,\mathbf{w})=\mathbf{x}(s)$$
因此，在线性例子中，通用的SGD更新变成一个特别的形式：
$$\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\Big[U_t-\hat{v}(S_t,\mathbf{w}_t)\Big]\mathbf{x}(S_t)$$
在线性情况下，函数只存在一个最优值，因此保证收敛到或接近局部最优值的任何方法，肯定也是全局最优值。例如梯度蒙特卡洛算法在线性函数逼近下也会收敛到全局最优值。

半梯度TD(0)算法也在线性函数逼近下收敛，但它并不遵从SGD的一般通用结构，所以我们需要有一个额外的定理来证明。权值向量也不是收敛到全局最优而是靠近局部最优的点。在每个时间点$t$的更新是：
$$\begin{aligned}
\mathbf{W}_{t+1}& \doteq\mathbf{w}_t+\alpha\Big(R_{t+1}+\gamma\mathbf{w}_t^\top\mathbf{x}_{t+1}-\mathbf{w}_t^\top\mathbf{x}_t\Big)\mathbf{x}_t  \\
&=\mathbf{w}_t+\alpha\Big(R_{t+1}\mathbf{x}_t-\mathbf{x}_t\big(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\big)^\top\mathbf{w}_t\Big)
\end{aligned}$$
这里使用了简写，$x_{t}=x(S_t)$ . 一旦系统到达了一个稳定的状态，对于任意的$w_t$，下一个时刻的权值向量的期望可以写作$$\mathbb{E}[\mathbf{w}_{t+1}|\mathbf{w}_t]=\mathbf{w}_t+\alpha(\mathbf{b}-\mathbf{A}\mathbf{w}_t),$$其中$$\mathbf{b}\doteq\mathbb{E}[R_{t+1}\mathbf{x}_t]\in\mathbb{R}^d\quad\mathrm{and}\quad\mathbf{A}\doteq\mathbb{E}\Big[\mathbf{x}_t\big(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\big)^\top\Big]\in\mathbb{R}^{d\times d}$$如果这个系统收敛了，那么一定会收敛于满足下式的权值向量$w_{TD}$， 
$$\begin{aligned}\mathbf{b}-\mathbf{A}\mathbf{w}_\mathrm{TD}&=\mathbf{0}\\\Rightarrow&\mathbf{b}&=\mathbf{A}\mathbf{w}_\mathrm{TD}\\\Rightarrow&\mathbf{w}_\mathrm{TD}&\doteq\mathbf{A}^{-1}\mathbf{b}.\end{aligned}$$
这个量叫做TD的不动点（fixed point），事实上，线性半梯度TD(0)就收敛到这个点上。

在这个TD不动点，已经被证明 它在可能的最小误差的一个扩展边界内
$$\mathrm{\overline{VE}(w_{TD})~\leq~\frac1{1-\gamma}\min_{\mathbf{w}}\overline{\mathrm{VE}}(\mathbf{w}).}$$
也就是说，TD法的渐进误差不会超过使用蒙特卡洛法能得到的最小可能误差的$\frac1{1-\gamma}$倍。尽管TD方法可能不会收敛到全局最优解，但它可以达到一个相对较好的解，并且这个解的误差是在可接受的范围内。


##### 基于函数逼近的 on-policy Control
这里我们回到控制问题，采用参数逼近的动作-价值函数 $\hat{q}(s,a,\mathbf{w})\approx q_*(s,a)$ ，$w$是有限维数的权值向量。这里讲的是半梯度Sarsa，是半梯度TD(0)在动作价值和在策略控制方面的自然扩展。

一旦我们使用了真正的函数逼近，我们必须放弃折扣，并转向控制问题的新的‘平均奖励’公式，以及新的‘差分’价值函数。

首先从分段的例子开始，将上一章提出的关于状态价值的函数逼近思想扩展到动作价值上。然后我们根据在策略GPI的通用模式将其扩展到控制上，使用ε-贪婪法进行动作选择。
近似动作价值函数 $\hat{q}\approx q_\pi$也被表示成使用权值向量$w$的参数化函数形式，更新的目标$U_t$可以是对于$q_\pi(S_t,A_t)$的任意近似，包括通常的备份值，比如完整的蒙特卡洛返回（Gt​）或任何n步Sarsa返回。

动作价值预测的一般梯度下降更新公式为：
$$\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\Big[U_t-\hat{q}(S_t,A_t,\mathbf{w}_t)\Big]\nabla\hat{q}(S_t,A_t,\mathbf{w}_t).$$
比如，一步Sarsa方法的更新值是
$$\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\Big[R_{t+1}+\gamma\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t)\Big]\nabla\hat{q}(S_t,A_t,\mathbf{w}_t).$$
我们把这个叫做分幕式（episodic）的半梯度一步Sarsa，对于一个恒定策略，这种方法以与TD(0)相同的方式收敛，并且具有同类型的误差界限$\frac1{1-\gamma}$

为了形成控制方法，我们需要将这样的动作价值预测方法与策略改进或是动作选择进行组队。适用于连续动作或大型离散集动作的合适技术是一个持续研究的话题，目前还没有明确的解决方案。但是如果并不是这样，那么我们就可以用前面说的方法，那就是，对于下个状态$S_{t+1}$所有可能的动作，我们计算$\hat{q}(S_{t+1},a,\mathbf{w}_t)$然后选择最大的那个动作。
然后，通过将估计策略改变为贪婪策略的软近似，例如ε-greedy策略，来进行策略改进。下面就是这种算法的伪代码
![](images/Pasted%20image%2020231115174511.png)


当我们使用n步回报作为在上面的半梯度SARSA更新等式（如下）
$$\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\Big[U_t-\hat{q}(S_t,A_t,\mathbf{w}_t)\Big]\nabla\hat{q}(S_t,A_t,\mathbf{w}_t).$$
的更新目标，那么n步回报就会从表格形式推广到函数逼近形式：
$$G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}),\quad t+n<T,$$
然后n步更新公式就变成了：
$$\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\left[G_{t:t+n}-\hat{q}(S_{t},A_{t},\mathbf{w}_{t+n-1})\right]\nabla\hat{q}(S_{t},A_{t},\mathbf{w}_{t+n-1}),\quad0\leq t<T.$$

使用中等程度的自举法往往能得到最好的性能，对应的就是大于1的n，如下，使用n=8相比于使用n=1拥有更快并获得更好的渐进性能。
![](images/Pasted%20image%2020231115200730.png)


##### 平均收益
平均收益 是第三种经典的马尔可夫决策问题的目标设定，不同于 **分幕式episodic**设定和 **折扣** 设定。平均收益设定也适用于持续性问题，但是它不考虑任何折扣，对于延迟收益的重视程度与即时收益相同。 折扣设定对函数逼近来说是有问题的，所以需要使用平均收益来替换它。



在平均收益设定中，一个策略$\pi$的质量被定义为在遵循该策略时奖励的平均率，简称平均奖励值，表示为$r(\pi)$ 
$$\begin{aligned}
r(\pi)& \doteq\lim_{h\to\infty}\frac1h\sum_{t=1}^h\mathbb{E}[R_t\mid S_0,A_{0:t-1}\sim\pi]  \\
&=\operatorname*{lim}_{t\to\infty}\mathbb{E}[R_{t}\mid S_{0},A_{0:t-1}\sim\pi], \\
&=\sum_{s}\mu_{\pi}(s)\sum_{a}\pi(a|s)\sum_{s^{\prime},r}p(s^{\prime},r|s,a)r,
\end{aligned}$$
这里的期望根据初始状态$S_0$和遵循$\pi$生成的动作序列$A_0,A_1,...,A_{t-1}$来决定



##### off-policy methods with approximation
在前面，就将on-policy和off-policy方法作为两种不同的方式来解决在GPI中探索与利用之间的冲突。
扩展off-policy方法到函数近似要比on-policy更加困难。表格型的off-policy方法可以很容易扩展到半梯度算法，但是它们并不像on-policy那样能够稳健地收敛。

在函数近似的off-policy学习有一个主要的挑战，off-policy的更新的分布和on-policy的分布不一样，on-policy的分布对于半梯度方法的稳定性极其重要。
这里有两种方法来解决，一个是再一次使用重要性采样，这次是把更新分布变成on-policy的分布，所以半梯度方法也能保证收敛，另一个是寻找真正的梯度方法，这个方法不依赖于任何特殊的分布才能稳定。

在前面描述了一系列表格型的off-policy算法，为了把它们转化称为半梯度的形式，我们简单把原来数组(V or Q)的更新改成对权值向量$w$的更新，使用近似价值函数及其梯度。
这些算法中都使用到了每步重要性采样率：
$$\rho_t\doteq\rho_{t:t}=\frac{\pi(A_t|S_t)}{b(A_t|S_t)}.$$
例如，单步的状态价值算法就是半梯度off-policy TD(0)，就只是在原来的on-policy方法上加了一个 $\rho_t$:
$$\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\rho_t\delta_t\nabla\hat{v}(S_t,\mathbf{w}_t),$$
其中 $\delta_t$就是：
$$\begin{aligned}\delta_t&\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t),\text{ or}\\\\\delta_t&\doteq R_{t+1}-\bar{R}_t+\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t).\end{aligned}$$



**更新的分布与on-policy分布不一致**，我们首先考虑一个简单的情况，假设这里有两个状态，它们估计的价值以函数形式表达是 $w$和$2w$，$w$只包含一个独立分量 w 。
在第一个状态，只有一个动作可选，一定导致向第二个状态转移，并获得奖励 0 
![](images/Pasted%20image%2020231115211244.png)

假设初始 w=10 ，那么转移就变成了从估计价值为10的状态变成20的状态。看起来是很好的转移，之后$w$将会增大来增加第一个状态的估计价值。如果$\gamma$接近于1，那么TD误差接近10，而且如果$\alpha$是0.1，$w$将会增加到接近11，为了减少TD误差。然而，第二个状态的估计值也增加了，接近22。如果转移再次方式，那么就从估计值11转移到估计值22的状态，其中TD误差是11，相比之前更大了。这看起来第一个状态被低估了，之后价值会再次增大，这次到了12.1。 事实上w将会发散到无穷。

这个例子的关键在于，一个转移重复地发生，$w$没有在其他转移上更新。这在off-policy训练时是可能的，因为行为策略可能选择目标策略永远不会选择的动作，那么对于那些转移来说，$\rho_t$将会是0，就没有更新了，但是在on-policy训练时，$\rho_t$永远都是1，每当从$w$转移到$2w$后都会增加$w$，但是当$2w$转移到其他状态时就可能减少$w$ 。
在on-policy情况下，对未来收益的承诺会保留，但是系统也会受到制约。但是在off-policy的情况下，可以做出承诺，但是采取一个目标策略永远不会做出的动作之后，便会忘掉这个承诺。


##### 致命三要素
只要我们的方法满足下述三个基本要素，就一i的那个会议不稳定和发散的危险，这叫做致命三要素：
**函数逼近**：如线性函数逼近和人工神经网络
**自举法**：使用当前的目标估计值进行更新以得到新的目标估计值（如DP和TD方法），而不是只依赖于真实reward和完整回报
off-policy 训练： 用来进行训练的状态转移分布不是由目标策略产生的。

这种风险并不是由控制或者是 广义策略迭代造成的，在比控制更简单的预测问题中，只要包含致命三要素，也会出现不稳定性。
只要有一个不满足，就能避免不稳定性，那么不使用 **自举法**是有可能的，付出的代价是计算和数据上的效率。那么不使用off-policy 学习呢？on-policy的方法通常来说足够了，但是off-policy对于构建强人工智能这个更大的目标来说很重要。












# 策略梯度
在这里考虑一些新的东西，到目前为止几乎所有的方法都是动作价值方法，学习动作的值然后基于估计的动作价值来选择动作。如果没有动作价值估计，那么策略也没有了。
现在我们考虑一个方法，参数化策略，可以直接选择动作而不需要依赖价值函数。
使用$\theta\in\mathbb{R}^{d^{\prime}}$作为策略的参数向量，因此$$\pi(a|s,\boldsymbol{\theta})=\Pr\{A_t=a\mid S_t=s,\boldsymbol{\theta}_t=\boldsymbol{\theta}\}$$就是状态s下采取动作a的概率。

后面的学习方法的学习策略参数是基于某种性能度量$J(θ)$的梯度，这些学习方法寻求最大化性能，所以他们的更新近似于$J$的梯度上升。
$$\theta_{t+1}=\theta_t+\alpha\widehat{\nabla J(\boldsymbol{\theta}_t)},$$
我们把所有符合这个框架的方法都称为 **策略梯度法**，不管它们是否还同时学习一个近似的价值函数。同时学习策略和价值函数的方法一般被称为 Actor-Critic方法，Actor是指学习到的策略，Critic是指学习到的价值函数，一般是状态价值函数。

首先我们考虑分幕式(episodic)情况，在这种情况下性能指标被定义为 **在当前参数化策略下初始状态的价值函数**。


在策略梯度方法中，策略可以用任意的方式参数化，只要 $\pi(a|s,θ)$对参数可导。
这里我们先介绍最常见的对于离散动作空间的策略参数化方法，并且指出其相对价值函数方法的优势。基于策略的方法也提供了良好的处理连续动作的方法。

如果动作空间离散并且不是很大，自然的参数化方法是对每一个 **状态-动作**二元组估计一个参数化的数值偏好 $h(s,a,θ)$ ，在每个状态下拥有最大偏好值得动作被选择得概率也最大，例如，可以根据指数的soft-max分布：
$$\pi(a|s,\boldsymbol{\theta})\doteq\frac{e^{h(s,a,\boldsymbol{\theta})}}{\sum_be^{h(s,b,\boldsymbol{\theta})}},$$
这里分母的作用仅仅是使每个状态下选择动作的概率之和为1.

这些动作偏好值可以被任意地参数化。
根据soft-max选择动作的一个直接好处是可以接近于一个确定策略，尽管基于$\epsilon-greedy$ 的动作选择中会有$\epsilon$的概率选择随机动作。当然我们也可以根据动作价值的soft-max分布选择动作，但是这不会让策略趋向于一个确定的策略，相反动作价值的估计值会收敛到对应的真实值，而这些真实值之间的差异是有限的，因此各个动作会对应到一个特定的概率值而不是0和1.
而动作偏好则不同，因为它们不趋向于任何特定的值，它们趋向于最优的随机策略，如果最优策略是确定的，那么最优动作的偏好值将可能趋向无限大于所有次优的动作。

利用动作偏好的soft-max分布的参数化策略还有另一个优势，就是它可以以任意的概率来选择动作。在有重要函数近似的问题中，最好的近似策略可能是一个随机策略。基于动作价值函数的方法没有一种自然的途径来求解随机最优策略，但是基于策略近似的方法可以。

策略参数化形式的选择有时是在基于强化学习的系统中引入理想中的策略形式的先验知识的一个好办法，这也是一般使用基于策略的学习方法的最重要的原因之一。


##### 策略梯度定理
策略参数化相对于$\epsilon-greedy$的动作选择除了有实践上的优势，还有理论优势。
对于连续的策略参数化，选择动作的概率作为被优化的参数的函数会平滑地变化，而在$\epsilon-greedy$中，行动概率可能因估算行动值的极小变化而发生剧烈变化，尤其是当这种变化导致不同行动具有最大值时。主要因为这个原因，对于策略梯度方法而言，比起行动值方法，有更强的收敛保证。特别是，策略依赖于参数的连续性，使得策略梯度方法能够近似梯度上升。

在分幕式和持续性的两种不同情况下，我们的性能指标$J(θ)$的定义不同，因此在一定程度上不得不分开处理，但是我们也将统一地介绍两种情况，给出一些记号，使得主要的理论结果可以用同一组公式表示。

在这里先考虑分幕式情况，将性能指标定义为episode初始状态的价值。通过假设每个分段都在某个特定的（非随机的）状态s0开始，我们可以简化表示法，而不失去任何有意义的普遍性。于是性能指标定义：
$$J(\boldsymbol{\theta})\doteq v_{\pi_\boldsymbol{\theta}}(s_{0}),$$
其中，$v_{\pi_\boldsymbol{\theta}}$是在策略$\pi_\theta$下的真实价值函数，策略由参数$\theta$决定。我们假设分幕式情况下没有折扣（$\gamma=1$）.

那么在函数逼近的情况下，想要通过调整策略参数来保证性能得到改善是一件很有挑战的事情。主要问题是性能既依赖于动作的选择，也依赖于动作选择时所处的状态的分布，而它们都会受策略参数的影响。
给定一个状态，策略参数对动作选择及收益的影响可以根据参数比较直观地计算出来。但是因为状态分布和环境有关，所以策略对状态分布的影响一般很难确切知道。而性能指标对模型参数的梯度却依赖于这种未知影响，那么如何估计这个梯度呢？

**策略梯度定理**就为这个问题提供了完美的理论解答，它给我们提供了一个性能指标对于参数的梯度的解析表达式，正是我们在近似梯度上升时需要的，其中没有涉及到对状态分布的求导，在分幕式情况下**策略梯度定理**表达式如下：
$$\nabla J(\boldsymbol{\theta})\propto\sum_s\mu(s)\sum_aq_\pi(s,a)\nabla\pi(a|s,\boldsymbol{\theta}),$$
其中$\pi$表示参数向量$\theta$对应的策略，符号$\propto$表示 **正比于**，在分幕式情况下，这个比例常量是episode的平均长度，在持续性情况下，这个常量是1，所以正比于变成了等式。$u$是指在策略$\pi$下的on-policy策略分布。


只需要基本的微积分知识和重排公式中的各项，我们就能证明策略梯度定理。为了让符号简单，我们让$\pi$作为$\theta$的函数隐含表示，梯度和$\theta$的关系也隐含。
首先，状态价值函数的梯度可以写为动作价值函数的形式：
$$\nabla v_\pi(s)=\nabla\left[\sum_a\pi(a|s)q_\pi(s,a)\right]$$
然后对里面求导，用到微积分的乘法法则：
$$=\sum_a\Big[\nabla\pi(a|s)q_\pi(s,a)+\pi(a|s)\nabla q_\pi(s,a)\Big]$$
然后对$q_\pi$使用定义式：
$$=\sum_{a}\Big[\nabla\pi(a|s)q_{\pi}(s,a)+\pi(a|s)\nabla\sum_{s',r}p(s',r|s,a)\big(r+v_{\pi}(s')\big)\Big]$$
与$r$无关，所以直接去掉
$$=\sum_a\Big[\nabla\pi(a|s)q_\pi(s,a)+\pi(a|s)\sum_{s'}p(s'|s,a)\nabla v_\pi(s')\Big]$$
然后把$v_\pi(s')$展开，得到，这里不清楚是什么原理
$$=\begin{aligned}&\sum_a\Big[\nabla\pi(a|s)q_\pi(s,a)+\pi(a|s)\sum_{s'}p(s'|s,a)\sum_{a'}\big[\nabla\pi(a'|s')q_\pi(s',a')+\pi(a'|s')\sum_{s''}p(s"|s',a')\nabla v_\pi(s")\big]\Big]\end{aligned}$$
然后变成
$$=\sum_{x\in\mathcal{S}}\sum_{k=0}^\infty\Pr(s\to x,k,\pi)\sum_a\nabla\pi(a|x)q_\pi(x,a)$$
这里的 $$\Pr(s\to x,k,\pi)$$是在策略$\pi$下，状态$s$在$k$步内转移到状态$x$的概率。所以我们可以得到：
$$\nabla J(\boldsymbol{\theta})=\nabla v_\pi(s_0)$$
用上面的带入
$$=\sum_{s}\left(\sum_{k=0}^{\infty}\Pr(s_{0}\rightarrow s,k,\pi)\right)\sum_{a}\nabla\pi(a|s)q_{\pi}(s,a)$$
然后$$=\sum_s\eta(s)\sum_a\nabla\pi(a|s)q_\pi(s,a)$$然后把形式变换一下，$$\begin{aligned}&=\sum_{s'}\eta(s')\sum_s\frac{\eta(s)}{\sum_{s'}\eta(s')}\sum_a\nabla\pi(a|s)q_\pi(s,a)\end{aligned}$$然后把复杂形式用一个简单方式代换$$\begin{aligned}&=\sum_{s'}\eta(s')\sum_s\mu(s)\sum_a\nabla\pi(a|s)q_\pi(s,a)\end{aligned}$$然后就得到它正比于下面这个$$\propto\sum_s\mu(s)\sum_a\nabla\pi(a|s)q_\pi(s,a)$$证明完毕。


##### REINFORCE
回想一下原来的随机梯度上升$$\theta_{t+1}=\theta_t+\alpha\widehat{\nabla J(\boldsymbol{\theta}_t)}$$我们需要一种获取样本的方法，这些采样的样本梯度的期望正比于性能指标对于策略参数的实际梯度。这些样本梯度只需要正比于实际的梯度，因为任何常数的正比系数显然都可以被吸收到步长参数$\alpha$中。
策略梯度定理给出了一个正比于梯度的精确表达式，现在只需要一种采样的方法，使得采样样本的梯度近似或者等于这个表达式。
注意，梯度策略定理公式的右边是将目标策略$\pi$下**每个状态出现的频率**作为加权系数的求和项，如果按策略$\pi$执行，则状态将按这个比例出现，所以
$$\begin{aligned}\nabla J(\boldsymbol{\theta})&\propto\sum_s\mu(s)\sum_aq_\pi(s,a)\nabla\pi(a|s,\boldsymbol{\theta})\\&=\mathbb{E}_\pi\bigg[\sum_aq_\pi(S_t,a)\nabla\pi(a|S_t,\boldsymbol{\theta})\bigg].\end{aligned}$$
我们可以停在这里，并将我们的随机梯度上升算法实例化为$$\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha\sum_a\hat{q}(S_t,a,\mathbf{w})\nabla\pi(a|S_t,\boldsymbol{\theta}),$$这里的$\hat{q}$是由学习得到的$q_\pi$的近似，这个算法被称为 all-actions方法，因为它的更新涉及到了所有可能的动作。
但是我们目前的兴趣还是在经典的REINFORCE算法，这种经典算法在时刻$t$的更新仅仅涉及到$A_t$，即在时刻$t$被实际采用的动作。

现在来推导REINFORCE，与前面引入$S_t$的过程类似，我们将$A_t$引入进来，把对**随机变量所有可能取值的求和运算**替换为对$\pi$的期望，然后对期望进行**采样**
![](images/Pasted%20image%2020231116152721.png)
前面这里涉及到了对动作的求和，但是每一项中并没有将$\pi(a|S_t,\theta)$作为加权系数，但是这却是对$\pi$求期望所必须的。所以，我们采用一个不改变等价性的方法来引入这个概率加权系数，将每个求和项分别乘上再除以概率$\pi(a|S_t,\theta)$就可以了。那么从上面这个公式继续推导：
$$\nabla J(\boldsymbol{\theta})\propto\mathbb{E}_\pi\bigg[\sum_a\pi(a|S_t,\boldsymbol{\theta})q_\pi(S_t,a)\frac{\nabla\pi(a|S_t,\boldsymbol{\theta})}{\pi(a|S_t,\boldsymbol{\theta})}\bigg]$$
然后用采样$A_t\sim\pi(.|S_t,\boldsymbol{\theta})$ 来替换对所有可能动作$a$的求和，当我们通过策略$\pi$采样一个特定的动作$A_t$，我们实际上是从所有可能动作的集合中随机选择一个动作。因此，期望值可以通过采样估计，而不是通过对所有动作的明确求和。这样，求和符号和乘以$\pi(a|S_t,\theta)$的项都不需要，因为我们不是在对所有动作求和，而是直接考虑一个具体的采样动作$A_t$,简单来说， **采样代替期望**

$$=\mathbb{E}_\pi\bigg[q_\pi(S_t,A_t)\frac{\nabla\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\bigg]$$
然后因为$\begin{aligned}\mathbb{E}_\pi[G_t|S_t,A_t]=q_\pi(S_t,A_t)\end{aligned}$，得到，这里没有把期望E也带进去是因为期望的性质
$$=\mathbb{E}_\pi\bigg[G_t\frac{\nabla\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\bigg]$$
这里$G_t$是通常的回报，括号中的最后一个表达式就恰好是我们想要的，这个量可以通过每步的采样计算得到，它的期望等于真实的梯度。使用这个梯度实现式中的随机梯度上升算法，就可以得到这个式子：
$$\theta_{t+1}\doteq\theta_t+\alpha G_t\frac{\nabla\pi(A_t|S_t,\boldsymbol{\theta}_t)}{\pi(A_t|S_t,\boldsymbol{\theta}_t)}.$$
这个算法就叫做REINFORCE，它的更新有直观上的吸引力。每个增量都正比于回报$G_t$和一个向量的乘积，这个向量是选取动作的概率的梯度除以这个概率本身。这个向量是参数空间中使得将来在状态$S_t$下重复选择动作$A_t$的概率增大最大的方向（是因为他是$\nabla\pi(A_t|S_t,\boldsymbol{\theta}_t)$，是为了增大这个）。这个更新使得参数向量沿着这个方向增加

**更新大小**正比于回报，反比于选择动作的概率。前者的意义在于它使得参数向着更有利于产生最大回报的动作方向更新。后者有意义是因为如果不这样的话，频繁被选择的动作会占优，即使这些动作不是产生最大回报的动作，最好也可能会胜出，就会影响性能指标的优化。

REINFORCE需要使用从时刻$t$开始的完全回报，即从当前时刻到幕结束的所有收益。从这个角度来看，REINFORCE是一个蒙特卡洛算法，只有在分幕式情形下才能很好地定义它，因为它所有地更新都只有在当前episode结束后才能进行。具体代码如下
![](images/Pasted%20image%2020231116170420.png)
可以看到伪代码中把原来的$\frac{\nabla\pi(A_t|\vec{S_t},\boldsymbol{\theta_t})}{\pi(A_t|S_t,\boldsymbol{\theta_t})}$改成了$\nabla\ln\pi(A_t|S_t,\boldsymbol{\theta}_t)$，这是因为有等式$\nabla\ln x=\frac{\nabla x}x$成立。这两种向量我都叫做 **迹向量**，这是算法中策略参数唯一出现的地方。

在代码中还增加了一个$\gamma^t$ ，这是因为我们前面说了只考虑没有折扣的情况，即$\gamma=1$，带折扣是更加一般化的算法。$γ^t$ 乘以梯度项的目的是为了调整该时间步的梯度贡献。这样做的理由是，距离当前时间步 $t$ 更远的决策（即在未来）对当前策略参数的影响应该更小。这种调整有助于反映出早期决策对后续结果的影响程度，同时也有助于保持学习过程的稳定性。


对于这个策略梯度定理，我们还可以将其进行推广
$$\nabla J(\boldsymbol{\theta})\propto\sum_s\mu(s)\sum_aq_\pi(s,a)\nabla\pi(a|s,\boldsymbol{\theta}),$$在其中加入任意一个与动作价值函数进行对比的基线$b(s)$，就得到了
$$\nabla J(\boldsymbol{\theta})\propto\sum_s\mu(s)\sum_a\Big(q_\pi(s,a)-b(s)\Big)\nabla\pi(a|s,\boldsymbol{\theta}).$$这个基线可以是任意函数，只要不随着动作$a$变化，上述推导的等式依然成立，是因为减的那一项等于0，
$$\sum_ab(s)\nabla\pi(a|s,\boldsymbol{\theta})=b(s)\nabla\sum_a\pi(a|s,\boldsymbol{\theta})=b(s)\nabla1=0.$$
然后加了基线也可以推导出更新规则，最新推导出的更新公式是一个包含基线的新的REINFORCE版本
$$\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha\Big(G_t-b(S_t)\Big)\frac{\nabla\pi(A_t|S_t,\boldsymbol{\theta}_t)}{\pi(A_t|S_t,\boldsymbol{\theta}_t)}.$$
因为基线也可以是常量0，所以这个式子是REINFORCE的推广。
一般来说，加入基线不会使更新值的期望发生变化，但是对方差会有很大的影响。

对于马尔可夫决策过程来说，这个基线应该根据状态的变化而变化，在一些状态下，所有动作的价值可能都比较大，因此我们需要一个较大的基线用以区分拥有更大值得动作和相对值不那么高的动作。在其他状态下当所有动作的值都较低时，基线也应该较小。

状态价值函数$\hat{v}(S_t,w)$就是我们能比较自然想到的基线，其中w是权值向量。REINFORCE使用蒙特卡洛方法学习策略参数$\theta$，所以也可以使用MC来学习状态价值函数的权值$w$，使用基线的REINFORCE伪代码如下：
![](images/Pasted%20image%2020231116212541.png)





##### Actor-Critic
尽管带基线的REINFORCE方法既学习了一个策略函数也学习了一个状态价值函数，我们也不认为它是一种 Actor-Critic方法，因为它的状态价值函数仅被用作基线，而不是作为一个 **评判器**，也就是说，它没有用于自举bootstrap操作（用后继各个状态的价值估计值来更新当前某个状态的价值估计值），而只是作为正被更新的状态价值的基线。要这样区分是因为只有采用自举法才会出现依赖于函数逼近质量的偏差和渐进性收敛。

通过自举法引入的偏差以及状态表示上的依赖经常是很有用的，因为它们降低了方差并加快了学习。带基线的REINFORCE是无偏差的，并且会渐进地收敛到局部最小值，但是因为是MC，所以学习比较缓慢，并且不适合在线实现和应用于持续性问题。

然后TD可以消除这些不便，为了在策略梯度中获得这些优势，我们使用带自举评判器的Actor-Critic方法。

首先来看单步的 “Actor-Critic”，就和TD(0)和Sarsa(0)一样，是完全在线和增量式的。单步AC方法使用单步回报（并使用估计的状态价值函数作为基线）来代替REINFORCE算法中的整个回报，$$\begin{aligned}
\theta_{t+1}& \dot{=}\boldsymbol{\theta}_t+\alpha\left(G_{t:t+1}-\hat{v}(S_t,\mathbf{w})\right)\frac{\nabla\pi(A_t|S_t,\boldsymbol{\theta}_t)}{\pi(A_t|S_t,\boldsymbol{\theta}_t)}  \\
&=\boldsymbol{\theta}_t+\alpha\Big(R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w})-\hat{v}(S_t,\mathbf{w})\Big)\frac{\nabla\pi(A_t|S_t,\boldsymbol{\theta}_t)}{\pi(A_t|S_t,\boldsymbol{\theta}_t)} \\
&=\boldsymbol{\theta}_t+\alpha\delta_t\frac{\nabla\pi(A_t|S_t,\boldsymbol{\theta}_t)}{\pi(A_t|S_t,\boldsymbol{\theta}_t)}.
\end{aligned}$$关于状态价值函数，很自然地采用半梯度TD(0)来学习。伪代码如下，这是一个完全在线、增量式的算法，状态、动作和收益都在第一次被收集时使用，之后都不会使用
![](images/Pasted%20image%2020231117142613.png)




基于参数化策略函数的方法还提供了解决动作空间大甚至动作空间连续的实际途径。我们不直接计算每一个动作的概率，而是学习概率分布的统计量。比如动作集是实数集的时候，根据正态分布来选择动作。

正态分布的概率密度函数一般可以写为：
$$p(x)\doteq\frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right),$$
这里，$u$和$\sigma$ 是正态分布的均值和标准差。几组不同的均值和标准差的概率密度函数如下
![](images/Pasted%20image%2020231117151112.png)
$p(x)$是指在$x$处的概率密度，而不是概率，它的值可以大于1，只需要在$p(x)$图像之下的总面积必须是1。我们可以对任意范围的x求小于$p(x)$的积分来得到$x$在此的概率。
![](images/Pasted%20image%2020231117163507.png)


我们就可以将策略定义为关于实数型的标量动作的正态概率密度，其中均值和标准差由状态的参数化函数近似给出，如下
$$\pi(a|s,\boldsymbol{\theta})\doteq\frac{1}{\sigma(s,\boldsymbol{\theta})\sqrt{2\pi}}\exp\biggl(-\frac{(a-\mu(s,\boldsymbol{\theta}))^2}{2\sigma(s,\boldsymbol{\theta})^2}\biggr)$$
其中，$u$和$\sigma$是两个参数化的近似函数。为此，我们就策略的参数向量划分为两个部分 $\theta=[\theta_{\mu},\theta_{\sigma}]^{\top}$，一部分用来近似均值，一部分用来近似标准差。均值可以用一个线性函数来逼近，但是标准差必须是正数，所以使用线性函数的指数形式
$$\mu(s,\boldsymbol{\theta})\doteq\boldsymbol{\theta}_{\mu}^\top\mathbf{x}_{\mu}(s)$$
$$\sigma(s,\boldsymbol{\theta})\doteq\exp\left(\boldsymbol{\theta}_\sigma{}^\top\mathbf{x}_\sigma(s)\right)$$




参数化策略的方法因策略梯度定理相较于动作价值函数拥有一个重要的理论优势，他给出了一个明确的公式来表明在不涉及状态分布导数的情况下，性能指标是如何被策略参数影响的，这个定理为所有的策略梯度法提供了理论依据。

REINFORCE直接来自于策略梯度定理。然后增加一个状态价值函数作为基线降低了REINFORCE的方差，同时也没有引入偏差。
使用状态价值函数进行自举会引入偏差，虽然如此，它还是可以被接受的，因为基于自举法的时序差分一般好于蒙特卡洛法（大幅度降低了方差）。状态价值函数可以用来给策略的动作选择进行评估打分，前者我们称为 **评判器**，后者我们称为 **行动器**





##### 预测与控制
本书中描述的算法分为两大类，预测（prediction）算法和控制（control）算法。
预测算法是一种策略评估算法，他们是策略改进算法中不可或缺的组成部分。


















