策略梯度方法（Policy Gradient Methods）是一类直接针对期望回报（Expected Return）通过 梯度下降（Gradient Descent）进行策略优化的增强学习方法

**在**策略梯度方法**中，我们不是基于预测误差来优化模型，而是试图直接最大化一个期望奖励的函数。这个期望奖励是基于我们的策略在环境中采取的行动序列和随后收到的奖励序列计算的。
为了达到这个目标，我们使用策略梯度定理来估计期望奖励函数关于策略参数的梯度，并使用这个梯度来更新策略参数，从而使得未来基于该策略所获得的奖励最大化。**
**因此，当我们优化（最大化）策略梯度时，我们确实是在使期望的累积奖励 J(θ) 最大化**



这一类方法避免了其他传统增强学 习方法所面临的一些困难，比如，没有一个准确的价值函数，或者由于连续的状态和动作空间， 以及状态信息的不确定性而导致的难解性（Intractability）

和学习 Q 值函数的深度 Q-Learning 方法不同， 策略梯度方法直接学习参数化的策略 $π_θ$。这样做的一个好处是不需要在动作空间中求解价值最 大化的优化问题，从而比较适合解决具有高维或者连续动作空间的问题。策略梯度方法的另一个 好处是可以很自然地对随机策略进行建模。最后，策略梯度方法利用了梯度的信息来引导优化 的过程。

顾名思义，策略梯度方法通过梯度上升的方法直接在神经网络的参数上优化智能体的策略。
![](images/Pasted%20image%2020230818171154.png)


# 同步优势 Actor-Critic
同步优势 Actor-Critic（Synchronous Advantage Actor-Critic，A2C）只是在 Actor-Critic 算法的基础上增加了并行计算的设计。 如图 5.2 所示，全局行动者和全局批判者在 Master 节点维护。每个 Worker 节点的增强学习 智能体通过协调器和全局行动者、全局批判者对话。在这个设计中，协调器负责收集各个 Worker 节点上与环境交互的经验（Experience），然后根据收集到的轨迹执行一步更新。更新之后，全局 行动者被同步到各个 Worker 上继续和环境交互。在 Master 节点上，全局行动者和全局批判者的 学习方法和 Actor-Critic 算法中行动者和批判者的学习方法一致，都是使用 TD 平方误差作为批 判者的损失函数，以及 TD 误差的策略梯度来更新行动者的。
![](images/Pasted%20image%2020230818215349.png)
在这种设计下，Worker 节点只负责和环境交互。所有的计算和更新都发生在 Master 节点
![](images/Pasted%20image%2020230818215910.png)
# 异步优势 Actor-Critic
异步优势 Actor-Critic（Asynchronous Advantage Actor-Critic, A3C）(Mnih et al., 2016) 是上一 节中 A2C 的异步版本。在 A3C 的设计中，协调器被移除。每个 Worker 节点直接和全局行动者和 全局批判者进行对话。Master 节点则不再需要等待各个 Worker 节点提供的梯度信息，而是在每 次有 Worker 节点结束梯度计算的时候直接更新全局 Actor-Critic。由于不再需要等待，A3C 有比 A2C 更高的计算效率。但是同样也由于没有协调器协调各个 Worker 节点，Worker 节点提供梯度 信息和全局 Actor-Critic 的一致性不再成立，即每次 Master 节点从 Worker 节点得到的梯度信息很 可能不再是当前全局 Actor-Critic 的梯度信息。

![](images/Pasted%20image%2020230818215920.png)



# TRPO
我们会开发一个能更好处理步长的策略梯度算法。这个算法的思想基于信赖域的 想法，所以被称为信赖域策略优化算法（Trust Region Policy Optimization，TRPO）(Schulman et al., 2015)。注意到，我们的目标是找到一个比原策略 πθ 更好的策略 π ′ θ。下述引理为 πθ 和 π ′ θ 的性能 提供了一个很深刻的联系：从 πθ 到 π ′ θ 在性能上的提升，可以由 πθ 的优势函数 Aπθ (s, a) 来计算。

# 深度确定性策略梯度DDPG
近年来，将深度 Q 网络算法与 Actor-Critic 方法 相结合的算法愈加流行，如深度确定性策略梯度（Deep Deterministic Policy Gradient，DDPG）算 法。这些算法结合了深度 Q 网络和 Actor-Critic 方法的优点，在大多数环境特别是连续动作空 间的环境中表现出优越的性能。

由于深度 Q 网络的存 在，Actor-Critic 方法转化为离线策略方法，可以使用回放缓存的样本对网络进行训练，从而提高 采样效率。从回放缓存中随机采样也可以打乱数据的序列关系，最小化样本之间的相关性，从而 使价值函数的学习更加稳定。Actor-Critic 方法使得我们可以通过网络学习策略函数 π，便于处理 深度 Q 网络很难解决的具有高维或连续动作空间的问题
![](images/Pasted%20image%2020230818222844.png)



深度确定性策略梯度算法可以看作是深度 Q 网络算法在连续动作空间 中的扩展。它可以解决深度 Q 网络算法无法直接应用于连续动作空间的问题
深度确定性策略 梯度算法同时建立 Q 值函数（Critic）和策略函数（Actor）。Q 值函数（Critic）与深度 Q 网络算 法相同，通过时间差分方法进行更新。策略函数（Actor）利用 Q 值函数（Critic）的估计，通过 策略梯度方法进行更新。
在深度确定性策略梯度算法中，Actor 是一个确定性策略函数，表示为 π(s)，待学习参数表 示为 $θ_π$。每个动作直接由 $A_t = π(S_t|{θ_t}^π )$ 计算，不需要从随机策略中采样。

这里，一个关键问题是如何平衡这种确定性策略的探索和利用（Exploration and Exploitation）。 深度确定性策略梯度算法通过在训练过程中添加随机噪声解决该问题。每个输出动作添加噪声 N，此时有动作为![](images/Pasted%20image%2020230818223518.png)
其中 N 可以根据具体任务进行选择

动作价值函数$Q(s, a|θ^Q)$ 和深度 Q 网络算法一样，通过贝尔 曼方程（Bellman Equations）进行更新![](images/Pasted%20image%2020230818223642.png)
![](images/Pasted%20image%2020230818223920.png)
此外，深度确定性策略梯度算法采用了类似深度 Q 网络算法的目标网络，但这里通过指数平滑方法而不是直接替换参数来更新目标网络：
![](images/Pasted%20image%2020230818224005.png)
由于参数 ρ ≪ 1，目标网络的更新缓慢且平稳，这种方式提高了学习的稳定性。
![](images/Pasted%20image%2020230818224026.png)

























