策略梯度方法可能是目前强化学习中使用最广泛的方法

这个不能解决非常大的行动空间的问题
但可以解决动作空间是连续但低维的


对于模仿学习，我们讨论了这样一个事实：有时人类很难写下奖励函数，因此对他们来说，演示策略可能会更容易。
类似地，在某些情况下，也许写下策略空间的参数化比写下状态行动价值函数空间的参数化更容易。![](images/Pasted%20image%2020230803224714.png)
缺点就是一般只能收敛到局部最优。

策略搜索方法可以使用无梯度的方式优化：
![](images/Pasted%20image%2020230803230538.png)
使用无梯度可能更容易达到全局最优


策略梯度的假设： ![](images/Pasted%20image%2020230803231213.png)
然后是为了得到最大的V
![](images/Pasted%20image%2020230803231329.png)
计算方法： ![](images/Pasted%20image%2020230803231456.png)
这是有限差分近似。


也可以直接计算策略梯度![](images/Pasted%20image%2020230803232334.png)
![](images/Pasted%20image%2020230803232352.png)是可微的意思。
![](images/Pasted%20image%2020230803232823.png)
然后就对这个进行求梯度
![](images/Pasted%20image%2020230803233111.png)
这个叫做likelihood ratio
![](images/Pasted%20image%2020230803233228.png)

![](images/Pasted%20image%2020230803233827.png)
R是奖励的和，那么后面这个需要进行求梯度
![](images/Pasted%20image%2020230803233955.png)
得到只有最后一项



有一个参数化的策略 π ,和他对应的价值V
![](images/Pasted%20image%2020230804102831.png)
就是要找到最大化V的策略
价值函数取决于策略，而策略取决于参数。
![](images/Pasted%20image%2020230804104057.png)
如何保证是单调改进而不是时好时坏
这是V对θ的梯度
![](images/Pasted%20image%2020230804110558.png)

但是上面这种方式的方差是很大的，为了减少方差，使用baseline
![](images/Pasted%20image%2020230804110759.png)
Gt是我们从这个时间步到本集结束所获得的奖励
![](images/Pasted%20image%2020230804111950.png)
b是无偏差的，最好的b的取值就是状态值函数V(s)，即![](images/Pasted%20image%2020230804112102.png)

b 只是状态的函数，它们都是无偏的。






