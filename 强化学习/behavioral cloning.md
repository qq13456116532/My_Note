这里是把他作为一个标准的监督学习
![](images/Pasted%20image%2020230803211314.png)
![](images/Pasted%20image%2020230803211523.png)

![](images/Pasted%20image%2020230803212749.png)
![](images/Pasted%20image%2020230803212806.png)
专家可以给你在某种state下行动的参考
一个专家（可以是人类，也可以是更优秀的AI系统）会演示如何在给定的环境中做出正确的决策，比如在游戏中如何移动，或者在自动驾驶汽车中如何控制车辆等。然后，机器学习算法会尝试模仿这些专家的行为，来学习如何做出决策。
但是如果你需要一个专家在每一步都在旁边指导，或者在整个轨迹（也就是一系列决策和结果）结束后回过头来给出所有的标签，那么这种方法的成本会非常高。也就是说，持续地需要专家的时间和经验来指导机器学习的过程，或者让专家来提供所有正确的行为标签，这会是非常昂贵和耗时的。


专家只在AI系统的动作与专家会采取的动作不同的时候才会干预并提供标签。换句话说，如果AI系统采取的动作与专家会采取的动作一样，那么专家就不干预。
但是这种成本仍然很高



# 逆向强化学习

feature-based reward function：
![](images/Pasted%20image%2020230803215217.png)
在没有任何最优性假设的时候，奖励R能否推断？这是几乎不可能的
如果教师的策略是最优性，那么得到的奖励函数R也不一定是唯一的。
![](images/Pasted%20image%2020230803220418.png)
根据上面这个公式来做学徒学习
![](images/Pasted%20image%2020230803220951.png)

![](images/Pasted%20image%2020230803221515.png)




![](images/Pasted%20image%2020230803222129.png)
但是前面要求的是expert policy一定是最优情况。
![](images/Pasted%20image%2020230803222358.png)这个是指norm2，如果是1那么是指norm1，范数
![](images/Pasted%20image%2020230803223205.png)
















