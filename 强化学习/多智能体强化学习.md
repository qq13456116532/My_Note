# 独立Q学习independent Q-learning (IQL)
独立Q学习一般是多智能体强化学习的基础。
在这种方法中，每个智能体都使用自己的Q学习算法独立地学习策略，就好像其他智能体是环境的一部分一样。这意味着每个智能体都不直接考虑其他智能体的策略或行为变化。
1. **简单性**：IQL的一个主要优势是其简单性。每个智能体都独立地应用标准的Q学习，不需要任何特殊的适应或修改。
2. **非平稳性问题**：尽管IQL方法很简单，但它面临一个主要问题，即其他智能体的策略可能会随时间变化，从而使每个智能体面临的环境变得非平稳。这种非平稳性使得学习变得困难，尤其是当使用深度Q学习和经验回放记忆（Experience Replay Memory）时。
3. **缺乏协调**：由于每个智能体都是独立地学习，因此它们之间可能缺乏必要的协调，这可能导致亚最优或不稳定的全局行为。
虽然有上面这些缺点，IQL还是在实际中产生了比较好的效果。

由于联合行动空间的指数级增长，尝试学习一个基于所有智能体的联合行动的Q函数变得非常不切实际，特别是当智能体数量增加时。而IQL，由于它只考虑每个智能体的行动，不需要处理这种指数级增长的复杂性，从而在伸缩性上更有优势




**回到主题**， 
多智能体分两类：
- 基于cooperation(协作)的算法，如何通过集中式的协同训练得到好的策略
- 基于communication(通信)的算法，建立智能体之间的通信来促进协同


集中式学习有这样的问题： 
- 在大规模多智能体中算法扩展性差
- 某一个智能体学到了有用策略，其他智能体就会选择懒惰的策略，因为其余智能体在学习新策略时会阻碍已经学到策略的智能体，导致全局奖励下降。


















