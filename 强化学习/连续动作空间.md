像SAC这种算法既可以实现连续动作，也可以实现连续空间的基本方法其实是使用高斯分布

策略$π(a|s)$通常由一个神经网络参数化，并且通常选择高斯分布作为输出。具体来说，神经网络输出动作$a$ 的均值 $μ$ 和标准差 $σ$，然后从这个分布中采样动作
然后对于高斯分布，$log π(a|s)$可以直接从 $μ$ 和 $σ$ 计算出来。具体的数学表达式为：
![](images/Pasted%20image%2020230923081719.png)
这里，$μ$ 和 $σ$ 是神经网络基于当前状态 $s$ 的输出。

通过这种方式，SAC 不仅可以计算出动作的概率密度，还可以计算其对数概率，这对于算法的训练是非常重要的。

![](images/Pasted%20image%2020230923101225.png)
这个是SAC的Actor的神经网络输出，这个方法就是在求 $log π(a|s)$，具体来看下方法内部：
![](images/Pasted%20image%2020230923102009.png)
这个方法的公式是这样的： 
![](images/Pasted%20image%2020230923102345.png)

我们说这个和前面的公式是一致的，可以看推导：
![](images/Pasted%20image%2020230923102559.png)
所以这就是对连续动作空间的支持。



这是他们做连续动作空间的方法，那么如果是离散动作空间，对于离散动作，通常使用 Softmax 函数来从神经网络的输出计算动作概率
![](images/Pasted%20image%2020230923104031.png)
其中，$Q(s,a)$ 是 $Q$-值函数，表示在状态 $s$ 下选择动作 $a$ 的预期回报。
在离散动作空间中，对数概率 $log π(a∣s)$ 就可以直接从 Softmax 输出中计算

























