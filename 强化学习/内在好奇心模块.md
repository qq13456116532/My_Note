Intrinsic Curiosity Module
在许多情况下，传统的RL算法主要依赖于从环境中获得的外部奖励信号来指导学习，但在许多任务中，外部奖励可能稀缺或非常延迟，这使得学习变得困难。为了解决这个问题，ICM引入了一种内在奖励机制来促进智能体的探索行为。
![](images/Pasted%20image%2020231018191233.png)
增加ICM模块后，随之增加了一个好奇心的奖励。也就是说，一个 $s$ ，一个 $a$ 进入网络后得到两个奖励，分别是 $r$ 和 $r^{i}$ ， $r$ 是Env给的， $r^{i}$ 是ICM给的，RL的目的是使两个奖励的加和达到最大。

$r^{i}$ 的输出方法如下：
![](images/Pasted%20image%2020231018191324.png)
输入有三个， $a_t$ ， $s_t$ ， $s_{t+1}$ 。 $a_t$ ， $s_t$ 输入Network1得到一个预估的 $\hat s_{t+1}$ ，判断 $\hat s_{t+1}$ 和 $s_{t+1}$ 的区别大小，如果区别大的话， $r_t^{i}$ 就很大，如果区别小的话， $r_t^{i}$ 就很小。也就是未来的状态越难被预测，得到的奖励就越大


可能在某个游戏里面，背景会有风吹草动、会有树叶飘动这种无关紧要的事情。也许树叶飘动这件事，是很难被预测的，对智能体来说，它在某一个状态什么都不做，就看着树叶飘动，发现树叶飘动是没有办法预测的，接下来它就会一直看树叶飘动。所以智能体仅仅有好奇心是不够的，还要让它知道，什么事情是真正重要的。
![](images/Pasted%20image%2020231018191624.png)

于是，增加Network2，用来判断这个东西是否与实验结果相关。
![](images/Pasted%20image%2020231018191846.png)
 $s_t$ 和 $s_{t+1}$ 会经过Feature Ext，从而生成两个向量 $\phi(s_t)$ 和 $\phi(s_{t+1})$ ，将这两个向量输入到Network2中，生成对 $a_t$ 的预测 $\hat a_t$ ，比较 $\hat a_t$ 和 $a_t$ 的关系，如果区别不大，证明这个东西是和结果有关系的，但是如果区别大，就证明这是一个无关紧要的东西。Network1和之前保持一致。














