
**模型预测控制**（model predictive control，MPC）算法并不构建一个显式的策略，只根据环境模型来选择当前步要采取的动作。

MPC 方法在每次采取动作时，首先会生成一些候选动作序列，然后根据当前状态来确定每一条候选序列能得到多好的结果，最终选择结果最好的那条动作序列的第一个动作来执行。因此，在使用 MPC 方法时，主要在两个过程中迭代，
- 一是根据历史数据学习环境模型$\hat{P}(s,a)$，
- 二是在和真实环境交互过程中用环境模型来选择动作。

在第k步时，我们要想做的就是最大化智能体的累积奖励
![](images/Pasted%20image%2020230825232520.png)
这里H是推演的长度，![](images/Pasted%20image%2020230825232544.png)是表示从所有动作序列中选取累积奖励最大的序列


MPC 方法中的一个关键是如何生成一些候选动作序列，候选动作生成的好坏将直接影响到 MPC 方法得到的动作。生成候选动作序列的过程我们称为**打靶**

- **随机打靶法**（random(随机) shooting method）的做法便是随机生成$N$条动作序列，即在生成每条动作序列的每一个动作时，都是从动作空间中随机采样一个动作，最终组合成$N$条长度为$N$的动作序列。

- **交叉熵方法**（cross entropy method，CEM）是一种进化策略方法，它的核心思想是维护一个带参数的分布，根据每次采样的结果来更新分布中的参数，使得分布中能获得**较高累积奖励**的动作序列的**概率比较高**。相比于随机打靶法，交叉熵方法能够利用之前采样到的比较好的结果，在一定程度上减少采样到一些较差动作的概率，从而使得算法更加高效。对于一个与连续动作交互的环境来说，每次交互时交叉熵方法的做法如下：
![](images/Pasted%20image%2020230825233233.png)


#  PETS 算法
带有轨迹采样的概率集成（probabilistic ensembles with trajectory sampling，PETS）是一种使用 MPC 的**基于模型**的强化学习算法。在 PETS 中，环境模型采用了集成学习的方法，即会构建多个环境模型，然后用这多个环境模型来进行预测，最后使用 CEM 进行模型预测控制。

在强化学习中，与智能体交互的环境是一个动态系统，所以拟合它的环境模型也通常是一个动态模型。我们通常认为一个系统中有两种不确定性，分别是**偶然不确定性**（aleatoric uncertainty和**认知不确定性**（epistemic uncertainty）
偶然不确定性是由于系统中本身存在的随机性引起的，而认知不确定性是由“见”过的数据较少导致的自身认知的不足而引起的

