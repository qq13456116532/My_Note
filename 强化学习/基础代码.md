运行环境:  
python=3.9  
pytorch=1.12.1  
gym=0.26.2
# 贪婪算法

下面代码是一个使用贪婪算法实现的老虎机模拟器。它的目标是通过尽可能地选择平均奖励最高的老虎机来最大化总奖励。

```python
import numpy as np  
import random  
  
# 初始化每个老虎机的中奖概率，这里我们有10个老虎机，每个老虎机的中奖概率是0-1之间的均匀分布  
probs = np.random.uniform(size=10)  
# 初始化每个老虎机的奖励记录，开始时每个老虎机的奖励都是1  
rewards = [[1] for _ in range(10)]  
# 定义贪婪算法  
def choose_one():  
	# 有1%的概率随机选择一个老虎机，这是为了增加探索性  
	if random.random() < 0.01:  
		return random.randint(0, 9)  
	# 计算每个老虎机的平均奖励,它遍历rewards列表中的每个子列表（即每个老虎机的奖励记录），并使用np.mean(i)计算每个子列表的平均值  
	rewards_mean = [np.mean(i) for i in rewards]  
	# 选择平均奖励最高的老虎机  
	return np.argmax(rewards_mean)  
  
def try_and_play():  
	# 使用贪婪算法选择一个老虎机  
	i = choose_one()  
	  
	# 模拟玩老虎机，如果随机数小于老虎机的中奖概率，则获得奖励  
	reward = 0  
	if random.random() < probs[i]:  
		reward = 1  
	  
	# 记录这次玩的结果  
	rewards[i].append(reward)  
  
  
def get_result():  
	# 玩5000次老虎机  
	for _ in range(5000):  
		try_and_play()  
	  
	# 计算理论上的最好结果，即所有次数都在中奖概率最高的老虎机上玩  
	target = probs.max() * 5000  
	  
	# 计算实际的结果，即所有老虎机的奖励总和  
	result = sum([sum(i) for i in rewards])  
	  
	return target, result  
  
  
# 获取并打印结果  
target ,result = get_result()  
print("tagrget:"+str(target))  
print("result:"+str(result))
```


## 递减的贪婪算法
如果将上面的选择老虎机的方法修改成如下：
```python
#随机选择的概率递减的贪婪算法
def choose_one():
    #求出现在已经玩了多少次了
    played_count = sum([len(i) for i in rewards])

    #随机选择的概率逐渐下降
    if random.random() < 1 / played_count:
        return random.randint(0, 9)

    #计算每个老虎机的奖励平均
    rewards_mean = [np.mean(i) for i in rewards]

    #选择期望奖励估值最大的拉杆
    return np.argmax(rewards_mean)
```
这就是递减的贪婪算法。在学习的早期阶段，智能体更倾向于探索，即以较高的概率选择随机的动作；随着学习的进行，这个探索的概率会逐渐降低，智能体会更倾向于利用已知的信息，即以较高的概率选择当前认为最优的动作。


## 上置信界算法UCB
Upper Confidence Bound，UCB算法的基本思想是，对于每个老虎机，我们不仅考虑其平均奖励（即我们对其价值的估计），还考虑我们对其价值的不确定性。不确定性越大，我们就越有可能选择这个老虎机，以获取更多的信息。代码如下：

```python
#随机选择的概率递减的贪婪算法
def choose_one():
    #求出每个老虎机各玩了多少次
    played_count = [len(i) for i in rewards]
    played_count = np.array(played_count)

    #求出上置信界
    #分子是总共玩了多少次,取根号后让他的增长速度变慢
    #分母是每台老虎机玩的次数,乘以2让他的增长速度变快
    #随着玩的次数增加,分母会很快超过分子的增长速度,导致分数越来越小
    #具体到每一台老虎机,则是玩的次数越多,分数就越小,也就是ucb的加权越小
    #所以ucb衡量了每一台老虎机的不确定性,不确定性越大,探索的价值越大
    fenzi = played_count.sum()**0.5
    fenmu = played_count * 2
    ucb = fenzi / fenmu

    #ucb本身取根号
    #大于1的数会被缩小,小于1的数会被放大,这样保持ucb恒定在一定的数值范围内
    ucb = ucb**0.5

    #计算每个老虎机的奖励平均
    rewards_mean = [np.mean(i) for i in rewards]
    rewards_mean = np.array(rewards_mean)

    #ucb和期望求和
    ucb += rewards_mean

    return ucb.argmax()
```
在这个函数中，首先计算每个老虎机被玩的次数，然后根据这些次数计算UCB。UCB的计算公式是`fenzi / fenmu`，其中`fenzi`是总的玩的次数的平方根，`fenmu`是每个老虎机被玩的次数乘以2。这个公式的意义是，随着一个老虎机被玩的次数的增加，其UCB会逐渐减小，这意味着我们对这个老虎机的不确定性在减小。同时，随着总的玩的次数的增加，所有老虎机的UCB都会增加，这意味着我们对所有老虎机的不确定性在增加。

## 汤普森采样

```python
def choose_one():
    #求出每个老虎机出1的次数+1
    count_1 = [sum(i) + 1 for i in rewards]
    #求出每个老虎机出0的次数+1
    count_0 = [sum(1 - np.array(i)) + 1 for i in rewards]
    #按照beta分布计算奖励分布,这可以认为是每一台老虎机中奖的概率
    beta = np.random.beta(count_1, count_0)
    return beta.argmax()
```
使用Beta分布生成一个随机数。Beta分布是一种在0到1之间的连续概率分布，它的形状由两个参数（在这里是中奖次数和未中奖次数）决定。在这个场景中，Beta分布可以被看作是我们对每个老虎机中奖概率的不确定性的表示。
最后，选择生成的随机数最大的老虎机

## 动态规划DP
![[Pasted image 20230710000300.png]]
在一个4x12的格子环境中，一个智能体可以尽快地到达目标位置，同时避免陷阱。
## 策略迭代算法
这个算法中是使用 Q<sub>s,a</sub>来更新Π

```python
#获取一个格子的状态
def get_state(row, col):
    if row != 3:
        return 'ground'

    if row == 3 and col == 0:
        return 'ground'

    if row == 3 and col == 11:
        return 'terminal'

    return 'trap'
#在一个格子里做一个动作
def move(row, col, action):
    #如果当前已经在陷阱或者终点，则不能执行任何动作，反馈都是0
    if get_state(row, col) in ['trap', 'terminal']:
        return row, col, 0

    #↑
    if action == 0:
        row -= 1

    #↓
    if action == 1:
        row += 1

    #←
    if action == 2:
        col -= 1

    #→
    if action == 3:
        col += 1

    #不允许走到地图外面去
    row = max(0, row)
    row = min(3, row)
    col = max(0, col)
    col = min(11, col)

    #是陷阱的话，奖励是-100，否则都是-1
    #这样强迫了机器尽快结束游戏,因为每走一步都要扣一分
    #结束最好是以走到终点的形式,避免被扣100分
    reward = -1
    if get_state(row, col) == 'trap':
        reward = -100

    return row, col, reward
```
如果当前位置是陷阱或目标位置，那么无法执行任何动作，奖励为0。否则，执行动作后，如果新位置是陷阱，奖励为-100，否则奖励为-1。

```python
import numpy as np

#初始化每个格子的价值，全设为0
values = np.zeros([4, 12])

#初始化每个格子下采用动作的概率，全设为0.25，因为每个状态下四个动作都有可能执行
pi = np.ones([4, 12, 4]) * 0.25


```
然后是计算 Q<sub>s,a</sub>,当前状态下执行一个动作的分数，是reward+γValue<sub>t+1</sub>

```python
#计算在一个状态下执行动作的分数
def get_qsa(row, col, action):
    #在当前状态下执行动作,得到下一个状态和reward
    next_row, next_col, reward = move(row, col, action)

    #计算下一个状态的分数,取values当中记录的分数即可,0.9是折扣因子
    value = values[next_row, next_col] * 0.9

    #如果下个状态是终点或者陷阱,则下一个状态的分数是0
    if get_state(next_row, next_col) in ['trap', 'terminal']:
        value = 0

    #动作的分数本身就是reward,加上下一个状态的分数
    return value + reward
```
下面是对每个状态的Value重新进行计算。

```python
#策略评估
def get_values():

    #初始化一个新的values,重新评估所有格子的分数
    new_values = np.zeros([4, 12])

    #遍历所有格子
    for row in range(4):
        for col in range(12):

            #计算当前格子4个动作分别的分数
            action_value = np.zeros(4)

            #遍历所有动作
            for action in range(4):
                action_value[action] = get_qsa(row, col, action)

            #每个动作的分数和它的概率相乘
            action_value *= pi[row, col]

            #最后这个格子的分数,等于该格子下所有动作的分数求和
            new_values[row, col] = action_value.sum()

    return new_values
```
下面函数用于提升当前策略。它遍历所有位置，对每个位置，计算执行每个动作的预期回报，然后选择预期回报最大的动作，更新该动作的概率为1，其他动作的概率为0。

```python
#策略评估
def get_values():

    #初始化一个新的values,重新评估所有格子的分数
    new_values = np.zeros([4, 12])

    #遍历所有格子
    for row in range(4):
        for col in range(12):

            #计算当前格子4个动作分别的分数
            action_value = np.zeros(4)

            #遍历所有动作
            for action in range(4):
                action_value[action] = get_qsa(row, col, action)

            #每个动作的分数和它的概率相乘
            action_value *= pi[row, col]

            #最后这个格子的分数,等于该格子下所有动作的分数求和
            new_values[row, col] = action_value.sum()

    return new_values
```
这里就是进行强化学习训练
```python
#循环迭代策略评估和策略提升,寻找最优解
for _ in range(10):
    for _ in range(100):
        values = get_values()
    pi = get_pi()
print("values: {}".format(values))  
print("pi: {}".format(pi))  
print(pi.shape)
```
后面是打印游戏并进行测试：

```python
#打印游戏，方便测试
def show(row, col, action):
    graph = [
        '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□',
        '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□',
        '□', '□', '□', '□', '□', '□', '□', '□', '□', '○', '○', '○', '○', '○',
        '○', '○', '○', '○', '○', '❤'
    ]

    action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]

    graph[row * 12 + col] = action

    graph = ''.join(graph)

    for i in range(0, 4 * 12, 12):
        print(graph[i:i + 12])

from IPython import display
import time


def test():
    #起点在0,0
    row = 0
    col = 0

    #最多玩N步
    for _ in range(200):

        #选择一个动作
        action = np.random.choice(np.arange(4), size=1, p=pi[row, col])[0]

        #打印这个动作
        display.clear_output(wait=True)
        time.sleep(0.1)
        show(row, col, action)

        #执行动作
        row, col, reward = move(row, col, action)

        #获取当前状态，如果状态是终点或者掉陷阱则终止
        if get_state(row, col) in ['trap', 'terminal']:
            break

```
打印所有格子的倾向
```python
#打印所有格子的动作倾向
for row in range(4):
    line = ''
    for col in range(12):
        action = pi[row, col].argmax()
        action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]
        line += action
    print(line)
```



## 价值迭代算法
价值迭代算法在每次迭代中，直接选择每个状态下能获得最大预期回报的动作，而不是计算每个动作的预期回报然后根据动作的概率求和。这个改变使得算法更加贪婪（greedy），因为它总是选择当前看起来最好的动作，而不是基于当前策略的概率分布选择动作。这可能会导致算法找到的策略不是最优策略，但在某些情况下，这种方法可以更快地收敛到一个足够好的策略。
只需要将上面的get_Values()方法修改成如下：

```python
#策略评估
def get_values():

    #初始化一个新的values,重新评估所有格子的分数
    new_values = np.zeros([4, 12])

    #遍历所有格子
    for row in range(4):
        for col in range(12):

            #计算当前格子4个动作分别的分数
            action_value = np.zeros(4)

            #遍历所有动作
            for action in range(4):
                action_value[action] = get_qsa(row, col, action)
            """和策略迭代算法唯一的不同点"""
            #求每一个格子的分数，等于该格子下所有动作的最大分数
            new_values[row, col] = action_value.max()

    return new_values

```


## 实践--冰湖

```python
import gym

#创建环境
#is_slippery控制会不会滑
#map_name决定地图的尺寸,还可以取8x8
#desc决定地形
env = gym.make('FrozenLake-v1',
               render_mode='human',
               is_slippery=False,
               map_name='4x4',
               desc=['SFFF', 'FHFH', 'FFFH', 'HFFG'])
env.reset()

#解封装才能访问状态转移矩阵P
env = env.unwrapped

#查看冰湖这个游戏的状态列表
#一共4*4=16个状态
#每个状态下都可以执行4个动作
#每个动作执行完，都有概率得到3个结果
#(0.3333333333333333, 0, 0.0, False)这个数据结构表示(概率，下个状态，奖励，是否结束)
print(len(env.P))
print( env.P[0])

import numpy as np

#初始化每个格子的价值
values = np.zeros(16)

#初始化每个格子下采用动作的概率
pi = np.ones([16, 4]) * 0.25

#两个算法都是可以的
algorithm = '策略迭代'
# algorithm = '价值迭代'

#计算qsa
def get_qsa(state, action):
    value = 0.0

    #每个动作都会有三个不同的结果，这里要按概率把他们加权求和
    for prop, next_state, reward, over in env.P[state][action]:

        #计算下个状态的分数,取values当中记录的分数,再打个折扣
        next_value = values[next_state] * 0.9

        #如果下个状态是终点或者陷阱，则下个状态的分数是0
        if over:
            next_value = 0

        #动作的分数就是reward,和下个状态的分数相加就是最终的分数了
        next_value += reward

        #因为下个状态是概率出现了,所以这里要乘以概率
        next_value *= prop

        value += next_value

    return value


#策略评估
def get_values():
    #初始化一个新的values,重新评估所有格子的分数
    new_values = np.zeros([16])

    #遍历所有格子
    for state in range(16):

        #计算当前格子4个动作分别的分数
        action_value = np.zeros(4)

        #遍历所有动作
        for action in range(4):
            action_value[action] = get_qsa(state, action)

        if algorithm == '策略迭代':
            #每个动作的分数和它的概率相乘
            action_value *= pi[state]
            #最后这个格子的分数,等于该格子下所有动作的分数求和
            new_values[state] = action_value.sum()

        if algorithm == '价值迭代':
            #求每一个格子的分数，等于该格子下所有动作的最大分数
            new_values[state] = action_value.max()

    return new_values

#策略提升
def get_pi():
    #重新初始化每个格子下采用动作的概率,重新评估
    new_pi = np.zeros([16, 4])

    #遍历所有格子
    for state in range(16):

        #计算当前格子4个动作分别的分数
        action_value = np.zeros(4)

        #遍历所有动作
        for action in range(4):
            action_value[action] = get_qsa(state, action)

        #计算当前state下，达到最大分数的动作有几个
        count = (action_value == action_value.max()).sum()

        #让这些动作均分概率
        for action in range(4):
            if action_value[action] == action_value.max():
                new_pi[state, action] = 1 / count
            else:
                new_pi[state, action] = 0

    return new_pi

#循环迭代策略评估和策略提升，寻找最优解
for _ in range(10):
    for _ in range(100):
        values = get_values()
    pi = get_pi()
print("values: {}".format(values))
print("pi: {}".format(pi))
print(pi.shape)

from IPython import display
import time


def play():
    env.reset()

    #起点在0
    index = 0

    #最多玩N步
    for i in range(200):
        #选择一个动作
        action = np.random.choice(np.arange(4), size=1, p=pi[index])[0]

        #执行动作
        index, reward, terminated, truncated, _ = env.step(action)

        #打印动画
        # display.clear_output(wait=True)
        time.sleep(0.1)
        # show()
        env.render()

        #获取当前状态，如果状态是终点或者掉陷阱则终止
        if terminated or truncated:
            break

    print(i)


play()

```
结果如下
![[动画.gif]]

## SARSA算法

```python
#获取一个格子的状态
def get_state(row, col):
    if row != 3:
        return 'ground'

    if row == 3 and col == 0:
        return 'ground'

    if row == 3 and col == 11:
        return 'terminal'

    return 'trap'

#在一个格子里做一个动作
def move(row, col, action):
    #如果当前已经在陷阱或者终点，则不能执行任何动作
    if get_state(row, col) in ['trap', 'terminal']:
        return row, col, 0

    #↑
    if action == 0:
        row -= 1

    #↓
    if action == 1:
        row += 1

    #←
    if action == 2:
        col -= 1

    #→
    if action == 3:
        col += 1

    #不允许走到地图外面去
    row = max(0, row)
    row = min(3, row)
    col = max(0, col)
    col = min(11, col)

    #是陷阱的话，奖励是-100，否则都是-1
    reward = -1
    if get_state(row, col) == 'trap':
        reward = -100

    return row, col, reward

import numpy as np

#初始化在每一个格子里采取每个动作的分数,初始化都是0,因为没有任何的知识
Q = np.zeros([4, 12, 4])

import random

#根据状态选择一个动作
def get_action(row, col):
    #有小概率选择随机动作
    if random.random() < 0.1:
        return random.choice(range(4))

    #否则选择分数最高的动作
    return Q[row, col].argmax()

#更新分数，每次更新取决于当前的格子，当前的动作，下个格子，和下个格子的动作
def get_update(row, col, action, reward, next_row, next_col, next_action):

    #计算target
    target = 0.9 * Q[next_row, next_col, next_action]
    target += reward

    #计算value
    value = Q[row, col, action]

    #根据时序差分算法,当前state,action的分数 = 下一个state,action的分数*gamma + reward
    #此处是求两者的差,越接近0越好
    update = target - value

    #这个0.1相当于lr
    update *= 0.1

    #更新当前状态和动作的分数
    return update

#训练
def train():
    for epoch in range(1500):
        #初始化当前位置
        row = random.choice(range(4))
        col = 0

        #初始化第一个动作
        action = get_action(row, col)

        #计算反馈的和，这个数字应该越来越小
        reward_sum = 0

        #循环直到到达终点或者掉进陷阱
        while get_state(row, col) not in ['terminal', 'trap']:

            #执行动作
            next_row, next_col, reward = move(row, col, action)
            reward_sum += reward

            #求新位置的动作
            next_action = get_action(next_row, next_col)

            #更新分数
            update = get_update(row, col, action, reward, next_row, next_col,
                                next_action)
            Q[row, col, action] += update

            #更新当前位置
            row = next_row
            col = next_col
            action = next_action

        if epoch % 150 == 0:
            print(epoch, reward_sum)


train()
#打印所有格子的动作倾向
for row in range(4):
    line = ''
    for col in range(12):
        action = Q[row, col].argmax()
        action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]
        line += action
    print(line)

```
这里面get_update是SARSA主要内容。

## Q-learning
这里和SARSA的区别就是在get_update上，不需要传入下一个动作

```python
#获取一个格子的状态
def get_state(row, col):
    if row != 3:
        return 'ground'

    if row == 3 and col == 0:
        return 'ground'

    if row == 3 and col == 11:
        return 'terminal'

    return 'trap'

#在一个格子里做一个动作
def move(row, col, action):
    #如果当前已经在陷阱或者终点，则不能执行任何动作
    if get_state(row, col) in ['trap', 'terminal']:
        return row, col, 0

    #↑
    if action == 0:
        row -= 1

    #↓
    if action == 1:
        row += 1

    #←
    if action == 2:
        col -= 1

    #→
    if action == 3:
        col += 1

    #不允许走到地图外面去
    row = max(0, row)
    row = min(3, row)
    col = max(0, col)
    col = min(11, col)

    #是陷阱的话，奖励是-100，否则都是-1
    reward = -1
    if get_state(row, col) == 'trap':
        reward = -100

    return row, col, reward


import numpy as np

#初始化在每一个格子里采取每个动作的分数,初始化都是0,因为没有任何的知识
Q = np.zeros([4, 12, 4])

Q.shape


import random


#根据状态选择一个动作
def get_action(row, col):
    #有小概率选择随机动作
    if random.random() < 0.1:
        return random.choice(range(4))

    #否则选择分数最高的动作
    return Q[row, col].argmax()

def get_update(row, col, action, reward, next_row, next_col):
    #target为下一个格子的最高分数，这里的计算和下一步的动作无关
    target = 0.9 * Q[next_row, next_col].max()
    #加上本步的分数
    target += reward

    #value为当前state和action的分数
    value = Q[row, col, action]

    #根据时序差分算法,当前state,action的分数 = 下一个state,action的分数*gamma + reward
    #此处是求两者的差,越接近0越好
    update = target - value

    #这个0.1相当于lr
    update *= 0.1

    return update


#训练
def train():
    for epoch in range(1500):
        #初始化当前位置
        row = random.choice(range(4))
        col = 0

        #初始化第一个动作
        action = get_action(row, col)

        #计算反馈的和，这个数字应该越来越小
        reward_sum = 0

        #循环直到到达终点或者掉进陷阱
        while get_state(row, col) not in ['terminal', 'trap']:

            #执行动作
            next_row, next_col, reward = move(row, col, action)
            reward_sum += reward

            #求新位置的动作
            next_action = get_action(next_row, next_col)

            #计算分数
            update = get_update(row, col, action, reward, next_row, next_col)

            #更新分数
            Q[row, col, action] += update

            #更新当前位置
            row = next_row
            col = next_col
            action = next_action

        if epoch % 100 == 0:
            print(epoch, reward_sum)


train()
#打印所有格子的动作倾向
for row in range(4):
    line = ''
    for col in range(12):
        action = Q[row, col].argmax()
        action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]
        line += action
    print(line)

```


## Dyna-Q
```python
#获取一个格子的状态
def get_state(row, col):
    if row != 3:
        return 'ground'

    if row == 3 and col == 0:
        return 'ground'

    if row == 3 and col == 11:
        return 'terminal'

    return 'trap'

#在一个格子里做一个动作
def move(row, col, action):
    #如果当前已经在陷阱或者终点，则不能执行任何动作
    if get_state(row, col) in ['trap', 'terminal']:
        return row, col, 0

    #↑
    if action == 0:
        row -= 1

    #↓
    if action == 1:
        row += 1

    #←
    if action == 2:
        col -= 1

    #→
    if action == 3:
        col += 1

    #不允许走到地图外面去
    row = max(0, row)
    row = min(3, row)
    col = max(0, col)
    col = min(11, col)

    #是陷阱的话，奖励是-100，否则都是-1
    reward = -1
    if get_state(row, col) == 'trap':
        reward = -100

    return row, col, reward


import numpy as np

#初始化在每一个格子里采取每个动作的分数,初始化都是0,因为没有任何的知识
Q = np.zeros([4, 12, 4])

#保存历史数据,键是(row,col,action),值是(next_row,next_col,reward)
history = dict()


import random


#根据状态选择一个动作
def get_action(row, col):
    #有小概率选择随机动作
    if random.random() < 0.1:
        return random.choice(range(4))

    #否则选择分数最高的动作
    return Q[row, col].argmax()

def get_update(row, col, action, reward, next_row, next_col):
    #target为下一个格子的最高分数，这里的计算和下一步的动作无关
    target = 0.9 * Q[next_row, next_col].max()
    #加上本步的分数
    target += reward

    #计算value
    value = Q[row, col, action]

    #根据时序差分算法,当前state,action的分数 = 下一个state,action的分数*gamma + reward
    #此处是求两者的差,越接近0越好
    update = target - value

    #这个0.1相当于lr
    update *= 0.1

    return update
import random


def q_planning():
    #Q planning循环,相当于是在反刍历史数据,随机取N个历史数据再进行离线学习
    for _ in range(20):
        #随机选择曾经遇到过的状态动作对
        row, col, action = random.choice(list(history.keys()))

        #再获取下一个状态和反馈
        next_row, next_col, reward = history[(row, col, action)]

        #计算分数
        update = get_update(row, col, action, reward, next_row, next_col)

        #更新分数
        Q[row, col, action] += update

#训练
def train():
    for epoch in range(300):
        #初始化当前位置
        row = random.choice(range(4))
        col = 0

        #初始化第一个动作
        action = get_action(row, col)

        #计算反馈的和，这个数字应该越来越小
        reward_sum = 0

        #循环直到到达终点或者掉进陷阱
        while get_state(row, col) not in ['terminal', 'trap']:

            #执行动作
            next_row, next_col, reward = move(row, col, action)
            reward_sum += reward

            #求新位置的动作
            next_action = get_action(next_row, next_col)

            #计算分数
            update = get_update(row, col, action, reward, next_row, next_col)

            #更新分数
            Q[row, col, action] += update

            #将数据添加到模型中
            history[(row, col, action)] = next_row, next_col, reward

            #反刍历史数据,进行离线学习
            q_planning()

            #更新当前位置
            row = next_row
            col = next_col
            action = next_action

        if epoch % 20 == 0:
            print(epoch, reward_sum)


train()
#打印所有格子的动作倾向
for row in range(4):
    line = ''
    for col in range(12):
        action = Q[row, col].argmax()
        action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]
        line += action
    print(line)

```
规划部分则是在每一步的执行之后，Dyna-Q使用其模型来模拟可能的转换，并使用这些模拟的转换来进一步更新其Q-table。这个模型在每一步的执行过程中都会被更新，以便反映智能体从与环境交互中获取的最新信息。
这里的Dyna-Q模型是历史数据。


















