- [[#贪婪算法|贪婪算法]]
	- [[#贪婪算法#递减的贪婪算法|递减的贪婪算法]]
	- [[#贪婪算法#上置信界算法UCB|上置信界算法UCB]]
	- [[#贪婪算法#汤普森采样|汤普森采样]]
	- [[#贪婪算法#动态规划DP|动态规划DP]]
	- [[#贪婪算法#策略迭代算法|策略迭代算法]]
	- [[#贪婪算法#价值迭代算法|价值迭代算法]]
	- [[#贪婪算法#实践--冰湖|实践--冰湖]]
	- [[#贪婪算法#SARSA算法|SARSA算法]]
	- [[#贪婪算法#Q-learning|Q-learning]]
	- [[#贪婪算法#Dyna-Q|Dyna-Q]]
- [[#DQN|DQN]]
	- [[#DQN#解决平衡车|解决平衡车]]
	- [[#DQN#解决倒立摆|解决倒立摆]]
	- [[#DQN#DDQN|DDQN]]
- [[#策略梯度|策略梯度]]
	- [[#策略梯度#REINFORCE|REINFORCE]]






运行环境:  
python=3.9  
pytorch=1.12.1  
gym=0.26.2


# 初始化
## 初始化动作概率
这里一般都是使用Softmax来进行初始化：
![](images/Pasted%20image%2020230811222428.png)
η 是一个用来控制温度的正值参数，$G_i$是一个动作的总奖励值


# 贪婪算法

下面代码是一个使用贪婪算法实现的老虎机模拟器。它的目标是通过尽可能地选择平均奖励最高的老虎机来最大化总奖励。

```python
import numpy as np  
import random  
  
# 初始化每个老虎机的中奖概率，这里我们有10个老虎机，每个老虎机的中奖概率是0-1之间的均匀分布  
probs = np.random.uniform(size=10)  
# 初始化每个老虎机的奖励记录，开始时每个老虎机的奖励都是1  
rewards = [[1] for _ in range(10)]  
# 定义贪婪算法  
def choose_one():  
	# 有1%的概率随机选择一个老虎机，这是为了增加探索性  
	if random.random() < 0.01:  
		return random.randint(0, 9)  
	# 计算每个老虎机的平均奖励,它遍历rewards列表中的每个子列表（即每个老虎机的奖励记录），并使用np.mean(i)计算每个子列表的平均值  
	rewards_mean = [np.mean(i) for i in rewards]  
	# 选择平均奖励最高的老虎机  
	return np.argmax(rewards_mean)  
  
def try_and_play():  
	# 使用贪婪算法选择一个老虎机  
	i = choose_one()  
	  
	# 模拟玩老虎机，如果随机数小于老虎机的中奖概率，则获得奖励  
	reward = 0  
	if random.random() < probs[i]:  
		reward = 1  
	  
	# 记录这次玩的结果  
	rewards[i].append(reward)  
  
  
def get_result():  
	# 玩5000次老虎机  
	for _ in range(5000):  
		try_and_play()  
	  
	# 计算理论上的最好结果，即所有次数都在中奖概率最高的老虎机上玩  
	target = probs.max() * 5000  
	  
	# 计算实际的结果，即所有老虎机的奖励总和  
	result = sum([sum(i) for i in rewards])  
	  
	return target, result  
  
  
# 获取并打印结果  
target ,result = get_result()  
print("tagrget:"+str(target))  
print("result:"+str(result))
```


## 递减的贪婪算法
如果将上面的选择老虎机的方法修改成如下：
```python
#随机选择的概率递减的贪婪算法
def choose_one():
    #求出现在已经玩了多少次了
    played_count = sum([len(i) for i in rewards])

    #随机选择的概率逐渐下降
    if random.random() < 1 / played_count:
        return random.randint(0, 9)

    #计算每个老虎机的奖励平均
    rewards_mean = [np.mean(i) for i in rewards]

    #选择期望奖励估值最大的拉杆
    return np.argmax(rewards_mean)
```
这就是递减的贪婪算法。在学习的早期阶段，智能体更倾向于探索，即以较高的概率选择随机的动作；随着学习的进行，这个探索的概率会逐渐降低，智能体会更倾向于利用已知的信息，即以较高的概率选择当前认为最优的动作。


## 上置信界算法UCB
Upper Confidence Bound，UCB算法的基本思想是，对于每个老虎机，我们不仅考虑其平均奖励（即我们对其价值的估计），还考虑我们对其价值的不确定性。不确定性越大，我们就越有可能选择这个老虎机，以获取更多的信息。代码如下：

```python
#随机选择的概率递减的贪婪算法
def choose_one():
    #求出每个老虎机各玩了多少次
    played_count = [len(i) for i in rewards]
    played_count = np.array(played_count)

    #求出上置信界
    #分子是总共玩了多少次,取根号后让他的增长速度变慢
    #分母是每台老虎机玩的次数,乘以2让他的增长速度变快
    #随着玩的次数增加,分母会很快超过分子的增长速度,导致分数越来越小
    #具体到每一台老虎机,则是玩的次数越多,分数就越小,也就是ucb的加权越小
    #所以ucb衡量了每一台老虎机的不确定性,不确定性越大,探索的价值越大
    fenzi = played_count.sum()**0.5
    fenmu = played_count * 2
    ucb = fenzi / fenmu

    #ucb本身取根号
    #大于1的数会被缩小,小于1的数会被放大,这样保持ucb恒定在一定的数值范围内
    ucb = ucb**0.5

    #计算每个老虎机的奖励平均
    rewards_mean = [np.mean(i) for i in rewards]
    rewards_mean = np.array(rewards_mean)

    #ucb和期望求和
    ucb += rewards_mean

    return ucb.argmax()
```
在这个函数中，首先计算每个老虎机被玩的次数，然后根据这些次数计算UCB。UCB的计算公式是`fenzi / fenmu`，其中`fenzi`是总的玩的次数的平方根，`fenmu`是每个老虎机被玩的次数乘以2。这个公式的意义是，随着一个老虎机被玩的次数的增加，其UCB会逐渐减小，这意味着我们对这个老虎机的不确定性在减小。同时，随着总的玩的次数的增加，所有老虎机的UCB都会增加，这意味着我们对所有老虎机的不确定性在增加。

## 汤普森采样

```python
def choose_one():
    #求出每个老虎机出1的次数+1
    count_1 = [sum(i) + 1 for i in rewards]
    #求出每个老虎机出0的次数+1
    count_0 = [sum(1 - np.array(i)) + 1 for i in rewards]
    #按照beta分布计算奖励分布,这可以认为是每一台老虎机中奖的概率
    beta = np.random.beta(count_1, count_0)
    return beta.argmax()
```
使用Beta分布生成一个随机数。Beta分布是一种在0到1之间的连续概率分布，它的形状由两个参数（在这里是中奖次数和未中奖次数）决定。在这个场景中，Beta分布可以被看作是我们对每个老虎机中奖概率的不确定性的表示。
最后，选择生成的随机数最大的老虎机

## 动态规划DP
![[Pasted image 20230710000300.png]]
在一个4x12的格子环境中，一个智能体可以尽快地到达目标位置，同时避免陷阱。
## 策略迭代算法
这个算法中是使用 Q<sub>s,a</sub>来更新Π

```python
#获取一个格子的状态
def get_state(row, col):
    if row != 3:
        return 'ground'

    if row == 3 and col == 0:
        return 'ground'

    if row == 3 and col == 11:
        return 'terminal'

    return 'trap'
#在一个格子里做一个动作
def move(row, col, action):
    #如果当前已经在陷阱或者终点，则不能执行任何动作，反馈都是0
    if get_state(row, col) in ['trap', 'terminal']:
        return row, col, 0

    #↑
    if action == 0:
        row -= 1

    #↓
    if action == 1:
        row += 1

    #←
    if action == 2:
        col -= 1

    #→
    if action == 3:
        col += 1

    #不允许走到地图外面去
    row = max(0, row)
    row = min(3, row)
    col = max(0, col)
    col = min(11, col)

    #是陷阱的话，奖励是-100，否则都是-1
    #这样强迫了机器尽快结束游戏,因为每走一步都要扣一分
    #结束最好是以走到终点的形式,避免被扣100分
    reward = -1
    if get_state(row, col) == 'trap':
        reward = -100

    return row, col, reward
```
如果当前位置是陷阱或目标位置，那么无法执行任何动作，奖励为0。否则，执行动作后，如果新位置是陷阱，奖励为-100，否则奖励为-1。

```python
import numpy as np

#初始化每个格子的价值，全设为0
values = np.zeros([4, 12])

#初始化每个格子下采用动作的概率，全设为0.25，因为每个状态下四个动作都有可能执行
pi = np.ones([4, 12, 4]) * 0.25


```
然后是计算 Q<sub>s,a</sub>,当前状态下执行一个动作的分数，是reward+γValue<sub>t+1</sub>

```python
#计算在一个状态下执行动作的分数
def get_qsa(row, col, action):
    #在当前状态下执行动作,得到下一个状态和reward
    next_row, next_col, reward = move(row, col, action)

    #计算下一个状态的分数,取values当中记录的分数即可,0.9是折扣因子
    value = values[next_row, next_col] * 0.9

    #如果下个状态是终点或者陷阱,则下一个状态的分数是0
    if get_state(next_row, next_col) in ['trap', 'terminal']:
        value = 0

    #动作的分数本身就是reward,加上下一个状态的分数
    return value + reward
```
下面是对每个状态的Value重新进行计算。

```python
#策略评估
def get_values():

    #初始化一个新的values,重新评估所有格子的分数
    new_values = np.zeros([4, 12])

    #遍历所有格子
    for row in range(4):
        for col in range(12):

            #计算当前格子4个动作分别的分数
            action_value = np.zeros(4)

            #遍历所有动作
            for action in range(4):
                action_value[action] = get_qsa(row, col, action)

            #每个动作的分数和它的概率相乘
            action_value *= pi[row, col]

            #最后这个格子的分数,等于该格子下所有动作的分数求和
            new_values[row, col] = action_value.sum()

    return new_values
```
下面函数用于提升当前策略。它遍历所有位置，对每个位置，计算执行每个动作的预期回报，然后选择预期回报最大的动作，更新该动作的概率为1，其他动作的概率为0。

```python
#策略评估
def get_values():

    #初始化一个新的values,重新评估所有格子的分数
    new_values = np.zeros([4, 12])

    #遍历所有格子
    for row in range(4):
        for col in range(12):

            #计算当前格子4个动作分别的分数
            action_value = np.zeros(4)

            #遍历所有动作
            for action in range(4):
                action_value[action] = get_qsa(row, col, action)

            #每个动作的分数和它的概率相乘
            action_value *= pi[row, col]

            #最后这个格子的分数,等于该格子下所有动作的分数求和
            new_values[row, col] = action_value.sum()

    return new_values
```
这里就是进行强化学习训练
```python
#循环迭代策略评估和策略提升,寻找最优解
for _ in range(10):
    for _ in range(100):
        values = get_values()
    pi = get_pi()
print("values: {}".format(values))  
print("pi: {}".format(pi))  
print(pi.shape)
```
后面是打印游戏并进行测试：

```python
#打印游戏，方便测试
def show(row, col, action):
    graph = [
        '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□',
        '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□', '□',
        '□', '□', '□', '□', '□', '□', '□', '□', '□', '○', '○', '○', '○', '○',
        '○', '○', '○', '○', '○', '❤'
    ]

    action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]

    graph[row * 12 + col] = action

    graph = ''.join(graph)

    for i in range(0, 4 * 12, 12):
        print(graph[i:i + 12])

from IPython import display
import time


def test():
    #起点在0,0
    row = 0
    col = 0

    #最多玩N步
    for _ in range(200):

        #选择一个动作
        action = np.random.choice(np.arange(4), size=1, p=pi[row, col])[0]

        #打印这个动作
        display.clear_output(wait=True)
        time.sleep(0.1)
        show(row, col, action)

        #执行动作
        row, col, reward = move(row, col, action)

        #获取当前状态，如果状态是终点或者掉陷阱则终止
        if get_state(row, col) in ['trap', 'terminal']:
            break

```
打印所有格子的倾向
```python
#打印所有格子的动作倾向
for row in range(4):
    line = ''
    for col in range(12):
        action = pi[row, col].argmax()
        action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]
        line += action
    print(line)
```



## 价值迭代算法
价值迭代算法在每次迭代中，直接选择每个状态下能获得最大预期回报的动作，而不是计算每个动作的预期回报然后根据动作的概率求和。这个改变使得算法更加贪婪（greedy），因为它总是选择当前看起来最好的动作，而不是基于当前策略的概率分布选择动作。这可能会导致算法找到的策略不是最优策略，但在某些情况下，这种方法可以更快地收敛到一个足够好的策略。
只需要将上面的get_Values()方法修改成如下：

```python
#策略评估
def get_values():

    #初始化一个新的values,重新评估所有格子的分数
    new_values = np.zeros([4, 12])

    #遍历所有格子
    for row in range(4):
        for col in range(12):

            #计算当前格子4个动作分别的分数
            action_value = np.zeros(4)

            #遍历所有动作
            for action in range(4):
                action_value[action] = get_qsa(row, col, action)
            """和策略迭代算法唯一的不同点"""
            #求每一个格子的分数，等于该格子下所有动作的最大分数
            new_values[row, col] = action_value.max()

    return new_values

```


## 实践--冰湖

```python
import gym

#创建环境
#is_slippery控制会不会滑
#map_name决定地图的尺寸,还可以取8x8
#desc决定地形
env = gym.make('FrozenLake-v1',
               render_mode='human',
               is_slippery=False,
               map_name='4x4',
               desc=['SFFF', 'FHFH', 'FFFH', 'HFFG'])
env.reset()

#解封装才能访问状态转移矩阵P
env = env.unwrapped

#查看冰湖这个游戏的状态列表
#一共4*4=16个状态
#每个状态下都可以执行4个动作
#每个动作执行完，都有概率得到3个结果
#(0.3333333333333333, 0, 0.0, False)这个数据结构表示(概率，下个状态，奖励，是否结束)
print(len(env.P))
print( env.P[0])

import numpy as np

#初始化每个格子的价值
values = np.zeros(16)

#初始化每个格子下采用动作的概率
pi = np.ones([16, 4]) * 0.25

#两个算法都是可以的
algorithm = '策略迭代'
# algorithm = '价值迭代'

#计算qsa
def get_qsa(state, action):
    value = 0.0

    #每个动作都会有三个不同的结果，这里要按概率把他们加权求和
    for prop, next_state, reward, over in env.P[state][action]:

        #计算下个状态的分数,取values当中记录的分数,再打个折扣
        next_value = values[next_state] * 0.9

        #如果下个状态是终点或者陷阱，则下个状态的分数是0
        if over:
            next_value = 0

        #动作的分数就是reward,和下个状态的分数相加就是最终的分数了
        next_value += reward

        #因为下个状态是概率出现了,所以这里要乘以概率
        next_value *= prop

        value += next_value

    return value


#策略评估
def get_values():
    #初始化一个新的values,重新评估所有格子的分数
    new_values = np.zeros([16])

    #遍历所有格子
    for state in range(16):

        #计算当前格子4个动作分别的分数
        action_value = np.zeros(4)

        #遍历所有动作
        for action in range(4):
            action_value[action] = get_qsa(state, action)

        if algorithm == '策略迭代':
            #每个动作的分数和它的概率相乘
            action_value *= pi[state]
            #最后这个格子的分数,等于该格子下所有动作的分数求和
            new_values[state] = action_value.sum()

        if algorithm == '价值迭代':
            #求每一个格子的分数，等于该格子下所有动作的最大分数
            new_values[state] = action_value.max()

    return new_values

#策略提升
def get_pi():
    #重新初始化每个格子下采用动作的概率,重新评估
    new_pi = np.zeros([16, 4])

    #遍历所有格子
    for state in range(16):

        #计算当前格子4个动作分别的分数
        action_value = np.zeros(4)

        #遍历所有动作
        for action in range(4):
            action_value[action] = get_qsa(state, action)

        #计算当前state下，达到最大分数的动作有几个
        count = (action_value == action_value.max()).sum()

        #让这些动作均分概率
        for action in range(4):
            if action_value[action] == action_value.max():
                new_pi[state, action] = 1 / count
            else:
                new_pi[state, action] = 0

    return new_pi

#循环迭代策略评估和策略提升，寻找最优解
for _ in range(10):
    for _ in range(100):
        values = get_values()
    pi = get_pi()
print("values: {}".format(values))
print("pi: {}".format(pi))
print(pi.shape)

from IPython import display
import time


def play():
    env.reset()

    #起点在0
    index = 0

    #最多玩N步
    for i in range(200):
        #选择一个动作
        action = np.random.choice(np.arange(4), size=1, p=pi[index])[0]

        #执行动作
        index, reward, terminated, truncated, _ = env.step(action)

        #打印动画
        # display.clear_output(wait=True)
        time.sleep(0.1)
        # show()
        env.render()

        #获取当前状态，如果状态是终点或者掉陷阱则终止
        if terminated or truncated:
            break

    print(i)


play()

```
结果如下
![[动画.gif]]

## SARSA算法

```python
#获取一个格子的状态
def get_state(row, col):
    if row != 3:
        return 'ground'

    if row == 3 and col == 0:
        return 'ground'

    if row == 3 and col == 11:
        return 'terminal'

    return 'trap'

#在一个格子里做一个动作
def move(row, col, action):
    #如果当前已经在陷阱或者终点，则不能执行任何动作
    if get_state(row, col) in ['trap', 'terminal']:
        return row, col, 0

    #↑
    if action == 0:
        row -= 1

    #↓
    if action == 1:
        row += 1

    #←
    if action == 2:
        col -= 1

    #→
    if action == 3:
        col += 1

    #不允许走到地图外面去
    row = max(0, row)
    row = min(3, row)
    col = max(0, col)
    col = min(11, col)

    #是陷阱的话，奖励是-100，否则都是-1
    reward = -1
    if get_state(row, col) == 'trap':
        reward = -100

    return row, col, reward

import numpy as np

#初始化在每一个格子里采取每个动作的分数,初始化都是0,因为没有任何的知识
Q = np.zeros([4, 12, 4])

import random

#根据状态选择一个动作
def get_action(row, col):
    #有小概率选择随机动作
    if random.random() < 0.1:
        return random.choice(range(4))

    #否则选择分数最高的动作
    return Q[row, col].argmax()

#更新分数，每次更新取决于当前的格子，当前的动作，下个格子，和下个格子的动作
def get_update(row, col, action, reward, next_row, next_col, next_action):

    #计算target
    target = 0.9 * Q[next_row, next_col, next_action]
    target += reward

    #计算value
    value = Q[row, col, action]

    #根据时序差分算法,当前state,action的分数 = 下一个state,action的分数*gamma + reward
    #此处是求两者的差,越接近0越好
    update = target - value

    #这个0.1相当于lr
    update *= 0.1

    #更新当前状态和动作的分数
    return update

#训练
def train():
    for epoch in range(1500):
        #初始化当前位置
        row = random.choice(range(4))
        col = 0

        #初始化第一个动作
        action = get_action(row, col)

        #计算反馈的和，这个数字应该越来越小
        reward_sum = 0

        #循环直到到达终点或者掉进陷阱
        while get_state(row, col) not in ['terminal', 'trap']:

            #执行动作
            next_row, next_col, reward = move(row, col, action)
            reward_sum += reward

            #求新位置的动作
            next_action = get_action(next_row, next_col)

            #更新分数
            update = get_update(row, col, action, reward, next_row, next_col,
                                next_action)
            Q[row, col, action] += update

            #更新当前位置
            row = next_row
            col = next_col
            action = next_action

        if epoch % 150 == 0:
            print(epoch, reward_sum)


train()
#打印所有格子的动作倾向
for row in range(4):
    line = ''
    for col in range(12):
        action = Q[row, col].argmax()
        action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]
        line += action
    print(line)

```
这里面get_update是SARSA主要内容。

## Q-learning
这里和SARSA的区别就是在get_update上，不需要传入下一个动作

```python
#获取一个格子的状态
def get_state(row, col):
    if row != 3:
        return 'ground'

    if row == 3 and col == 0:
        return 'ground'

    if row == 3 and col == 11:
        return 'terminal'

    return 'trap'

#在一个格子里做一个动作
def move(row, col, action):
    #如果当前已经在陷阱或者终点，则不能执行任何动作
    if get_state(row, col) in ['trap', 'terminal']:
        return row, col, 0

    #↑
    if action == 0:
        row -= 1

    #↓
    if action == 1:
        row += 1

    #←
    if action == 2:
        col -= 1

    #→
    if action == 3:
        col += 1

    #不允许走到地图外面去
    row = max(0, row)
    row = min(3, row)
    col = max(0, col)
    col = min(11, col)

    #是陷阱的话，奖励是-100，否则都是-1
    reward = -1
    if get_state(row, col) == 'trap':
        reward = -100

    return row, col, reward


import numpy as np

#初始化在每一个格子里采取每个动作的分数,初始化都是0,因为没有任何的知识
Q = np.zeros([4, 12, 4])

Q.shape


import random


#根据状态选择一个动作
def get_action(row, col):
    #有小概率选择随机动作
    if random.random() < 0.1:
        return random.choice(range(4))

    #否则选择分数最高的动作
    return Q[row, col].argmax()

def get_update(row, col, action, reward, next_row, next_col):
    #target为下一个格子的最高分数，这里的计算和下一步的动作无关
    target = 0.9 * Q[next_row, next_col].max()
    #加上本步的分数
    target += reward

    #value为当前state和action的分数
    value = Q[row, col, action]

    #根据时序差分算法,当前state,action的分数 = 下一个state,action的分数*gamma + reward
    #此处是求两者的差,越接近0越好
    update = target - value

    #这个0.1相当于lr
    update *= 0.1

    return update


#训练
def train():
    for epoch in range(1500):
        #初始化当前位置
        row = random.choice(range(4))
        col = 0

        #初始化第一个动作
        action = get_action(row, col)

        #计算反馈的和，这个数字应该越来越小
        reward_sum = 0

        #循环直到到达终点或者掉进陷阱
        while get_state(row, col) not in ['terminal', 'trap']:

            #执行动作
            next_row, next_col, reward = move(row, col, action)
            reward_sum += reward

            #求新位置的动作
            next_action = get_action(next_row, next_col)

            #计算分数
            update = get_update(row, col, action, reward, next_row, next_col)

            #更新分数
            Q[row, col, action] += update

            #更新当前位置
            row = next_row
            col = next_col
            action = next_action

        if epoch % 100 == 0:
            print(epoch, reward_sum)


train()
#打印所有格子的动作倾向
for row in range(4):
    line = ''
    for col in range(12):
        action = Q[row, col].argmax()
        action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]
        line += action
    print(line)

```


## Dyna-Q
```python
#获取一个格子的状态
def get_state(row, col):
    if row != 3:
        return 'ground'

    if row == 3 and col == 0:
        return 'ground'

    if row == 3 and col == 11:
        return 'terminal'

    return 'trap'

#在一个格子里做一个动作
def move(row, col, action):
    #如果当前已经在陷阱或者终点，则不能执行任何动作
    if get_state(row, col) in ['trap', 'terminal']:
        return row, col, 0

    #↑
    if action == 0:
        row -= 1

    #↓
    if action == 1:
        row += 1

    #←
    if action == 2:
        col -= 1

    #→
    if action == 3:
        col += 1

    #不允许走到地图外面去
    row = max(0, row)
    row = min(3, row)
    col = max(0, col)
    col = min(11, col)

    #是陷阱的话，奖励是-100，否则都是-1
    reward = -1
    if get_state(row, col) == 'trap':
        reward = -100

    return row, col, reward


import numpy as np

#初始化在每一个格子里采取每个动作的分数,初始化都是0,因为没有任何的知识
Q = np.zeros([4, 12, 4])

#保存历史数据,键是(row,col,action),值是(next_row,next_col,reward)
history = dict()


import random


#根据状态选择一个动作
def get_action(row, col):
    #有小概率选择随机动作
    if random.random() < 0.1:
        return random.choice(range(4))

    #否则选择分数最高的动作
    return Q[row, col].argmax()

def get_update(row, col, action, reward, next_row, next_col):
    #target为下一个格子的最高分数，这里的计算和下一步的动作无关
    target = 0.9 * Q[next_row, next_col].max()
    #加上本步的分数
    target += reward

    #计算value
    value = Q[row, col, action]

    #根据时序差分算法,当前state,action的分数 = 下一个state,action的分数*gamma + reward
    #此处是求两者的差,越接近0越好
    update = target - value

    #这个0.1相当于lr
    update *= 0.1

    return update
import random


def q_planning():
    #Q planning循环,相当于是在反刍历史数据,随机取N个历史数据再进行离线学习
    for _ in range(20):
        #随机选择曾经遇到过的状态动作对
        row, col, action = random.choice(list(history.keys()))

        #再获取下一个状态和反馈
        next_row, next_col, reward = history[(row, col, action)]

        #计算分数
        update = get_update(row, col, action, reward, next_row, next_col)

        #更新分数
        Q[row, col, action] += update

#训练
def train():
    for epoch in range(300):
        #初始化当前位置
        row = random.choice(range(4))
        col = 0

        #初始化第一个动作
        action = get_action(row, col)

        #计算反馈的和，这个数字应该越来越小
        reward_sum = 0

        #循环直到到达终点或者掉进陷阱
        while get_state(row, col) not in ['terminal', 'trap']:

            #执行动作
            next_row, next_col, reward = move(row, col, action)
            reward_sum += reward

            #求新位置的动作
            next_action = get_action(next_row, next_col)

            #计算分数
            update = get_update(row, col, action, reward, next_row, next_col)

            #更新分数
            Q[row, col, action] += update

            #将数据添加到模型中
            history[(row, col, action)] = next_row, next_col, reward

            #反刍历史数据,进行离线学习
            q_planning()

            #更新当前位置
            row = next_row
            col = next_col
            action = next_action

        if epoch % 20 == 0:
            print(epoch, reward_sum)


train()
#打印所有格子的动作倾向
for row in range(4):
    line = ''
    for col in range(12):
        action = Q[row, col].argmax()
        action = {0: '↑', 1: '↓', 2: '←', 3: '→'}[action]
        line += action
    print(line)

```
规划部分则是在每一步的执行之后，Dyna-Q使用其模型来模拟可能的转换，并使用这些模拟的转换来进一步更新其Q-table。这个模型在每一步的执行过程中都会被更新，以便反映智能体从与环境交互中获取的最新信息。
这里的Dyna-Q模型是历史数据。



# DQN



## 解决平衡车
![[Pasted image 20230711195135.png]]
深度Q学习（Deep Q-Learning，DQN）的强化学习算法来解决 "CartPole-v1" 这个 Gym 环境下的控制问题。

```python
#定义环境
class MyWrapper(gym.Wrapper):

    def __init__(self):
        env = gym.make('CartPole-v1', render_mode='rgb_array')
        super().__init__(env)
        self.env = env
        self.step_n = 0

    def reset(self):
        state, _ = self.env.reset()
        self.step_n = 0
        return state

    def step(self, action):
        state, reward, terminated, truncated, info = self.env.step(action)
        done = terminated or truncated
        self.step_n += 1
        if self.step_n >= 200:
            done = True
        return state, reward, done, info
env = MyWrapper()
```
`MyWrapper` 类，封装了基本的 Gym 环境，并添加了一些额外的功能。例如，在每一步中记录游戏步骤数量，并设定了最大步骤限制，防止出现无限循环之类的问题

```python
import torch

#计算动作的模型,也是真正要用的模型
model = torch.nn.Sequential(
    torch.nn.Linear(4, 128),
    torch.nn.ReLU(),
    torch.nn.Linear(128, 2),
)
#经验网络,用于评估一个状态的分数
next_model = torch.nn.Sequential(
    torch.nn.Linear(4, 128),
    torch.nn.ReLU(),
    torch.nn.Linear(128, 2),
)

#把model的参数复制给next_model
next_model.load_state_dict(model.state_dict())
```
`model` 是用来计算Q值的主网络，即`value`，`next_model` 是用来计算Q目标的辅助网络，即`target`。这两个网络的结构相同，且 `next_model` 的参数定期从 `model` 中更新。这种做法可以防止目标值过快更新，从而增加训练的稳定性。

```python
import random

#得到一个动作
def get_action(state):
    if random.random() < 0.01:
        return random.choice([0, 1])

    #走神经网络,得到一个动作
    state = torch.FloatTensor(state).reshape(1, 4)

    return model(state).argmax().item()
```
model(state)是一个大小为 2 的向量，因为模型对两个可能的动作（`action=0` 和 `action=1`）进行了预，`.argmax()`: 这个函数返回的是具有最大值的索引，也就是模型认为最佳的动作，`.item()`: 这个函数将一个只包含一个元素的张量转化为 Python 标准的数据类型，这是为了能够在 Python 中直接使用模型的预测结果。

```python

#样本池
datas = []
#向样本池中添加N条数据,删除M条最古老的数据
def update_data():
    old_count = len(datas)

    #玩到新增了N个数据为止
    while len(datas) - old_count < 200:
        #初始化游戏
        state = env.reset()

        #玩到游戏结束为止
        over = False
        while not over:
            #根据当前状态得到一个动作
            action = get_action(state)

            #执行动作,得到反馈
            next_state, reward, over, _ = env.step(action)

            #记录数据样本
            datas.append((state, action, reward, next_state, over))

            #更新游戏状态,开始下一个动作
            state = next_state

    update_count = len(datas) - old_count
    drop_count = max(len(datas) - 10000, 0)

    #数据上限,超出时从最古老的开始删除
    while len(datas) > 10000:
        datas.pop(0)

    return update_count, drop_count

```
更新神经网络肯定是需要使用最新的模型进行游戏的样本。
```python
#获取一批数据样本
def get_sample():
    #从样本池中采样
    samples = random.sample(datas, 64)
    #[b, 4]
    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 4)
    #[b, 1]
    action = torch.LongTensor([i[1] for i in samples]).reshape(-1, 1)
    #[b, 1]
    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)
    #[b, 4]
    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 4)
    #[b, 1]
    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)
    return state, action, reward, next_state, over
```
采样数据样本进行神经网络的更新。

```python
def get_value(state, action):
    #使用状态计算出动作的logits
    #[b, 4] -> [b, 2]
    value = model(state)

    value = value.gather(dim=1, index=action)
    return value
```
这是获取这一批state的价值value。
dim=1（即第二个维度）上进行操作，index=action 表示我们需要提取的索引就是实际执行的动作。所以，这一步的结果 value 的形状是 [b, 1]，表示每个实例执行的动作的预测价值。

```python
def get_target(reward, next_state, over):
    #上面已经把模型认为的状态下执行动作的分数给评估出来了
    #下面使用next_state和reward计算真实的分数
    #针对一个状态,它到底应该多少分,可以使用以往模型积累的经验评估
    #这也是没办法的办法,因为显然没有精确解,这里使用延迟更新的next_model评估

    #使用next_state计算下一个状态的分数
    #[b, 4] -> [b, 2]
    with torch.no_grad():
        target = next_model(next_state)

    #取所有动作中分数最大的
    #[b, 2] -> [b, 1]
    target = target.max(dim=1)[0]
    target = target.reshape(-1, 1)

    #下一个状态的分数乘以一个系数,相当于权重
    target *= 0.98

    #如果next_state已经游戏结束,则next_state的分数是0
    #因为如果下一步已经游戏结束,显然不需要再继续玩下去,也就不需要考虑next_state了.
    #[b, 1] * [b, 1] -> [b, 1]
    target *= (1 - over)

    #加上reward就是最终的分数
    #[b, 1] + [b, 1] -> [b, 1]
    target += reward

    return target
```
`get_target()`函数根据Q-learning的更新公式计算了目标Q值。这个目标Q值将用于后续的训练过程，通过优化预测Q值与目标Q值之间的差距，不断地更新模型参数，使模型的预测结果越来越接近实际情况。
1. `target.max(dim=1)`: `max()`函数返回一个给定张量中的最大值。参数`dim=1`表示沿着第二个维度（即行）取最大值，也就是说，在每一行中找到最大值。在PyTorch中，维度从0开始计数，所以第二个维度实际上是张量的行。这个函数会返回两个结果，一个是最大值，另一个是最大值对应的索引。
    
2. `[0]`: `max()`函数返回两个结果，最大值和对应的索引，而这里我们只关心最大值，不关心最大值的位置，所以我们通过`[0]`来获取最大值。


```python
def train():
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)
    loss_fn = torch.nn.MSELoss()

    #训练N次
    for epoch in range(500):
        #更新N条数据
        update_count, drop_count = update_data()

        #每次更新过数据后,学习N次
        for i in range(200):
            #采样一批数据
            state, action, reward, next_state, over = get_sample()

            #计算一批样本的value和target
            value = get_value(state, action)
            target = get_target(reward, next_state, over)

            #更新参数
            loss = loss_fn(value, target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            #把model的参数复制给next_model
            if (i + 1) % 10 == 0:
                next_model.load_state_dict(model.state_dict())
train()
torch.save(model, 'model_dqn.pth')
```
然后可以使用上面这个训练好的模型来玩游戏

```python
import random  
  
import gym  
import torch  
  
def get_action(state):  
if random.random() < 0.01:  
return random.choice([0, 1])  
  
#走神经网络,得到一个动作  
state = torch.FloatTensor(state).reshape(1, 4)  
  
return model(state).argmax().item()  
  
  
model = torch.load('model_dqn.pth')  
  
env = gym.make('CartPole-v1', render_mode='human')  
#初始化游戏  
state = env.reset()  
state = state[0]  
print("state before reshaping:", state)  
#记录反馈值的和,这个值越大越好  
reward_sum = 0  
#玩到游戏结束为止  
over = False  
while not over:  
# 根据当前状态得到一个动作  
action = get_action(state)  
  
# 执行动作,得到反馈  
state, reward, over, *_ = env.step(action)  
reward_sum += reward  
print("reward_sum:", reward_sum)
```


## 解决倒立摆
倒立摆问题和平衡车问题的区别是，这里的state是3维向量，action是 `Box(-2.0, 2.0, (1,), float32)`，即-2到2 的连续值，平衡车是0或1

```python
import gym
#定义环境
class MyWrapper(gym.Wrapper):
    def __init__(self):
        env = gym.make('Pendulum-v1', render_mode='rgb_array')
        super().__init__(env)
        self.env = env
        self.step_n = 0

    def reset(self):
        state, _ = self.env.reset()
        self.step_n = 0
        return state

    def step(self, action):
        state, reward, terminated, truncated, info = self.env.step(action)
        done = terminated or truncated
        self.step_n += 1
        if self.step_n >= 200:
            done = True
        return state, reward, done, info


env = MyWrapper()

env.reset()
import torch
#计算动作的模型,也是真正要用的模型
model = torch.nn.Sequential(
    torch.nn.Linear(3, 128),
    torch.nn.ReLU(),
    torch.nn.Linear(128, 11),
)

#经验网络,用于评估一个状态的分数
next_model = torch.nn.Sequential(
    torch.nn.Linear(3, 128),
    torch.nn.ReLU(),
    torch.nn.Linear(128, 11),
)

#把model的参数复制给next_model
next_model.load_state_dict(model.state_dict())
```
这里是设置了value和target两个模型，target模型，`next_model` 的参数定期从 `model` 中更新。这种做法可以防止目标值过快更新，从而增加训练的稳定性。

```python
import random
def get_action(state):
    #走神经网络,得到一个动作
    state = torch.FloatTensor(state).reshape(1, 3)
    # 网络输出的是一个长度为11的向量，action得到其中最大的那个数值
    action = model(state).argmax().item()

    if random.random() < 0.01:
        action = random.choice(range(11))

    #离散动作连续化，action是0-1
    action_continuous = action
    action_continuous /= 10
    action_continuous *= 4
    action_continuous -= 2

    return action, action_continuous


#样本池
datas = []


#向样本池中添加N条数据,删除M条最古老的数据
def update_data():
    old_count = len(datas)

    #玩到新增了N个数据为止
    while len(datas) - old_count < 200:
        #初始化游戏
        state = env.reset()

        #玩到游戏结束为止
        over = False
        while not over:
            #根据当前状态得到一个动作
            action, action_continuous = get_action(state)

            #执行动作,得到反馈
            next_state, reward, over, _ = env.step([action_continuous])

            #记录数据样本
            datas.append((state, action, reward, next_state, over))

            #更新游戏状态,开始下一个动作
            state = next_state

    update_count = len(datas) - old_count
    drop_count = max(len(datas) - 5000, 0)

    #数据上限,超出时从最古老的开始删除
    while len(datas) > 5000:
        datas.pop(0)

    return update_count, drop_count


#获取一批数据样本
def get_sample():
    #从样本池中采样
    samples = random.sample(datas, 64)

    #[b, 3]
    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 3)
    #[b, 1]
    action = torch.LongTensor([i[1] for i in samples]).reshape(-1, 1)
    #[b, 1]
    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)
    #[b, 3]
    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 3)
    #[b, 1]
    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)

    return state, action, reward, next_state, over

def get_value(state, action):
    #使用状态计算出动作的logits
    #[b, 3] -> [b, 11]
    value = model(state)

    #根据实际使用的action取出每一个值
    #这个值就是模型评估的在该状态下,执行动作的分数
    #在执行动作前,显然并不知道会得到的反馈和next_state
    #所以这里不能也不需要考虑next_state和reward
    #[b, 11] -> [b, 1]
    value = value.gather(dim=1, index=action)

    return value
```
这是获取这一批state的价值value。
dim=1（即第二个维度）上进行操作，index=action 表示我们需要提取的索引就是实际执行的动作。所以，这一步的结果 value 的形状是 [b, 1]，表示每个实例执行的动作的预测价值

```python
def get_target(reward, next_state, over):
    #上面已经把模型认为的状态下执行动作的分数给评估出来了
    #下面使用next_state和reward计算真实的分数
    #针对一个状态,它到底应该多少分,可以使用以往模型积累的经验评估
    #这也是没办法的办法,因为显然没有精确解,这里使用延迟更新的next_model评估

    #使用next_state计算下一个状态的分数
    #[b, 3] -> [b, 11]
    with torch.no_grad():
        target = next_model(next_state)

    #取所有动作中分数最大的
    #[b, 11] -> [b, 1]
    target = target.max(dim=1)[0]
    target = target.reshape(-1, 1)

    #下一个状态的分数乘以一个系数,相当于权重
    target *= 0.98

    #如果next_state已经游戏结束,则next_state的分数是0
    #因为如果下一步已经游戏结束,显然不需要再继续玩下去,也就不需要考虑next_state了.
    #[b, 1] * [b, 1] -> [b, 1]
    target *= (1 - over)

    #加上reward就是最终的分数
    #[b, 1] + [b, 1] -> [b, 1]
    target += reward

    return target

```
`get_target()`函数根据Q-learning的更新公式计算了目标Q值。这个目标Q值将用于后续的训练过程，通过优化预测Q值与目标Q值之间的差距，不断地更新模型参数，使模型的预测结果越来越接近实际情况。
1. `target.max(dim=1)`:   `max()`函数返回一个给定张量中的最大值。参数`dim=1`表示沿着第二个维度（即行）取最大值，也就是说，在每一行中找到最大值。在PyTorch中，维度从0开始计数，所以第二个维度实际上是张量的行。这个函数会返回两个结果，一个是最大值，另一个是最大值对应的索引。
    
2. `[0]`:    `max()`函数返回两个结果，最大值和对应的索引，而这里我们只关心最大值，不关心最大值的位置，所以我们通过`[0]`来获取最大值。

```python
def train():
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
    loss_fn = torch.nn.MSELoss()

    #训练N次
    for epoch in range(200):
        #更新N条数据
        update_count, drop_count = update_data()

        #每次更新过数据后,学习N次
        for i in range(200):
            #采样一批数据
            state, action, reward, next_state, over = get_sample()

            #计算一批样本的value和target
            value = get_value(state, action)
            target = get_target(reward, next_state, over)

            #更新参数
            loss = loss_fn(value, target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            #把model的参数复制给next_model
            if (i + 1) % 50 == 0:
                next_model.load_state_dict(model.state_dict())
train()  
torch.save(model, 'model_daolibai.pth')
```
上面是进行训练，然后是测试：

```python
import random

import  gym
import  torch

model = torch.load('model_daolibai.pth')

env = gym.make('Pendulum-v1', render_mode='human')
def get_action(state):
    # 走神经网络,得到一个动作
    state = torch.FloatTensor(state).reshape(1, 3)
    # 网络输出的是一个长度为11的向量，action得到其中最大的那个数值
    action = model(state).argmax().item()

    if random.random() < 0.01:
        action = random.choice(range(11))

    # 离散动作连续化，action是0-1
    action_continuous = action
    action_continuous /= 10
    action_continuous *= 4
    action_continuous -= 2

    return  action_continuous


state = env.reset()
state = state[0]
reward_sum = 0

# print(state)
over = False
step=0
while not over:
    # 根据当前状态得到一个动作
    action = get_action(state)
    print("step:", step)
    step += 1
    # 执行动作,得到反馈
    state, reward, over, *_ = env.step([action])
    reward_sum += reward
print("reward_sum:", reward_sum)

```


## DDQN
Double DQN 是深度强化学习中的一个算法，它是对基础的 DQN（Deep Q-Network）算法的改进。Double DQN 的提出主要是为了解决原始的 DQN 算法中的一个问题，即过估计（overestimation）Q 值的问题。
在原始的 DQN 算法中，我们使用相同的网络既来选择动作又来估计这个动作的 Q 值，这样就会倾向于过估计某些动作的 Q 值。如果对某些动作的 Q 值过估计，那么我们的策略就可能会过于乐观，导致在实际环境中无法达到预期的效果。
在原来的 `get_target`中，是直接取分数最大的：

```python
with torch.no_grad():  
	target = next_model(next_state)
target = target.max(dim=1)[0]
```
现在变成如下，我们使用当前的 Q-network 来选择动作的索引，然后使用另一个目标 Q-network 来估计这个动作的 Q 值。

```python
def get_target(reward, next_state, over):
    with torch.no_grad():
        target = next_model(next_state)

    """以下是主要的Double DQN和DQN的区别"""
    # 取所有动作中分数最大的
    # [b, 11] -> [b]
    # target = target.max(dim=1)[0]

    # 使用model计算下一个状态的分数
    # [b, 3] -> [b, 11]
    with torch.no_grad():
        model_target = model(next_state)

    # 取分数最高的下标
    # [b, 11] -> [b, 1]
    model_target = model_target.max(dim=1)[1]
    model_target = model_target.reshape(-1, 1)

    # 以这个下标取next_value当中的值
    # [b, 11] -> [b]
    target = target.gather(dim=1, index=model_target)
    """以上是主要的Double DQN和DQN的区别"""

    #下一个状态的分数乘以一个系数,相当于权重
    target *= 0.98

    #如果next_state已经游戏结束,则next_state的分数是0
    #因为如果下一步已经游戏结束,显然不需要再继续玩下去,也就不需要考虑next_state了.
    #[b, 1] * [b, 1] -> [b, 1]
    target *= (1 - over)

    #加上reward就是最终的分数
    #[b, 1] + [b, 1] -> [b, 1]
    target += reward

    return target
```
`model_target.max(dim=1)[1]`在张量的第1个维度（即每一行）中找到最大值，`max()`函数返回两个结果：最大值和对应的索引，我们用`[1]`取的是索引。
`model_target.reshape(-1, 1)`：这行代码是将`model_target`张量改变形状，让其每行只有一个元素（即一个动作的索引）
`target = target.gather(dim=1, index=model_target)`：这行代码的作用是从`target`张量中取出与`model_target`中索引对应的元素。这里`dim=1`表示在第1个维度（即每一行）中按照索引取元素，`index=model_target`就是指定索引。取出来的元素就是我们需要的，具有最大Q值的动作对应的`target`值。


# 策略梯度
策略网络的目标函数是这样：![](images/Pasted%20image%2020230715200820.png)
他的梯度：![](images/Pasted%20image%2020230715200843.png)


## REINFORCE
REINFORCE使用蒙特卡洛采样时,可得如下导函数:
![](images/Pasted%20image%2020230715200914.png)

代码如下

```python
# 导入OpenAI的gym库，它提供了一系列环境供强化学习算法训练
import gym

# 创建一个新的环境包装器
class MyWrapper(gym.Wrapper):
    # 构造函数，初始化环境和步数计数器
    def __init__(self):
        # 创建CartPole-v1环境，这是一个常见的强化学习任务，其中的目标是防止一根杆倒下
        env = gym.make('CartPole-v1', render_mode='rgb_array')
        super().__init__(env)  # 初始化父类
        self.env = env  # 保存环境
        self.step_n = 0  # 初始化步数计数器

    # 重置环境，通常在每次新的尝试（或新的"游戏"）开始时调用
    def reset(self):
        state, _ = self.env.reset()  # 重置环境，并获取初始状态
        self.step_n = 0  # 重置步数计数器
        return state  # 返回初始状态

    # 这是一个步骤函数，它执行给定的动作并返回新的状态，奖励，以及是否结束的信息
    def step(self, action):
        # 执行给定的动作，并获取返回的结果
        state, reward, terminated, truncated, info = self.env.step(action)
        done = terminated or truncated  # 结束条件，如果游戏终止或被截断
        self.step_n += 1  # 步数计数器增加
        if self.step_n >= 200:  # 如果步数达到200，也认为游戏结束
            done = True
        return state, reward, done, info  # 返回新的状态，奖励，是否结束以及其他信息

# 创建环境实例
env = MyWrapper()

# 重置环境
env.reset()

```
上面是初始化环境，和前面一样

```python
# 导入PyTorch库，一个用于构建和训练神经网络的库
import torch

# 定义一个神经网络模型，该模型将环境状态作为输入，并输出每个动作的概率
model = torch.nn.Sequential(
    torch.nn.Linear(4, 128),  # 第一层线性层，将4个状态特征映射到128个隐藏单元
    torch.nn.ReLU(),  # 非线性ReLU激活函数
    torch.nn.Linear(128, 2),  # 第二层线性层，将128个隐藏单元映射到2个动作输出
    torch.nn.Softmax(dim=1),  # Softmax层，将线性输出转换为概率
)

```
定义了神经网络，输出是2，两个动作的概率


```python
# 导入random库，用于随机选择动作
import random

# 定义一个函数，该函数根据当前状态和模型输出选择一个动作
def get_action(state):
    state = torch.FloatTensor(state).reshape(1, 4)  # 将状态转换为PyTorch张量
    prob = model(state)  # 通过模型得到动作的概率分布
    action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]  # 根据概率分布选择一个动作
    return action  # 返回选定的动作
```
先通过神经网络输出两个动作的概率，然后通过概率选择动作。
`range(2)`就代表了[0, 1]
`prob[0].tolist()`: 这将模型输出的概率分布转换为列表形式。`prob[0]`是因为模型的输出是一个二维数组，我们需要的概率分布是在第一个维度中。
`choices`表示从输入的集合（这里是`range(2)`）中随机选择k个元素返回，选择的概率由`weights`参数指定。在这里，它从动作集合[0, 1]中根据概率`prob[0].tolist()`选择一个动作。
`[0]`是为了从返回的列表中取出那个选定的动作。
```python
# 定义一个函数，它玩一次游戏并返回游戏数据
def get_data():
    states = []  # 初始化状态列表
    rewards = []  # 初始化奖励列表
    actions = []  # 初始化动作列表
    state = env.reset()  # 重置环境并获取初始状态
    over = False  # 初始化结束标志
    while not over:  # 当游戏未结束时，持续循环
        action = get_action(state)  # 获取一个动作
        next_state, reward, over, _ = env.step(action)  # 执行动作并获取结果
        states.append(state)  # 记录状态
        rewards.append(reward)  # 记录奖励
        actions.append(action)  # 记录动作
        state = next_state  # 更新状态，用于下一个动作
    return states, rewards, actions  # 返回所有的状态，奖励和动作

```
这里是由当前模型产生一局游戏的states和rewards和actions用来更新

```python
# 定义训练函数，使用REINFORCE算法训练模型
def train():
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # 创建一个优化器

    # 在1000局游戏中训练模型
    for epoch in range(1000):
        states, rewards, actions = get_data()  # 玩一局游戏并获取数据

        optimizer.zero_grad()  # 清空优化器的梯度

        reward_sum = 0  # 初始化奖励和
        for i in reversed(range(len(states))):  # 从最后一步开始反向迭代
            reward_sum *= 0.98  # 衰减奖励和
            reward_sum += rewards[i]  # 加上当前步骤的奖励

            state = torch.FloatTensor(states[i]).reshape(1, 4)  # 将状态转换为PyTorch张量
            prob = model(state)  # 通过模型得到动作的概率分布
            prob = prob[0, actions[i]]  # 获取对应动作的概率

            loss = -prob.log() * reward_sum  # 根据REINFORCE算法计算损失

            loss.backward(retain_graph=True)  # 反向传播计算梯度

        optimizer.step()  # 使用优化器更新模型参数

# 调用训练函数，开始训练
train()

# 保存训练好的模型
torch.save(model, 'model_reinforce.pth')
```
- `prob`: 代表的是策略网络对一个动作的预测概率。
- `reward_sum`: 从当前状态开始，该策略可能获得的未来总奖励（也就是返回）。
- `.log()`: 计算了概率的对数。在许多情况下，对概率取对数可以让优化过程更稳定，同时也因为Policy Gradient中的公式就包含了对数项。
- `-prob.log() * reward_sum`: 这是根据**REINFORCE算法**计算损失的方式。负号是因为大部分优化算法如SGD都是做最小化优化，而在强化学习中我们需要最大化总的预期奖励，因此在目标函数前面加上负号转化为最小化问题。
用这个loss对θ进行梯度上升，使J(θ)更大![](images/Pasted%20image%2020230716111000.png)![](images/Pasted%20image%2020230716110939.png)

测试模型：

```python
import random

import gym
import torch

def get_action(state):
    state = torch.FloatTensor(state).reshape(1, 4)

    #[1, 4] -> [1, 2]
    prob = model(state)

    #根据概率选择一个动作
    action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]

    return action


model = torch.load('model_reinforce.pth')

env = gym.make('CartPole-v1', render_mode='human')
#初始化游戏
state = env.reset()
state = state[0]
print("state before reshaping:", state)
#记录反馈值的和,这个值越大越好
reward_sum = 0
#玩到游戏结束为止
over = False
while not over:
    # 根据当前状态得到一个动作
    action = get_action(state)

    # 执行动作,得到反馈
    state, reward, over, *_ = env.step(action)
    reward_sum += reward
print("reward_sum:", reward_sum)
```




# Actor-Critic
Actor-Critic方法结合了Value-based方法和Policy-based方法的优点。
- "Actor" 部分是指策略部分，它是用来选择动作的，也就是这段代码中的 `model`。它的工作是为每个状态产生一个动作，这个动作取决于策略函数的输出。在这个代码中，Actor的策略梯度被定义为负的对数概率乘以时间差分误差的平均值。
    
- "Critic" 部分是指值函数部分，它是用来评估当前策略的好坏的，也就是这段代码中的 `model_td`。Critic的工作是为每个状态赋值，也就是计算每个状态的价值函数。在这个代码中，Critic的目标函数是预测的状态价值和实际的状态价值（目标值）之间的均方误差。
AC算法通过同时学习Actor和Critic来优化策略，Critic根据Actor的行为提供反馈，以调整Actor的策略。这种方法试图结合策略优化的直接性和值函数优化的稳定性，从而达到更好的学习效果。


```python
import gym


#定义环境
class MyWrapper(gym.Wrapper):

    def __init__(self):
        env = gym.make('CartPole-v1', render_mode='rgb_array')
        super().__init__(env)
        self.env = env
        self.step_n = 0

    def reset(self):
        state, _ = self.env.reset()
        self.step_n = 0
        return state

    def step(self, action):
        state, reward, terminated, truncated, info = self.env.step(action)
        done = terminated or truncated
        self.step_n += 1
        if self.step_n >= 200:
            done = True
        return state, reward, done, info


env = MyWrapper()

env.reset()

```
然后定义两个网络，一个策略网络，一个价值网络

```python
import torch

#定义模型
model = torch.nn.Sequential(
    torch.nn.Linear(4, 128),
    torch.nn.ReLU(),
    torch.nn.Linear(128, 2),
    torch.nn.Softmax(dim=1),
)

model_td = sequential = torch.nn.Sequential(
    torch.nn.Linear(4, 128),
    torch.nn.ReLU(),
    torch.nn.Linear(128, 1),
)
```
选择动作：

```python
import random


#得到一个动作
def get_action(state):
    state = torch.FloatTensor(state).reshape(1, 4)
    #[1, 4] -> [1, 2]
    prob = model(state)

    #根据概率选择一个动作
    action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]

    return action


```
获得训练的数据：

```python

def get_data():
    states = []
    rewards = []
    actions = []
    next_states = []
    overs = []

    #初始化游戏
    state = env.reset()

    #玩到游戏结束为止
    over = False
    while not over:
        #根据当前状态得到一个动作
        action = get_action(state)

        #执行动作,得到反馈
        next_state, reward, over, _ = env.step(action)

        #记录数据样本
        states.append(state)
        rewards.append(reward)
        actions.append(action)
        next_states.append(next_state)
        overs.append(over)

        #更新游戏状态,开始下一个动作
        state = next_state

    #[b, 4]
    states = torch.FloatTensor(states).reshape(-1, 4)
    #[b, 1]
    rewards = torch.FloatTensor(rewards).reshape(-1, 1)
    #[b, 1]
    actions = torch.LongTensor(actions).reshape(-1, 1)
    #[b, 4]
    next_states = torch.FloatTensor(next_states).reshape(-1, 4)
    #[b, 1]
    overs = torch.LongTensor(overs).reshape(-1, 1)

    return states, rewards, actions, next_states, overs


```
训练过程：
AC算法通过同时学习Actor和Critic来优化策略，Critic根据Actor的行为提供反馈，以调整Actor的策略

```python
def train():
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    optimizer_td = torch.optim.Adam(model_td.parameters(), lr=1e-2)
    loss_fn = torch.nn.MSELoss()

    #玩N局游戏,每局游戏训练一次
    for i in range(1000):
        #玩一局游戏,得到数据
        #states -> [b, 4]
        #rewards -> [b, 1]
        #actions -> [b, 1]
        #next_states -> [b, 4]
        #overs -> [b, 1]
        states, rewards, actions, next_states, overs = get_data()

        #计算values和targets
        #[b, 4] -> [b ,1]
        #model_td用于评估每个状态的价值，对于下一个状态的评估结果，乘以一个折扣因子（在这里是0.98）
        values = model_td(states)

        #[b, 4] -> [b ,1]
        targets = model_td(next_states) * 0.98
        #[b ,1] * [b ,1] -> [b ,1]
        targets *= (1 - overs)
        #[b ,1] + [b ,1] -> [b ,1]
        # 加上实际观察到的r，用于时序差分值的计算
        targets += rewards

        #时序差分误差
        #[b ,1] - [b ,1] -> [b ,1]
        # 计算时序差分误差，它就是当前状态的目标值与估计值的差值。
        delta = (targets - values).detach()

        #重新计算对应动作的概率
        #[b, 4] -> [b ,2]
        probs = model(states)
        #[b ,2] -> [b ,1]
        probs = probs.gather(dim=1, index=actions)

        #根据策略梯度算法的导函数实现
        #只是把公式中的reward_sum替换为了时序差分的误差
        #[b ,1] * [b ,1] -> [b ,1] -> scala
        # 依据策略梯度算法的原理，将概率的对数乘以时序差分误差，然后求平均，得到策略网络的损失函数。
        # ，分别对这两个网络进行优化。先将梯度清零，然后进行反向传播，计算出梯度，最后更新网络参数。
        loss = (-probs.log() * delta).mean()

        #时序差分的loss就是简单的value和target求mse loss即可
        loss_td = loss_fn(values, targets.detach())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        optimizer_td.zero_grad()
        loss_td.backward()
        optimizer_td.step()
train()  
torch.save(model,"ac_model.pth")
```
实际测试：

```python
import random  
  
import gym  
import torch  
  
  
#得到一个动作  
def get_action(state):  
state = torch.FloatTensor(state).reshape(1, 4)  
#[1, 4] -> [1, 2]  
prob = model(state)  
  
#根据概率选择一个动作  
action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]  
  
return action  
  
  
model = torch.load('ac_model.pth')  
  
env = gym.make('CartPole-v1', render_mode='human')  
#初始化游戏  
state = env.reset()  
state = state[0]  
print("state before reshaping:", state)  
#记录反馈值的和,这个值越大越好  
reward_sum = 0  
#玩到游戏结束为止  
over = False  
while not over:  
# 根据当前状态得到一个动作  
action = get_action(state)  
  
# 执行动作,得到反馈  
state, reward, over, *_ = env.step(action)  
reward_sum += reward  
print("reward_sum:", reward_sum)

```


# PPO算法
在PPO中，我们使用了一个带有`基线baseline`的`Actor-Critic`架构。这个基线是由一个神经网络（通常与策略共享一些层）预测的，用来估计每个状态的值函数。这个基线被用来计算优势函数，优势函数再被用来更新策略。这样，PPO算法就可以在减少方差的同时，进行有效的策略更新。
PPO（Proximal Policy Optimization）算法可以被视为是TRPO（Trust Region Policy Optimization）算法的一种简化和实用化版本，但是效果可能比TRPO还要好，所以有了PPO，TRPO就很少用了。
Proximal Policy Optimization (PPO)算法是一种策略优化方法，它通过限制策略更新的步长来避免在策略空间中进行过大的跳跃，从而提高学习的稳定性。
所以PPO算法的很多原理都从TRPO中来，在下面代码中没有细讲，只能说大概是这样写的。

```python
import gym


#定义环境
class MyWrapper(gym.Wrapper):

    def __init__(self):
        env = gym.make('CartPole-v1', render_mode='rgb_array')
        super().__init__(env)
        self.env = env
        self.step_n = 0

    def reset(self):
        state, _ = self.env.reset()
        self.step_n = 0
        return state

    def step(self, action):
        state, reward, terminated, truncated, info = self.env.step(action)
        done = terminated or truncated
        self.step_n += 1
        if self.step_n >= 200:
            done = True
        return state, reward, done, info


env = MyWrapper()

env.reset()
```
这里是定义环境

```python
import torch

#定义模型
model = torch.nn.Sequential(
    torch.nn.Linear(4, 128),
    torch.nn.ReLU(),
    torch.nn.Linear(128, 2),
    torch.nn.Softmax(dim=1),
)

model_td = torch.nn.Sequential(
    torch.nn.Linear(4, 128),
    torch.nn.ReLU(),
    torch.nn.Linear(128, 1),
)

```
定义模型，和AC算法一样。

```python
import random


#得到一个动作
def get_action(state):
    state = torch.FloatTensor(state).reshape(1, 4)
    #[1, 4] -> [1, 2]
    prob = model(state)

    #根据概率选择一个动作
    action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]

    return action

def get_data():
    states = []
    rewards = []
    actions = []
    next_states = []
    overs = []

    #初始化游戏
    state = env.reset()

    #玩到游戏结束为止
    over = False
    while not over:
        #根据当前状态得到一个动作
        action = get_action(state)

        #执行动作,得到反馈
        next_state, reward, over, _ = env.step(action)

        #记录数据样本
        states.append(state)
        rewards.append(reward)
        actions.append(action)
        next_states.append(next_state)
        overs.append(over)

        #更新游戏状态,开始下一个动作
        state = next_state

    #[b, 4]
    states = torch.FloatTensor(states).reshape(-1, 4)
    #[b, 1]
    rewards = torch.FloatTensor(rewards).reshape(-1, 1)
    #[b, 1]
    actions = torch.LongTensor(actions).reshape(-1, 1)
    #[b, 4]
    next_states = torch.FloatTensor(next_states).reshape(-1, 4)
    #[b, 1]
    overs = torch.LongTensor(overs).reshape(-1, 1)

    return states, rewards, actions, next_states, overs

```
选择动作和玩一局游戏，和AC算法一样。

```python
#优势函数
def get_advantages(deltas):
    advantages = []
    #反向遍历deltas
    s = 0.0
    for delta in deltas[::-1]:
        s = 0.98 * 0.95 * s + delta
        advantages.append(s)
    #逆序
    advantages.reverse()
    return advantages
```
获取优势函数

```python
def train():
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    optimizer_td = torch.optim.Adam(model_td.parameters(), lr=1e-2)
    loss_fn = torch.nn.MSELoss()

    #玩N局游戏,每局游戏训练M次
    for epoch in range(500):
        #玩一局游戏,得到数据
        #states -> [b, 4]
        #rewards -> [b, 1]
        #actions -> [b, 1]
        #next_states -> [b, 4]
        #overs -> [b, 1]
        states, rewards, actions, next_states, overs = get_data()

        #计算values和targets
        #[b, 4] -> [b, 1]
        values = model_td(states)

        #[b, 4] -> [b, 1]
        targets = model_td(next_states).detach()
        targets = targets * 0.98
        targets *= (1 - overs)
        targets += rewards

        #计算优势,这里的advantages有点像是策略梯度里的reward_sum
        #只是这里计算的不是reward,而是target和value的差
        #[b, 1]
        # 这行代码首先计算了目标值（targets）和值函数（values）的差值，然后使用squeeze(dim=1)函数去掉维度为1的维度，最后将结果转换为Python列表。
        # 这个差值（delta）反映了实际的回报和模型预测的值函数之间的差距。
        deltas = (targets - values).squeeze(dim=1).tolist()
        # 输出是优势函数的值
        advantages = get_advantages(deltas)
        advantages = torch.FloatTensor(advantages).reshape(-1, 1)
        #取出每一步动作的概率
        #[b, 2] -> [b, 2] -> [b, 1]
        old_probs = model(states)
        # gather(dim=1, index=actions)的作用是沿着dim=1（列方向）按照actions的索引来收集元素
        old_probs = old_probs.gather(dim=1, index=actions)
        old_probs = old_probs.detach()
        #每批数据反复训练10次
        for _ in range(10):
            #重新计算每一步动作的概率
            #[b, 4] -> [b, 2]
            new_probs = model(states)
            #[b, 2] -> [b, 1]
            new_probs = new_probs.gather(dim=1, index=actions)
            new_probs = new_probs
            #求出概率的变化
            #[b, 1] - [b, 1] -> [b, 1]
            # 这行代码计算了新旧动作概率的比率。这个比率反映了模型的策略变化的程度
            ratios = new_probs / old_probs
            #计算截断的和不截断的两份loss,取其中小的
            #[b, 1] * [b, 1] -> [b, 1]
            # 第一种损失直接使用比率和优势函数的乘积
            surr1 = ratios * advantages
            #[b, 1] * [b, 1] -> [b, 1]
            # 第二种损失将比率限制在一个范围内（0.8到1.2）然后再乘以优势函数
            surr2 = torch.clamp(ratios, 0.8, 1.2) * advantages
            # 这行代码选择了两种损失中的较小值作为最终的损失。这是PPO算法的一个关键步骤，它通过限制策略更新的步长来避免在策略空间中进行过大的跳跃。
            loss = -torch.min(surr1, surr2)
            loss = loss.mean()
            #重新计算value,并计算时序差分loss
            values = model_td(states)
            loss_td = loss_fn(values, targets)
            #更新参数
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            optimizer_td.zero_grad()
            loss_td.backward()
            optimizer_td.step()

train()
torch.save(model,'model_ppo.pth')
```



# DDPG算法
DDPG是连续空间的DQN算法，也可以进行离线学习
![](images/Pasted%20image%2020230715000257.png)


```python
import gym

#定义环境
class MyWrapper(gym.Wrapper):

    def __init__(self):
        #连续动作空间
        env = gym.make('Pendulum-v1', render_mode='rgb_array')
        super().__init__(env)
        self.env = env
        self.step_n = 0

    def reset(self):
        state, _ = self.env.reset()
        self.step_n = 0
        return state

    def step(self, action):
        state, reward, terminated, truncated, info = self.env.step(action)
        done = terminated or truncated
        self.step_n += 1
        if self.step_n >= 200:
            done = True
        return state, reward, done, info


env = MyWrapper()

env.reset()


```
DDPG是适用于连续空间的，所以这里使用钟摆游戏。

```python
import torch

#action网络模型
class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.sequential = torch.nn.Sequential(
            torch.nn.Linear(3, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 1),
            torch.nn.Tanh(),
        )

    def forward(self, state):
        return self.sequential(state) * 2.0


model_action = Model()
# 延迟更新的模型
model_action_next = Model()

model_action_next.load_state_dict(model_action.state_dict())

# value网络模型
model_value = torch.nn.Sequential(
    torch.nn.Linear(4, 64),
    torch.nn.ReLU(),
    torch.nn.Linear(64, 64),
    torch.nn.ReLU(),
    torch.nn.Linear(64, 1),
)
model_value_next = torch.nn.Sequential(
    torch.nn.Linear(4, 64),
    torch.nn.ReLU(),
    torch.nn.Linear(64, 64),
    torch.nn.ReLU(),
    torch.nn.Linear(64, 1),
)

model_value_next.load_state_dict(model_value.state_dict())

```
这里创建了四个模型，看上面的图，策略网络和价值网络都需要两个

```python
import random


def get_action(state):
    state = torch.FloatTensor(state).reshape(1, 3)
    action = model_action(state).item()
    #给动作添加噪声,增加探索
    action += random.normalvariate(mu=0, sigma=0.01)
    return action

#样本池
datas = []


#向样本池中添加N条数据,删除M条最古老的数据
def update_data():
    #初始化游戏
    state = env.reset()

    #玩到游戏结束为止
    over = False
    while not over:
        #根据当前状态得到一个动作
        action = get_action(state)

        #执行动作,得到反馈
        next_state, reward, over, _ = env.step([action])

        #记录数据样本
        datas.append((state, action, reward, next_state, over))

        #更新游戏状态,开始下一个动作
        state = next_state

    #数据上限,超出时从最古老的开始删除
    while len(datas) > 10000:
        datas.pop(0)


#获取一批数据样本
def get_sample():
    #从样本池中采样
    samples = random.sample(datas, 64)

    #[b, 3]
    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 3)
    #[b, 1]
    action = torch.FloatTensor([i[1] for i in samples]).reshape(-1, 1)
    #[b, 1]
    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)
    #[b, 3]
    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 3)
    #[b, 1]
    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)
    return state, action, reward, next_state, over
```
这里也都是通用的比如选择动作，更新数据池等。

```python

# 定义获取状态-动作值函数
def get_value(state, action):
    # 合并状态和动作的张量，这里使用的是PyTorch的cat函数
    input = torch.cat([state, action], dim=1)
    # 将合并后的张量输入到值函数网络（value network），计算出相应的值函数
    return model_value(input)

```
这里是相当于这一步：![](images/Pasted%20image%2020230717152526.png)

```python
# 定义获取目标值函数
def get_target(next_state, reward, over):
    # 使用目标动作网络（target action network）计算下一个状态的动作
    action = model_action_next(next_state)
    # 将下一个状态和动作合并
    input = torch.cat([next_state, action], dim=1)
    # 使用目标值函数网络（target value network）计算目标值函数
    target = model_value_next(input) * 0.98
    # 根据是否终止调整目标值函数
    target *= (1 - over)
    # 在目标值函数上加上立即奖励
    target += reward
    return target
```
这里应该是计算更新价值网络的TD目标值

```python
# 定义获取动作损失函数
def get_loss_action(state):
    # 使用动作网络（action network）计算动作
    action = model_action(state)
    # 合并状态和动作
    input = torch.cat([state, action], dim=1)
    # 使用值函数网络计算该状态-动作对的值函数，并取负值作为损失，因为我们希望最大化值函数
    # 如果我们的目标是最大化一个函数f(x)，那么我们可以转化为最小化-f(x)。因为，当-f(x)取得最小值时，f(x)就取得了最大值
    loss = -model_value(input).mean()
    return loss

```
计算这个损失是为了让策略网络生成的动作得到价值网络生成的最大的的价值

```python
# 定义网络参数软更新函数
def soft_update(model, model_next):
    # 对于模型中的每一组参数和目标模型中的相应参数
    for param, param_next in zip(model.parameters(), model_next.parameters()):
        # 使用一个小的系数进行更新
        value = param_next.data * 0.995 + param.data * 0.005
        param_next.data.copy_(value)
```
这是更新两种网络的next模型参数。
**原因：** 如果我们仅仅直接用主网络的新权重来更新目标网络的权重，可能会引发训练的不稳定。因为目标网络的快速改变可能会导致我们追踪的目标Q值不断变化，使得学习过程变得困难和不稳定。
为了解决这个问题，我们可以使用“软更新”策略。在这种策略中，我们不是直接用主网络的权重替换目标网络的权重，而是让目标网络的权重缓慢地向主网络的权重靠近。这通过对主网络的权重进行小比例的更新并保留目标网络大部分权重来实现

```python
# 定义训练函数
def train():
    # 将模型设置为训练模式
    model_action.train()
    model_value.train()
    # 定义优化器
    optimizer_action = torch.optim.Adam(model_action.parameters(), lr=5e-4)
    optimizer_value = torch.optim.Adam(model_value.parameters(), lr=5e-3)
    # 定义损失函数
    loss_fn = torch.nn.MSELoss()

    # 循环进行多轮训练
    for epoch in range(200):
        # 更新数据集
        update_data()

        # 对每批新数据进行学习
        for i in range(200):
            # 从经验回放池中抽取一批数据
            state, action, reward, next_state, over = get_sample()

            # 计算值函数和目标值函数
            value = get_value(state, action)
            target = get_target(next_state, reward, over)

            # 计算值函数的损失并进行反向传播和优化
            loss_value = loss_fn(value, target)
            optimizer_value.zero_grad()
            loss_value.backward()
            optimizer_value.step()

            # 计算动作损失并进行反向传播和优化
            loss_action = get_loss_action(state)
            optimizer_action.zero_grad()
            loss_action.backward()
            optimizer_action.step()

            # 对动作网络和值函数网络进行软更新
            soft_update(model_action, model_action_next)
            soft_update(model_value, model_value_next)


train()
torch.save(model_action,'model_ddpg.pth')
```
这是具体训练过程。训练完成之后，只需要model_action即可测试，就像get_action()方法中那样。
测试的代码：
```python
import random

import gym
import torch
class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.sequential = torch.nn.Sequential(
            torch.nn.Linear(3, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 1),
            torch.nn.Tanh(),
        )

    def forward(self, state):
        return self.sequential(state) * 2.0

model_action = torch.load('model_ddpg.pth')

def get_action(state):
    state = torch.FloatTensor(state).reshape(1, 3)
    action = model_action(state).item()
    #给动作添加噪声,增加探索
    action += random.normalvariate(mu=0, sigma=0.01)
    return action

env = gym.make('Pendulum-v1', render_mode='human')
#初始化游戏
state = env.reset()
state = state[0]
#记录反馈值的和,这个值越大越好
reward_sum = 0
#玩到游戏结束为止
over = False
while not over:
    # 根据当前状态得到一个动作
    action = get_action(state)

    # 执行动作,得到反馈
    state, reward, over, truncated, info = env.step([action])
    reward_sum += reward
print("reward_sum:", reward_sum)
```
经过测试，很稳定，基本不会掉。





# SAC算法
Soft Actor-Critic（SAC）是一种强化学习的算法，具有一种与模型无关的（model-free）、在线更新的、策略梯度方法。SAC以最大化熵为目标，熵是一个表示随机变量不确定性的度量，这使得SAC能够在探索和利用之间找到更好的平衡。其核心思想是：在优化期望奖励的同时，也尽可能地让策略保持高熵。
就是最大化Q函数的同时，最大化Q函数的熵
![](images/Pasted%20image%2020230717182053.png)
`alpha`是熵加权系数

基础代码：

```python
import torch


class ModelAction(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc_state = torch.nn.Sequential(
            torch.nn.Linear(3, 128),
            torch.nn.ReLU(),
        )
        self.fc_mu = torch.nn.Linear(128, 1)
        self.fc_std = torch.nn.Sequential(
            torch.nn.Linear(128, 1),
            torch.nn.Softplus(),
        )

    def forward(self, state):
        #[b, 3] -> [b, 128]
        state = self.fc_state(state)

        #[b, 128] -> [b, 1]
        mu = self.fc_mu(state)

        #[b, 128] -> [b, 1]
        std = self.fc_std(state)

        #根据mu和std定义b个正态分布
        dist = torch.distributions.Normal(mu, std)

        #采样b个样本
        #这里用的是rsample,表示重采样,其实就是先从一个标准正态分布中采样,然后乘以标准差,加上均值
        sample = dist.rsample()

        #样本压缩到-1,1之间,求动作
        action = torch.tanh(sample)

        #求概率对数
        log_prob = dist.log_prob(sample)

        #这个值描述动作的熵
        entropy = log_prob - (1 - action.tanh()**2 + 1e-7).log()
        entropy = -entropy

        return action * 2, entropy


model_action = ModelAction()
```
熵正则的求法就在这里策略网络的`forward()`函数中

```python
class ModelValue(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.sequential = torch.nn.Sequential(
            torch.nn.Linear(4, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 1),
        )

    def forward(self, state, action):
        #[b, 3+1] -> [b, 4]
        state = torch.cat([state, action], dim=1)

        #[b, 4] -> [b, 1]
        return self.sequential(state)


model_value1 = ModelValue()
model_value2 = ModelValue()

model_value_next1 = ModelValue()
model_value_next2 = ModelValue()

model_value_next1.load_state_dict(model_value1.state_dict())
model_value_next2.load_state_dict(model_value2.state_dict())
```
创建价值网络模型。

```python
import random
import numpy as np

def get_action(state):
    state = torch.FloatTensor(state).reshape(1, 3)
    action, _ = model_action(state)
    return action.item()
```
获取动作

```python
#样本池
datas = []


#向样本池中添加N条数据,删除M条最古老的数据
def update_data():
    #初始化游戏
    state = env.reset()

    #玩到游戏结束为止
    over = False
    while not over:
        #根据当前状态得到一个动作
        action = get_action(state)

        #执行动作,得到反馈
        next_state, reward, over, _ = env.step([action])

        #记录数据样本
        datas.append((state, action, reward, next_state, over))

        #更新游戏状态,开始下一个动作
        state = next_state

    #数据上限,超出时从最古老的开始删除
    while len(datas) > 100000:
        datas.pop(0)
```
上面是更新数据池

```python
#获取一批数据样本
def get_sample():
    #从样本池中采样
    samples = random.sample(datas, 64)

    #[b, 3]
    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 3)
    #[b, 1]
    action = torch.FloatTensor([i[1] for i in samples]).reshape(-1, 1)
    #[b, 1]
    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)
    #[b, 3]
    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 3)
    #[b, 1]
    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)

    return state, action, reward, next_state, over

```
上面 是随机抽样样本

```python
def soft_update(model, model_next):
    for param, param_next in zip(model.parameters(), model_next.parameters()):
        #以一个小的比例更新
        value = param_next.data * 0.995 + param.data * 0.005
        param_next.data.copy_(value)

```
软更新，和前面一样。

```python
import math

#这也是一个可学习的参数
alpha = torch.tensor(math.log(0.01))
alpha.requires_grad = True

```

```python
def get_target(reward, next_state, over):
    #首先使用model_action计算动作和动作的熵
    #[b, 4] -> [b, 1],[b, 1]
    action, entropy = model_action(next_state)

    #评估next_state的价值
    #[b, 4],[b, 1] -> [b, 1]
    target1 = model_value_next1(next_state, action)
    target2 = model_value_next2(next_state, action)

    #取价值小的,这是出于稳定性考虑
    #[b, 1]
    target = torch.min(target1, target2)

    #exp和log互为反操作,这里是把alpha还原了
    #这里的操作是在target上加上了动作的熵,alpha作为权重系数
    #[b, 1] - [b, 1] -> [b, 1]
    target += alpha.exp() * entropy

    #[b, 1]
    target *= 0.99
    target *= (1 - over)
    target += reward

    return target
def get_loss_action(state):
    #计算action和熵
    #[b, 3] -> [b, 1],[b, 1]
    action, entropy = model_action(state)

    #使用两个value网络评估action的价值
    #[b, 3],[b, 1] -> [b, 1]
    value1 = model_value1(state, action)
    value2 = model_value2(state, action)

    #取价值小的,出于稳定性考虑
    #[b, 1]
    value = torch.min(value1, value2)

    #alpha还原后乘以熵,这个值期望的是越大越好,但是这里是计算loss,所以符号取反
    #[1] - [b, 1] -> [b, 1]
    loss_action = -alpha.exp() * entropy

    #减去value,所以value越大越好,这样loss就会越小
    loss_action -= value

    return loss_action.mean(), entropy


```

```python
def train():
    optimizer_action = torch.optim.Adam(model_action.parameters(), lr=3e-4)
    optimizer_value1 = torch.optim.Adam(model_value1.parameters(), lr=3e-3)
    optimizer_value2 = torch.optim.Adam(model_value2.parameters(), lr=3e-3)

    #alpha也是要更新的参数,所以这里要定义优化器
    optimizer_alpha = torch.optim.Adam([alpha], lr=3e-4)

    loss_fn = torch.nn.MSELoss()

    #训练N次
    for epoch in range(100):
        #更新N条数据
        update_data()

        #每次更新过数据后,学习N次
        for i in range(200):
            #采样一批数据
            state, action, reward, next_state, over = get_sample()

            #对reward偏移,为了便于训练
            reward = (reward + 8) / 8

            #计算target,这个target里已经考虑了动作的熵
            #[b, 1]
            target = get_target(reward, next_state, over)
            target = target.detach()

            #计算两个value
            value1 = model_value1(state, action)
            value2 = model_value2(state, action)

            #计算两个loss,两个value的目标都是要贴近target
            loss_value1 = loss_fn(value1, target)
            loss_value2 = loss_fn(value2, target)

            #更新参数
            optimizer_value1.zero_grad()
            loss_value1.backward()
            optimizer_value1.step()

            optimizer_value2.zero_grad()
            loss_value2.backward()
            optimizer_value2.step()

            #使用model_value计算model_action的loss
            loss_action, entropy = get_loss_action(state)
            optimizer_action.zero_grad()
            loss_action.backward()
            optimizer_action.step()

            #熵乘以alpha就是alpha的loss
            #[b, 1] -> [1]
            loss_alpha = (entropy + 1).detach() * alpha.exp()
            loss_alpha = loss_alpha.mean()

            #更新alpha值
            optimizer_alpha.zero_grad()
            loss_alpha.backward()
            optimizer_alpha.step()

            #增量更新next模型
            soft_update(model_value1, model_value_next1)
            soft_update(model_value2, model_value_next2)
train()
torch.save(model_action, 'model_sac_daolibai.pth')
```
测试代码如下：

```python

import  gym
import  torch
class ModelAction(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc_state = torch.nn.Sequential(
            torch.nn.Linear(3, 128),
            torch.nn.ReLU(),
        )
        self.fc_mu = torch.nn.Linear(128, 1)
        self.fc_std = torch.nn.Sequential(
            torch.nn.Linear(128, 1),
            torch.nn.Softplus(),
        )

    def forward(self, state):
        #[b, 3] -> [b, 128]
        state = self.fc_state(state)

        #[b, 128] -> [b, 1]
        mu = self.fc_mu(state)

        #[b, 128] -> [b, 1]
        std = self.fc_std(state)

        #根据mu和std定义b个正态分布
        dist = torch.distributions.Normal(mu, std)

        #采样b个样本
        #这里用的是rsample,表示重采样,其实就是先从一个标准正态分布中采样,然后乘以标准差,加上均值
        sample = dist.rsample()

        #样本压缩到-1,1之间,求动作
        action = torch.tanh(sample)

        #求概率对数
        log_prob = dist.log_prob(sample)

        #这个值描述动作的熵
        entropy = log_prob - (1 - action.tanh()**2 + 1e-7).log()
        entropy = -entropy

        return action * 2, entropy

model_action = torch.load('model_sac_daolibai.pth')

env = gym.make('Pendulum-v1', render_mode='human')
def get_action(state):
    state = torch.FloatTensor(state).reshape(1, 3)
    action, _ = model_action(state)
    return action.item()

state = env.reset()
state = state[0]
reward_sum = 0

# print(state)
over = False
step=0
while not over:
    # 根据当前状态得到一个动作
    action = get_action(state)
    print("step:", step)
    step += 1
    state, reward, over, *_ = env.step([action])
    reward_sum += reward
print("reward_sum:", reward_sum)
```

# 模仿学习
训练一个学生模型，使其能够模仿教师模型的行为。这是通过训练一个鉴别器来区分教师模型和学生模型的行为，然后使用鉴别器的预测结果来更新学生模型的参数实现的。

```python
import gym


#定义环境
class MyWrapper(gym.Wrapper):

    def __init__(self):
        env = gym.make('CartPole-v1', render_mode='rgb_array')
        super().__init__(env)
        self.env = env
        self.step_n = 0

    def reset(self):
        state, _ = self.env.reset()
        self.step_n = 0
        return state

    def step(self, action):
        state, reward, terminated, truncated, info = self.env.step(action)
        done = terminated or truncated
        self.step_n += 1
        if self.step_n >= 200:
            done = True
        return state, reward, done, info


env = MyWrapper()

env.reset()

import torch
import random
```
然后创建学生和教师的类，基本方法都相同，所以使用一个类：

```python
class PPO:
    def __init__(self):
        #定义模型
        self.model = torch.nn.Sequential(
            torch.nn.Linear(4, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 2),
            torch.nn.Softmax(dim=1),
        )

        self.model_td = torch.nn.Sequential(
            torch.nn.Linear(4, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 1),
        )

        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)
        self.optimizer_td = torch.optim.Adam(self.model_td.parameters(),
                                             lr=1e-2)
        self.loss_fn = torch.nn.MSELoss()

    #得到一个动作
    def get_action(self, state):
        state = torch.FloatTensor(state).reshape(1, 4)
        #[1, 4] -> [1, 2]
        prob = self.model(state)

        #根据概率选择一个动作
        action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]

        return action

    def _get_advantages(self, deltas):
        advantages = []

        #反向遍历deltas
        s = 0.0
        for delta in deltas[::-1]:
            s = 0.98 * 0.95 * s + delta
            advantages.append(s)

        #逆序
        advantages.reverse()
        return advantages

    def train(self, states, rewards, actions, next_states, overs):
        #states -> [b, 4]
        #rewards -> [b, 1]
        #actions -> [b, 1]
        #next_states -> [b, 4]
        #overs -> [b, 1]

        #计算values和targets
        #[b, 4] -> [b, 1]
        values = self.model_td(states)

        #[b, 4] -> [b, 1]
        targets = self.model_td(next_states) * 0.98
        targets *= (1 - overs)
        targets += rewards

        #[b, 1]
        deltas = (targets - values).squeeze(dim=1).tolist()
        advantages = self._get_advantages(deltas)
        advantages = torch.FloatTensor(advantages).reshape(-1, 1)

        #取出每一步的动作概率
        #[b, 2] -> [b, 2] -> [b, 1]
        old_probs = self.model(states)
        old_probs = old_probs.gather(1, actions)
        old_probs = old_probs.detach()

        #每批数据反复训练10次
        for _ in range(10):
            #[b, 4] -> [b, 2]
            new_probs = self.model(states)

            #[b, 2] -> [b, 1]
            new_probs = new_probs.gather(1, actions)
            new_probs = new_probs

            #[b, 1] - [b, 1] -> [b, 1]
            ratios = new_probs / old_probs

            #[b, 1] * [b, 1] -> [b, 1]
            surr1 = ratios * advantages

            #[b, 1] * [b, 1] -> [b, 1]
            surr2 = torch.clamp(ratios, 0.8, 1.2) * advantages

            loss = -torch.min(surr1, surr2)
            loss = loss.mean()

            values = self.model_td(states)
            loss_td = self.loss_fn(values, targets.detach())

            #更新参数
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            self.optimizer_td.zero_grad()
            loss_td.backward()
            self.optimizer_td.step()

    def get_data(self):
        states = []
        rewards = []
        actions = []
        next_states = []
        overs = []

        #初始化游戏
        state = env.reset()

        #玩到游戏结束为止
        over = False
        while not over:
            #根据当前状态得到一个动作
            action = self.get_action(state)

            #执行动作,得到反馈
            next_state, reward, over, _ = env.step(action)

            #记录数据样本
            states.append(state)
            rewards.append(reward)
            actions.append(action)
            next_states.append(next_state)
            overs.append(over)

            #更新游戏状态,开始下一个动作
            state = next_state

        #[b, 4]
        states = torch.FloatTensor(states).reshape(-1, 4)
        #[b, 1]
        rewards = torch.FloatTensor(rewards).reshape(-1, 1)
        #[b, 1]
        actions = torch.LongTensor(actions).reshape(-1, 1)
        #[b, 4]
        next_states = torch.FloatTensor(next_states).reshape(-1, 4)
        #[b, 1]
        overs = torch.LongTensor(overs).reshape(-1, 1)

        return states, rewards, actions, next_states, overs

        return reward_sum

```
然后训练教师模型

```python
teacher = PPO()

for i in range(500):
    teacher.train(*teacher.get_data())

#使用训练好的模型获取一批教师数据
teacher_states, _, teacher_actions, _, _ = teacher.get_data()

#删除教师,只留下教师的数据就可以了
del teacher

```
创建学生模型和鉴别网络：

```python
#初始化学生模型
student = PPO()
#定义鉴别器网络,它的任务是鉴定一批数据是出自teacher还是student
class Discriminator(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.sequential = torch.nn.Sequential(
            torch.nn.Linear(6, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 1),
            torch.nn.Sigmoid(),
        )
    def forward(self, states, actions):
        one_hot = torch.nn.functional.one_hot(actions.squeeze(dim=1),
                                              num_classes=2)
        cat = torch.cat([states, one_hot], dim=1)
        return self.sequential(cat)
discriminator = Discriminator()
```
模仿学习的具体方法：

```python
#模仿学习
def copy_learn():
    optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-3)
    # 定义了一个二元交叉熵损失函数（Binary Cross Entropy Loss）。这个损失函数通常用于二分类问题
    bce_loss = torch.nn.BCELoss()
    for i in range(500):
        #使用学生模型获取一局游戏的数据,不需要reward
        states, _, actions, next_states, overs = student.get_data()
        #使用鉴别器鉴定两批数据是来自教师的还是学生的
        prob_teacher = discriminator(teacher_states, teacher_actions)
        prob_student = discriminator(states, actions)
        #老师的用0表示,学生的用1表示,计算二分类loss
        #计算教师模型数据的损失，目标是所有教师模型的数据被鉴别器预测为0
        loss_teacher = bce_loss(prob_teacher, torch.zeros_like(prob_teacher))
        #计算学生模型数据的损失，目标是所有学生模型的数据被鉴别器预测为1。
        loss_student = bce_loss(prob_student, torch.ones_like(prob_student))
        loss = loss_teacher + loss_student
        #调整鉴别器的loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        #使用一批数据来自学生的概率作为reward,取log,再符号取反
        #因为鉴别器会把学生数据的概率贴近1,所以目标是让鉴别器无法分辨,这是一种对抗网络的思路
        #计算学生模型的奖励，奖励是学生模型数据被鉴别器预测为来自学生模型的概率的负对数。
        rewards = -prob_student.log().detach()
        #更新学生模型参数,使用PPO模型本身的更新方式
        student.train(states, rewards, actions, next_states, overs)
copy_learn()
```
然后是测试模型：

```python
print("训练完毕")
torch.save(student,"model_imitate.pth")
# student = torch.load("model_imitate.pth")
env1 = gym.make('CartPole-v1', render_mode='human')
#初始化游戏
state = env1.reset()
state = state[0]
print("state before reshaping:", state)
#记录反馈值的和,这个值越大越好
reward_sum = 0
#玩到游戏结束为止
over = False
while not over:
    # 根据当前状态得到一个动作
    action = student.get_action(state)
    # 执行动作,得到反馈
    state, reward, over, *_ = env1.step(action)
    reward_sum += reward
print("reward_sum:", reward_sum)
```


# 离线学习CQL
**CQL（Conservative Q-Learning）**：CQL是一种离线强化学习算法。离线强化学习，也被称为批量强化学习，是指在没有与环境交互的情况下，仅使用预先收集的数据进行学习。CQL的主要目标是解决离线强化学习中的过度估计问题，它通过显式地对Q函数进行惩罚来实现这一目标。代码待学习。




# MPC
MPC（Model Predictive Control）是一种优化控制策略，也被称为预测控制，会在行动之前进行推演。








