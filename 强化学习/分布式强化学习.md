在同等级规模的任务中，计算的性能和表现取决于以下两 类策略：一类是合并多个计算单元共同计算一个任务，另一类是分别使用多个计算单元同 时并行计算多个任务。基于第一类计算策略，随着越来越多的计算单元用于一个计算任务， 完成任务的效率会先上升，然后会因为一些瓶颈的环节而逐渐收敛。因而在深度强化学习中，在有计算资源充足的前提下，将**一个计算任务拆分成多个相互独立的子任务**，并且将 每个子任务分配适当的计算资源进行并行计算，是寻求计算效率提升的重要方向。

在监督学习中，一种简单的提升学习速度的方法是同时训练多种不同的训练数据。然而，在深度强化学习中，智能体和环境需要在时间上顺序多次交互来逐步获得有效信息，因而不可能把所有数据集合在一起让模型同时学习。在深度强化学习中提高并行计算的能力可以通过让 智能体在训练中同时并行学习多个训练轨迹，或者可以积累批量的数据来训练深度强化学习模型中的参数


### 星形结构
在并行计算中，最普及的数据计算和传输方法采用类似星形的拓扑结构，是由一个主节点和 多个奴隶节点组成的。主节点整体上管理数据信息，完成从奴隶节点的数据分发和收集。基于从奴隶节点收集到的数据，总体的网络参数将得到学习并更新。每一个奴隶节点，相应地，会从主 节点收到分配的数据，进行具体的数据计算，并将计算的结果提交给主节点。由于在主节点的管理下，同时可以有多个奴隶节点进行数据计算。这样的并行计算可以合作高效地完成大规模模型 参数训练问题。

星形结构在解决深度强化学习的问题中得到了广泛的应用，例如，在 Actor-Critic 方法的并行计算版本，通常会采用**一个主节点**，以及***多个奴隶节点***。每个奴隶节点会维护一个深度策略网络，该策略网络的结构和其他奴隶节点和主节点一样。因此，奴隶节点在初始化的时候会从主节点同步策略网络的参数，然后其将独立与环境交互学习。在与环境交互之后，奴隶节点将再次和主节点通信，将学习到的信息提交给主节点，其中学习到的信息基于不同的架构可以是

- 单步探索所得到的经验
- 连续探索的轨迹的经验
- 存储的带有权重的探索经验
- 网络参数的梯度信息

 等等。在收集到每个奴隶节点探索并学习的经验和反馈之后，主节点将更新其网络的参数，并同步给所有奴隶节点，用于奴隶节点下一轮的探索。



然而基于不同的计算能力，不同奴隶节点完成探索并收集经验的时间可能不会完全相同，因而制定数据传输的模式会因所解决的问题和系统架构的不同而有所变化。一般来说，其分为同步通信和异步通信两种模式。



### 同步通信
![](images/Pasted%20image%2020230918151020.png)

红色的区间代表数据通信所使用的时间，蓝色的区间表示 与环境交互和数据计算所使用的的时间。主节点将用相同**固定的时间段**同时与所有奴隶节点进行通 信。然而，有更强算力的奴隶节点不得不等待其他所有弱算力的节点完成本轮计算任务之后才能 继续和主节点通信，所以在奴隶节点中会有大量**计算资源**因为等待同步而造成**浪费**。



### 异步通信

为了减少奴隶节点的等待时间，提升计算资源的使用效率，异步通信模式相应被提出。
![](images/Pasted%20image%2020230918180955.png)

如图，只要奴隶节点完成了本轮的计算或探索任务，可以**立刻**将信息提交给主节点。只有 奴隶节点提交信息，主节点才会用收集到的信息更新网络参数并与该奴隶节点进行同步，使其奴 隶节点能够继续开始下一轮的计算或探索任务。基于此种方式，主节点和众多奴隶节点的数据通 信将会在**多段不同的时间区间**内完成，主节点需要不定时地与不同的奴隶节点进行通信，确保奴隶节点中的计算资源得到了充分的利用。



### 分布式强化学习

#### A3C

Asynchronous Advantage Actor-Critic，
![](images/Pasted%20image%2020230918182037.png)

如图，很多个Actor-learner将和多个独立且完全相同的环境交互，而且使用A2C算法更新网络参数

每个Actor-learner都需要维护一个策略网络Actor和一个价值网络Critic来指导其在与环境的交互中采取明智的决策动作。

然后最上面的参数服务器是让所有的行动-学习者的网络参数初始化相同且保持同步。

##### 学习过程

对每一个Actor-Learner，在他的每个学习周期中，

1. 通过异步通信，每个行动-学习者首先会从参数服务器中**同步**其网络参数
2. 基于更新的策略网络，Actor-Learner会做出决策动作并与环境一次episode中最多交互$t_{max}$次，交互的经验会被收集并用来训练其自身的策略网络和价值网络，分别得到两个网络参数的梯度$dθ$和$dθ_v$ 。
3. 在交互完总的探索步数 $T_{max}$次之后，Actor-Learner会将两个网络所有累积的**梯度之和**提交给参数服务器，使其能够分别异步更新网络服务器中的网络参数$θ$和$θ_v$
![](images/Pasted%20image%2020230918183435.png)



#### GA3C

为了更好地利用 GPU 的计算资源从而提高整体计算效率，A3C 进一步优化提升为 GPU/CPU 混合式异步优势 Actor-Critic，即Hybrid GPU/CPU A3C

![](images/Pasted%20image%2020230918183738.png)

可以看到，现在的GA3C由智能体、预测者和训练者三部分组成了。

- **智能体**： 多个智能体分别与模拟的环境进行交互，然而每个智能体自身**不需要维护**一个策略网络来指导其做出决策动作，而是基于当前的状态$S_t$将如何决策的请求发送给预测序列，然后预测者根据整体策略网络来提供决策建议。 然后当采取 $A_t$动作之后，获得奖励 $R_t$并进入状态 $S_{t+1}$时，智能体将其探索经验的信息 ($S_t$, $A_t$, $R_t$,$S_{t+1}$)发送给训练者，用于学习网络参数的训练提升。
- **预测者**：从预测队列中批量获取决策请求，并将其输入决策网络中，为每一个请求得到建议的决策动作。由于**批量**的数据输入使得模型在推论时可以利用 **GPU 的并行计算能力**，因而提升 了学习模型的计算效率。
- **训练者**：在收到多个智能体的交互经验信息后，训练者会将信息存储在训练队列中，并从中 批量选取数据，用于整体策略网络和价值网络的模型训练。在模型训练中，**批量**的信息输入利用 **GPU 的并行计算能力**提升了计算效率，同时也降低了训练结果的方差。



#### DPPO

分布式近端策略优化（Distributed Proximal Policy Optimization，DPPO）

其架构如下：
![](images/Pasted%20image%2020230918184916.png)

数据的采集和梯度计算分布在多个工人中执行，从而大大降低了学习的 时间。领导者**周期性地接收**每一个工人提交的平均梯度值，然后更新其自身的网络参数并将最新的网络参数同步更新给所有工人

这里还是采用PPO-clip算法。

##### 学习过程

**领导者伪代码：**
![](images/Pasted%20image%2020230918185440.png)
![](images/Pasted%20image%2020230918185450.png)

在每一次迭代中，策略网络和价值网络分别将执行 M 和 B 次子迭代。在每一次子迭代中，领导 者至少等待所有工人提交 (W − D) 组梯度数据，然后用所有这些梯度的**均值**来更新网络参数。更新的网络参数将会和所有工人同步，用于其之后的采样和梯度计算。



**工人伪代码：**

工人会先收集数据样本并计算梯度，然后将梯度传递给领导者。在每次迭代中，工人首先会收集一组数据$D_k$，并根据收集的数据计算算法中的 $\hat{G_t}$或$\hat{A_t}$，将当前探索的策略 $π_θ$存储为$π_{old}$，在策略网络和价 值网络中分别重复 M 和 B 次子迭代过程，
![](images/Pasted%20image%2020230918190223.png)





#### IMPALA

重要性加权的行动者-学习者结构（Importance Weighted Actor-Learner Architecture，IMPALA）在分布式计算中使用智能体探索轨迹的**所有经验**作为通信信息

架构变成这样了：
![](images/Pasted%20image%2020230918190531.png)

- **行动者**：每个行动者中会有一个**复制的策略网络**，用于在和模拟的环境交互时做出决策，在 交互时收集到的经验将会**存储到缓冲区**中，在与环境交互固定次数后，每个行动者会将其存储的探索轨迹经验发送给学习者，并和其他行动者以**同步通信**的方式从学习者中收到更新的策略网络参数信息。
- **学习者**：通过和行动者通信，学习者收到所有行动者收集的轨迹经验信息并用其训练模型



每个学习者会和 不同的行动者通信并独立完成模型训练，但周期性地，所有工人学习者需要和主学习者通信，每 个工人学习者会将学习到的网络参数梯度发送给主学习者，然后主学习者会更新其自身的网络参 数并同步更新到所有的工人学习中。



设在状态 $S_T$下的价值估计为n步V轨迹Target，定义是：
![](images/Pasted%20image%2020230918191825.png)

这里的 $\delta_tV$是指时间差分， $\delta _ {t}  V=  \rho _ {t}  (  R_ {t}  +  \gamma  V(  S_ {t+1}  )-V(  S_ {t}  ))$
![](images/Pasted%20image%2020230918192156.png)

$π$是学习者的决策策略，$μ$是上一轮同步时所有行动者的策略 $μ$ 的均值。





#### SEED

可扩展高效深度强化学习（Scalable，Efficient Deep-RL，SEED）架构.

他和IMPALA很像，只是策略网络的推断过程会从**行动者部分转移到学习者**中，从而 降低了行动者的算力要求和通信延时，所以，很多弱算力的计算资源可以加入架构中并成为独立的行动者

根据学习者 指导的决策动作，每一个行动者将一步的经验反馈给学习者，其反馈的经验信息将首先存储在**学习者的经验缓冲器**中。
![](images/Pasted%20image%2020230918193046.png)



#### Ape-X

这是典型的包含带有优先级的经验回放部分的分布式架构

除了行动者和学习者，架构中还有回放缓冲区收集所 有行动者采集的信息，维护并更新每一个存储经验的优先级，并且根据优先程度从中批量选取数 据发送给学习者进行模型训练。经过回放缓冲区的处理，批量且标有优先级的训练数据能够有效 地提升计算效率及模型训练结果
![](images/Pasted%20image%2020230918193445.png)

##### 学习过程

**行动者的伪代码**：
![](images/Pasted%20image%2020230918193645.png)

行动者在收到环境带来的反馈后，行动者 会计算其中探索经验数据的优先级，并将带有优先级信息的数据发送给回放缓冲区。

当回放缓冲区从行动者中收集到**确定数目的经验**信息后，学习者将开始从回放缓冲区获取批量信息进行学习。

**学习者的伪代码**：
![](images/Pasted%20image%2020230918194036.png)

学习者首先将 回放缓冲区中获得带有优先级的批量经验数据，批量的经验数据将用来训练学习者的网络参数，并周期性地将更新的网络参数与所有行动者中的策略网络参数保持同步。

另外，在模型训练之后，采样数据的优先级将会被调整 并在回放缓冲区中更新。由于容量大小的限制，在回放缓冲区中会周期性地将具有较低优先级的 数据删除。







当训练模型分别使用 DQN 或 DPG 算法的时候，基于如上架构，**Ape-X 深度 Q 网络**（Ape-X DQN）和 **Ape-X 深度策略梯度**（Ape-X DPG）相对应被提出。



#### Gorila

像上面这种当基于深度Q网络算法时，就是Gorila算法，架构如下
![](images/Pasted%20image%2020230918194847.png)

当和参数服务器中深度 Q 网络的参数保持同步之后，行动者将在 深度 Q 网络的指导下和环境进行交互，并将通过交互收集到的经验直接发送到回放缓冲区。回 放缓冲区将存储并管理所有从行动者中收集经验信息。当从回放缓冲区中获取批量数据后，学习 者将会进行模型学习并计算 Q 网络中参数的梯度。在学习者中会用一个学习 Q 网络和一个目标 Q 网络来计算 TD 误差，其中学习 Q 网络将会在学习的每一步和参数服务器中的网络参数保持同 步，然而目标 Q 网络只在每过 N 步之后和参数服务器同步。参数服务器将周期性地从学习者中 接收网络参数的梯度信息，并更新自身的网络参数，以使之后的探索更具效率









