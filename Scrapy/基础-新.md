è¿™é‡Œé€‰æ‹©åœ¨Vscodeé‡Œé¢ä½¿ç”¨scrapy

é¦–å…ˆä½¿ç”¨Pythonå®˜æ–¹æ¨èçš„è™šæ‹Ÿç¯å¢ƒç®¡ç†æ¨¡å—æ¥åˆ›å»ºç¯å¢ƒï¼š
```python
python -m venv venv
```
- **`-m venv`**ï¼šä½¿ç”¨è¿è¡Œåº“æ¨¡å—`venv`ï¼Œç”¨äºåˆ›å»ºè™šæ‹Ÿç¯å¢ƒã€‚
- **`venv`**ï¼šè¿™æ˜¯åˆ›å»ºçš„è™šæ‹Ÿç¯å¢ƒçš„ç›®å½•åã€‚
ç„¶åæ˜¯åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸‹åˆ›å»ºè¿™ä¸ªè™šæ‹Ÿç¯å¢ƒ
![](images/Pasted%20image%2020240828162550.png)
ç„¶åæ‰€æœ‰é€šè¿‡pipå®‰è£…çš„PythonåŒ…éƒ½å°†è¢«å®‰è£…åœ¨è¿™ä¸ªç›®å½•çš„ç›¸åº”å­ç›®å½•ä¸­ï¼Œè€Œä¸ä¼šå½±å“åˆ°ç³»ç»Ÿä¸­çš„å…¶ä»–Pythonç¯å¢ƒã€‚
ç„¶åä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤æ¿€æ´»ï¼š
```ls
.\venv\Scripts\activate
```
ç»“æœæ˜¯è¿™æ ·ï¼Œè¿™å°±æ˜¯æˆåŠŸåœ°æ¿€æ´»äº†å½“å‰çš„ç¯å¢ƒï¼Œåœ¨æ­¤å‘½ä»¤æç¤ºç¬¦ä¸‹è¿è¡Œçš„ä»»ä½•Pythonå’Œpipå‘½ä»¤éƒ½å°†ä»…åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œã€‚![](images/Pasted%20image%2020240828162816.png)


ç„¶åå®‰è£…scrapyåŒ…
```python
pip install scrapy
```

# æµç¨‹å¤§æ¦‚è¯´æ˜
é¦–å…ˆå°±å¾—æŠŠsettingsæ–‡ä»¶é‡Œçš„
```python
# Obey robots.txt rules
ROBOTSTXT_OBEY = True
```
æ”¹æˆ`False`
åœ¨spidersé‡Œé¢æœ‰ä¸ª`start_requests`ï¼Œé‡Œé¢ä½¿ç”¨Urlè¯·æ±‚ä¹‹åï¼Œå¾—åˆ°å“åº”åè‡ªåŠ¨ä¼ é€’ç»™`parse()`æ¥å¤„ç†å“åº”çš„å†…å®¹ï¼Œæå–éœ€è¦çš„æ•°æ®å­—æ®µï¼Œå°†è¿™äº›æ•°æ®å°è£…åˆ° `items` ä¸­å®šä¹‰çš„ç±»ä¸­ï¼Œç„¶åè¿™äº›ç±»é€šè¿‡`pipelines`ï¼Œä¿å­˜åˆ°æ•°æ®åº“é‡Œé¢å»ã€‚
ç„¶åé‡Œé¢çš„`middlewares` å¯ä»¥å¤„ç†è¶…æ—¶è®¾ç½®ã€è¯·æ±‚å¤´çš„è‡ªå®šä¹‰ã€Cookie ç®¡ç†ç­‰é€»è¾‘ã€‚ä¸­é—´ä»¶å¯ä»¥å‡å°‘çˆ¬è™«ç±»ä¸­çš„ä»£ç é‡ï¼Œå°†ä¸€äº›é€šç”¨çš„é€»è¾‘è½¬ç§»åˆ°ä¸­é—´ä»¶ä¸­è¿›è¡Œå¤„ç†ã€‚
ç„¶å`settings`é‡Œé¢ä¸»è¦å°±æ˜¯å¯ç”¨å’Œä¸å¯ç”¨æŸäº›è®¾ç½®ï¼Œä¾‹å¦‚å¯ä»¥å¯ç”¨æˆ–ç¦ç”¨æŸä¸ªä¸­é—´ä»¶ã€æ˜¯å¦å¯ç”¨ Pipelinesã€è®¾ç½®ä¸‹è½½å»¶è¿Ÿç­‰ã€‚
# å¼€å‘æµç¨‹
é¦–å…ˆè¦ç¡®ä¿å‰é¢çš„scrapyåŒ…å’Œè™šæ‹Ÿç¯å¢ƒå·²ç»æ¿€æ´»ï¼Œå†åšè¿™ä¸€æ­¥ã€‚

åˆ›å»ºçš„ä»£ç ï¼š
```python
scrapy startproject bookscraper
```
è¿™é‡Œç¬¬ä¸‰éƒ¨åˆ†`bookscraper`æ˜¯è¿™ä¸ªé¡¹ç›®çš„åç§°ã€‚

#### å¼€å¯è®¾ç½®
åœ¨`settings.py`é‡Œé¢æ‰“å¼€`pipelines`å’Œ`middlewares`ï¼Œå¦åˆ™æ— æ³•ä½¿ç”¨:

```python
# Enable or disable downloader middlewares
DOWNLOADER_MIDDLEWARES = {
Â  Â "bookscraper.middlewares.BookscraperDownloaderMiddleware": 543,
}
# Configure item pipelines
# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
Â  Â "bookscraper.pipelines.BookscraperPipeline": 300,
}
```

#### åˆ›å»ºspider
åˆ›å»ºä»£ç ï¼š
```js
scrapy genspider bookspider books.toscrape.com
```
`bookspider`æ˜¯è¿™ä¸ªspiderçš„åå­—ï¼Œ`books.toscrape.com`æ˜¯è¿™ä¸ªspiderè¦çˆ¬å–çš„ç½‘ç«™
```ad-important
books.toscrape.comè¿™ä¸ªç½‘ç«™å°±æ˜¯ä¸“é—¨ç”¨äºè®­ç»ƒçˆ¬è™«çš„ç½‘ç«™
```
![](images/Pasted%20image%2020240828215546.png)
![](images/Pasted%20image%2020240828215600.png)
ä¸Šé¢å°±æ˜¯åˆ›å»ºå‡ºçš„æœ€åŸºæœ¬çš„spiderï¼Œ å…¶ä¸­çš„allowed_domains ä¸»è¦æ˜¯é˜²æ­¢å› ä¸ºæœ‰å¤–éƒ¨é“¾æ¥è€Œå¯¼è‡´ä¸€ç›´å‘å¤–è®¿é—®åˆ°å…¶ä»–çš„åœ°æ–¹å»äº†ã€‚


#### æ‰‹åŠ¨æŸ¥çœ‹å…ƒç´ 
ç„¶åå®‰è£…`ipython`
```js
pip install ipython
```
ç„¶ååœ¨`scrapy.cfg`é‡Œé¢é…ç½®shellå±æ€§
```python
[settings]
default = bookscraper.settings
shell = ipython
```
ç„¶åå°±å¯ä»¥åœ¨ç»ˆç«¯ä½¿ç”¨`scrapy shell`å‘½ä»¤ï¼Œå¾—åˆ°ä¸‹é¢çš„ç»“æœï¼š
![](images/Pasted%20image%2020240829095258.png)
ç„¶åä½¿ç”¨
```python
fetch('https://books.toscrape.com')
```
ç„¶åæ‰€æœ‰çš„ç»“æœéƒ½å­˜åœ¨è¿™ä¸ªå…ƒç´ é‡Œäº†
![](images/Pasted%20image%2020240829100552.png)
ç°åœ¨éœ€è¦åšçš„æ˜¯åˆ°æµè§ˆå™¨æ‰‹åŠ¨æŸ¥çœ‹ä¸€ä¸ªå…ƒç´ çš„CSSæ˜¯ä»€ä¹ˆï¼Œä¾‹å¦‚![](images/Pasted%20image%2020240829101047.png)
ç„¶åå°±ä»¥Cssé€‰æ‹©å™¨çš„å½¢å¼æ‹¿åˆ°å…ƒç´ 
```python
response.css('article.product_pod')
response.css('article.product_pod').get() #æ‹¿åˆ°htmlå­—ç¬¦ä¸²
```

å¦‚æœæ˜¯è¦æ‹¿ä¸€ä¸ªå…ƒç´ å†…çš„æ–‡æœ¬ï¼Œä¾‹å¦‚ï¼š![](images/Pasted%20image%2020240829102347.png)é‚£ä¹ˆæ˜¯ä½¿ç”¨
```python
In [13]: book.css('h3 a::text').get()
Out[13]: 'A Light in the ...'
```


å¦‚æœæ˜¯è¦æ‹¿ä¸€ä¸ªå…ƒç´ çš„å±æ€§ï¼Œä¾‹å¦‚`<a>`é‡Œçš„hrefå±æ€§ï¼Œå°±ä½¿ç”¨
```python
In [16]: book.css('h3 a').attrib['href']
Out[16]: 'catalogue/a-light-in-the-attic_1000/index.html'
```


#### ç¼–å†™parse()æ–¹æ³•
å‰é¢å·²ç»çŸ¥é“äº†å…ƒç´ çš„è·å–æ–¹æ³•ï¼Œç„¶åå°±å¯ä»¥ç¼–å†™parseå‡½æ•°äº†ï¼Œå¾ªç¯åœ°å¤„ç†é¡µé¢é‡Œçš„æ‰€æœ‰ç±»ä¼¼å…ƒç´ ã€‚åœ¨`spider.py`é‡Œé¢
```python
Â  Â  def parse(self, response):
Â  Â  Â  Â  books = response.css('article.product_pod')
Â  Â  Â  Â  for book in books:
Â  Â  Â  Â  Â  Â  yield{
Â  Â  Â  Â  Â  Â  Â  Â  'name': book.css('h3 a::text').get(),
Â  Â  Â  Â  Â  Â  Â  Â  'price': book.css('div.product_price p.price_color::text').get(),
Â  Â  Â  Â  Â  Â  Â  Â  'url':book.css('h3 a').attrib['href']
Â  Â }
```

å†™å®Œparseåï¼Œå°±å¯ä»¥å¯åŠ¨spideräº†ï¼Œ
```python
scrapy crawl bookspider
```
`bookspider`æ˜¯åœ¨spider.pyä¸­çš„nameå±æ€§å€¼
```ad-attention
ä½¿ç”¨è¿™ä¸ªå‘½ä»¤å¿…é¡»åœ¨scrapy.cfgçš„åŒçº§ç›®å½•ä¸‹ï¼Œè¿™é‡Œå…ˆæŠŠpipelineså’Œmiddlewareså…¨éƒ¨å…³é—­ï¼Œå¹¶ä¸”ä¸è¦ä½¿ç”¨ä»£ç†ï¼ï¼ï¼
```
ç„¶åç»“æœé‡Œé¢ä¼šæœ‰ä¸ª`item_scrapped_count`ï¼Œè¿™ä¸ªå±æ€§è¡¨ç¤ºçš„æ˜¯ ä»£ç ä¸­`yield`çš„æ€»æ•°
![](images/Pasted%20image%2020240829154242.png)



#### åˆ†é¡µå¤„ç†
æ•°æ®ä¸€èˆ¬æ˜¯åˆ†å¾ˆå¤šé¡µå¤„ç†ï¼Œå¦‚ä¸‹ï¼Œæ‰€ä»¥éœ€è¦åœ¨å½“å‰é¡µè½¬åˆ°ä¸‹ä¸€é¡µ
![](images/Pasted%20image%2020240829111117.png)
é¦–å…ˆä½¿ç”¨cssé€‰æ‹©å™¨æ‹¿åˆ°è¿™ä¸ªæŒ‰é’®å¯¹åº”çš„é“¾æ¥
```python
In [2]: response.css('li.next a').attrib['href']
Out[2]: 'catalogue/page-2.html'
```
ç„¶ååœ¨parse()ä¸­ä½¿ç”¨å¦‚ä¸‹çš„ä»£ç ï¼š
```python
# å¦‚æœåˆ°äº†æœ€åä¸€é¡µå°±æ²¡æœ‰é“¾æ¥äº†
next_page = response.css('li.next a').attrib['href']
if next_page is not None:
Â  next_page_url = "https://books.toscrape.com/"+next_page
Â  yield response.follow(next_page_url,callback = self.parse)
```
`follow` æ–¹æ³•å‘é€ä¸€ä¸ªæ–°çš„è¯·æ±‚ï¼Œç›®æ ‡æ˜¯æ„å»ºçš„ `next_page_url`ï¼Œ`callback=self.parse` è¡¨ç¤ºå½“æ”¶åˆ°å“åº”æ—¶ï¼Œç»§ç»­è°ƒç”¨å½“å‰çš„ `parse` æ–¹æ³•æ¥å¤„ç†ä¸‹ä¸€é¡µçš„å†…å®¹ã€‚è¿™å°±å½¢æˆäº†ä¸€ä¸ªé€’å½’è¿‡ç¨‹ï¼Œçˆ¬è™«ä¼šä¸€é¡µæ¥ä¸€é¡µåœ°æŠ“å–ï¼Œç›´åˆ°æ²¡æœ‰â€œä¸‹ä¸€é¡µâ€ä¸ºæ­¢ã€‚

#### è¯¦æƒ…é¡µå¤„ç†
ä¸€èˆ¬éƒ½æ˜¯ä»åˆ—è¡¨é¡µ-->è¯¦æƒ…é¡µï¼Œåœ¨åˆ—è¡¨é¡µé¢é‡Œé¢ä¸€ä¸ªä¸€ä¸ªèµ°å®Œè¯¦æƒ…é¡µä¹‹åå†å»ä¸‹ä¸€é¡µï¼Œé‚£ä¹ˆè¿™é‡Œä½¿ç”¨çš„æ–¹æ³•å’Œåˆ†é¡µå¤„ç†å¤§å·®ä¸å·®ï¼Œå°±æ˜¯æ‰¾åˆ°å»è¯¦æƒ…é¡µé¢çš„é“¾æ¥ï¼Œç„¶ååˆ†æé“¾æ¥å¯¹åº”çš„é¡µé¢
```python
for book in books:
   # æ‹¿åˆ°è¯¦æƒ…é¡µçš„é“¾æ¥
Â  Â relative_url = book.css('h3 a').attrib['href']
Â  Â if 'catalogue/' in relative_url:
Â  Â  Â  Â next_relative_url = "https://books.toscrape.com/"+relative_url
Â  Â else:
Â  Â  Â  Â next_relative_url = next_page_url = "https://books.toscrape.com/catalogue/"+relative_url
Â  Â  Â  Â # è¯¦æƒ…é¡µé“¾æ¥è¿”å›å“åº”åï¼Œå›è°ƒåé¢çš„å‡½æ•°å¤„ç†
Â  Â  Â  Â yield response.follow(next_relative_url,callback = self.parse_book_page)
```



#### ä½¿ç”¨Xpath
æœ‰äº›æç«¯æƒ…å†µå¯èƒ½è®©æˆ‘ä»¬æ²¡æœ‰ç®€å•çš„ç±»åæˆ–HTMLæ ‡ç­¾çš„IDï¼Œæ¯”å¦‚ä¸‹é¢è¿™ä¸ª![](images/Pasted%20image%2020240829164032.png)æƒ³å¾—åˆ°çš„æ˜¯é‚£ä¸ª Poetry ,ä½†æ˜¯çˆ¶æ ‡ç­¾`li`æ— æ³•æ‰¾åˆ°ï¼Œæ‰€ä»¥è¿™å°±éœ€è¦ä½¿ç”¨åˆ°Xpathäº†ï¼Œå› ä¸º`ul`å’Œä¸‹é¢çš„`li .active`å¯ä»¥æ‰¾åˆ°ï¼Œæ‰€ä»¥å°±ç›¸å¯¹è¿™ä¸¤ä¸ªæ ‡ç­¾æ¥æ‰¾åˆ°å½“å‰çš„æ ‡ç­¾ï¼Œå¦‚ä¸‹
```js
response.xpath("//ul[@class='breadcrumb']/li[@class='active']/preceding-sibling::li[1]/a/text()").get()
```
**æ³¨æ„æ³¨æ„æ³¨æ„ï¼Œè¿™ä¸ªæ˜¯ä¸éœ€è¦è‡ªå·±çœ‹ç€å†™çš„ï¼Œå¯ä»¥ç›´æ¥å³é”®å¤åˆ¶å‡ºæ¥ï¼š**
![](images/Pasted%20image%2020240831200046.png)

```ad-attention

- `//ul[@class='breadcrumb']` ä¼šé€‰ä¸­æ•´ä¸ª `<ul>`ã€‚
- `/li[@class='active']` ä¼šé€‰ä¸­ `Current Page` æ‰€åœ¨çš„ `<li>` å…ƒç´ ã€‚
- `/preceding-sibling::li[1]` ä¼šé€‰ä¸­ç´§é  `Current Page` ä¹‹å‰çš„é‚£ä¸ª `<li>`ï¼Œä¹Ÿå°±æ˜¯åŒ…å« `Category` é“¾æ¥çš„ `<li>`ã€‚å¯¹åº”çš„æœ‰`following-sibling`ã€‚ä½¿ç”¨[1]æ˜¯å› ä¸ºå…„å¼Ÿæ ‡ç­¾æœ‰å¾ˆå¤šä¸ª`<li>`
- `/a/text()` ä¼šæå–å‡º `Category` é“¾æ¥çš„æ–‡æœ¬ï¼Œå³ `"Category"`ã€‚

```



#### å®šä¹‰Item
`Item`æ˜¯å¸®åŠ©æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬æƒ³è¦çš„ä¸œè¥¿ï¼Œä¸»è¦æ˜¯è§„èŒƒåŒ–å’Œæ ‡å‡†åŒ–æŠ“å–çš„æ•°æ®ã€‚
![](images/Pasted%20image%2020240829192132.png)åƒä¸Šé¢è¿™æ ·çš„è¿”å›æ–¹å¼å°±æ˜¯å¾ˆä¹±ï¼Œä½†æ˜¯å¦‚æœæ˜¯è¿”å›ä¸€ä¸ªå¯¹è±¡å°±ä¼šæ˜¾å¾—æ›´åŠ æ•´é½ã€‚
å®šä¹‰è¿™ä¸ªç±»ï¼š
```python
class BookItem(scrapy.Item):
Â  Â  # define the fields for your item here like:
Â  Â  # name = scrapy.Field()
Â  Â  url = scrapy.Field()
Â  Â  title = scrapy.Field()
Â  Â  product_type = scrapy.Field()
Â  Â  price_excl_tax = scrapy.Field()
Â  Â  price_incl_tax = scrapy.Field()
Â  Â  tax = scrapy.Field()
Â  Â  availability = scrapy.Field()
Â  Â  num_reviews = scrapy.Field()
Â  Â  stars = scrapy.Field()
Â  Â  category = scrapy.Field()
Â  Â  description = scrapy.Field()
Â  Â  price = scrapy.Field()
```
ä½¿ç”¨ï¼š![](images/Pasted%20image%2020240829201125.png)


#### ä½¿ç”¨pipelines
è¿™ä¸ªä¸»è¦æ˜¯æ¥å¯¹æ”¶é›†åˆ°çš„æ•°æ®è¿›è¡Œå¤„ç†ï¼Œæ¯”å¦‚è¯´æŠŠå­—ç¬¦ä¸²è½¬ä¸ºæ•°å­—ï¼Œå»æ‰ç©ºæ ¼ï¼ŒåŠ ä¸ªç¬¦å·ä¹‹ç±»çš„ã€‚
é‡Œé¢æœ‰ä¸ªç±»æ˜¯`ItemAdapter`ï¼Œ**å¾ˆé‡è¦**ï¼Œå¯ä»¥éå†ä¼ å…¥çš„ç±»çš„å±æ€§ï¼Œç„¶åè¿›è¡Œå¤„ç†ï¼Œä½¿ç”¨ç¤ºä¾‹ï¼š
```python
from itemadapter import ItemAdapter

class BookscraperPipeline:
Â  Â  def process_item(self, item, spider):
Â  Â  Â  Â  adapter = ItemAdapter(item)
Â  Â  Â  Â  field_names = adapter.field_names()
Â  Â  Â  Â  # å»æ‰ç©ºæ ¼
Â  Â  Â  Â  for field_name in field_names:
Â  Â  Â  Â  Â  Â  if field_name != 'description':
Â  Â  Â  Â  Â  Â  Â  Â  value = adapter.get(field_name)
Â  Â  Â  Â  Â  Â  Â  Â  adapter[field_name] = value.strip()
Â  Â  Â  Â  return item
```
å¯ä»¥çœ‹åˆ°ä¸»è¦æ˜¯ `get()` å’Œ`[]`ä¸¤ç§ç”¨æ³•ã€‚

ç¼–å†™`pipelines`å®Œæˆåè¦æ³¨æ„åœ¨`settings.py`é‡Œé¢å¯åŠ¨å®ƒã€‚


#### æ•°æ®ä¿å­˜æ–‡ä»¶
æœ€ç›´æ¥çš„æ–¹å¼å°±æ˜¯åœ¨å‘½ä»¤ä¸ŠæŒ‡å®šï¼š
```python
 scrapy crawl bookspider -O bookdata.csv
```
å¯ä»¥æ˜¯`csv`ï¼Œä¹Ÿå¯ä»¥æ˜¯`json`


ä¹Ÿå¯ä»¥é€šè¿‡åœ¨`settings.py`é‡Œé¢è®¾ç½®æ•°æ®çš„ä¿å­˜æ–¹æ³•ï¼š
```python
FEEDS = {
Â  Â  'booksdata.json':{'format':'json'}
}
```
ç„¶åè¿™æ ·å°±ä¸éœ€è¦æŒ‡å®š -0 äº†ã€‚åªéœ€è¦
```python
 scrapy crawl bookspider
```


ç¬¬ä¸‰ç§å°±æ˜¯åœ¨`pipelines`é‡Œé¢ä¿å­˜æ•°æ®ï¼Œä¹Ÿåªæœ‰è¿™ç§æ–¹å¼å¯ä»¥ä¿å­˜åˆ°æ•°æ®åº“å†…ã€‚
é¦–å…ˆå®‰è£…ï¼š
```python
pip install mysql mysql-connector-python
```
ç„¶ååˆ›å»ºä¸€ä¸ªä½¿ç”¨MySQLçš„pipelineï¼Œåœ¨åˆå§‹åŒ–ç±»æ—¶åˆ›å»ºå‡ºè¿æ¥ï¼Œç¼–å†™`process_item(self, item, spider)`æ–¹æ³•ï¼Œç„¶åspiderå…³é—­æ—¶å…³é—­è¿æ¥ï¼Œå¦‚ä¸‹
```python
import mysql.connector
class SaveToMySQLPipeline:
Â  Â  def __init__(self) :
Â  Â  Â  Â  self.conn = mysql.connector.connect(
Â  Â  Â  Â  Â  Â  host = 'localhost',
Â  Â  Â  Â  Â  Â  user='root',
Â  Â  Â  Â  Â  Â  password='root',
Â  Â  Â  Â  Â  Â  database='books'
Â  Â  Â  Â  )
Â  Â  Â  Â  self.cur = self.conn.cursor()
Â  Â  Â  Â  self.cur.execute("""
Â  Â  Â  Â  Â  Â  CREATE TABLE IF NOT EXISTS books (
Â  Â  Â  Â  Â  Â  id INT AUTO_INCREMENT PRIMARY KEY ,
Â  Â  Â  Â  Â  Â  url VARCHAR(255),
Â  Â  Â  Â  Â  Â  price DECIMAL
Â  Â  Â  Â  Â  Â  );
Â  Â  Â  Â  """)
Â  Â  def process_item(self, item, spider):
Â  Â  Â  Â  self.cur.execute("""
Â  Â  Â  Â  Â  Â  INSERT INTO books (
Â  Â  Â  Â  Â  Â  Â  Â  url,
Â  Â  Â  Â  Â  Â  Â  Â  price
Â  Â  Â  Â  Â  Â  ) VALUES (%s,%s,%s);Â """,item["url"],item["price"])
Â  Â  Â  Â  self.conn.commit()
Â  Â  Â  Â  return item
Â  Â  # å½“spiderå‡†å¤‡å…³é—­æ—¶
Â  Â  def close_spider(self,spider):
Â  Â  Â  Â  self.cur.close()
Â  Â  Â  Â  self.conn.close()
```
ç„¶ååœ¨`settings`é‡Œé¢å¼€å¯è¿™ä¸ªpipelineï¼Œ
```python
ITEM_PIPELINES = {
Â  Â "bookscraper.pipelines.BookscraperPipeline": 300,
Â  Â "bookscraper.pipelines.SaveToMySQLPipeline": 400,
}
```
åé¢çš„æ•°å­—æ˜¯å†³å®šæ‰§è¡Œçš„é¡ºåºï¼Œæ•°å­—è¶Šå°ï¼Œè¶Šå…ˆæ‰§è¡Œã€‚









# è§£å†³Blocked
é€šè¿‡æ£€æŸ¥ HTTP è¯·æ±‚ä¸­çš„ `User-Agent` å­—ç¬¦ä¸²ï¼Œç½‘ç«™å¯ä»¥åŒºåˆ†è¯·æ±‚æ˜¯æ¥è‡ªæµè§ˆå™¨è¿˜æ˜¯å…¶ä»–ç±»å‹çš„ HTTP å®¢æˆ·ç«¯ï¼Œå¦‚ API è¯·æ±‚æˆ–ç‰¹å®šç±»å‹çš„çˆ¬è™«ã€‚`User-Agent` å­—ç¬¦ä¸²é€šå¸¸åŒ…å«äº†å…³äºå‘å‡ºè¯·æ±‚çš„æµè§ˆå™¨æˆ–å…¶ä»–å®¢æˆ·ç«¯çš„ä¿¡æ¯ï¼Œæ¯”å¦‚å®ƒçš„åç§°ã€ç‰ˆæœ¬ä»¥åŠæ“ä½œç³»ç»Ÿã€‚
å¦‚æœ User-Agent æ˜¾ç¤ºä¸ºæŸäº›ç‰¹å®šçš„åº“ï¼ˆå¦‚ Python çš„ `requests` é»˜è®¤ User-Agent æˆ–è€…æŸäº›çˆ¬è™«æ¡†æ¶ï¼‰ï¼Œç½‘ç«™å¯èƒ½ä¼šé€‰æ‹©é˜»æ­¢è¿™äº›è¯·æ±‚ã€‚
å¦‚æœHeadersé‡Œé¢å†…å®¹æ¯æ¬¡éƒ½ç›¸ä¼¼çš„è¯ä¹Ÿå®¹æ˜“è¢«çœ‹å‡ºæ¥æ˜¯çˆ¬è™«

æ‰€ä»¥è§£å†³æ–¹æ³•å°±æ˜¯è®©æ¯æ¬¡è¯·æ±‚çš„è¿™äº›å†…å®¹éƒ½å‘ç”Ÿæ”¹å˜ã€‚

#### å¤åˆ¶è‡ªå·±çš„user-agent
åœ¨æµè§ˆå™¨è®¿é—®è¦çˆ¬å–çš„é¡µé¢æ‰¾åˆ°é‚£ä¸ªUser-Agent, ç„¶åå¤åˆ¶åˆ°settings.py,
![](images/Pasted%20image%2020240830154818.png)
åœ¨`settings`é‡Œ
```python
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0"
```
ä½†æ˜¯è¿™ç§å›ºå®šä¸€ä¸ªçš„æ–¹å¼å°±ä¼šè¢«ç½‘ç«™è®¤ä¸ºåŒä¸€ä¸ªäººè¿ç»­è¯·æ±‚äº†å¾ˆå¤šæ¬¡ï¼Œå®¹æ˜“è¢«æ€€ç–‘ä¸ºçˆ¬è™«ã€‚


#### è½®è¯¢agent
åœ¨`spider`å†…æ”¾ä¸€ä¸ª`user-agent`çš„åˆ—è¡¨
```python
user_agent_list = [
Â  Â  Â  Â  Â  Â  'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
Â  Â  Â  Â  Â  Â  'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',
Â  Â  Â  Â  Â  Â  'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0'
Â  Â  Â  Â  Â  Â  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0"
]
```
ç„¶ååœ¨è¯·æ±‚çš„åœ°æ–¹ä½¿ç”¨è¿™ä¸ª`user-agent`æ¥ä»£æ›¿
```python
yield response.follow(next_relative_url,callback = self.parse_book_page , headers={"User-Agent": self.user_agent_list[random.randint(0,len(self.user_agent_list)-1)]})
```
å°±æ˜¯éšæœºæŒ‘ä¸€ä¸ª`user-agent`å»è¯·æ±‚

ä½†æ˜¯ï¼ï¼ï¼å¦‚æœçŸ­æ—¶é—´å‡ åƒä¸ªè¯·æ±‚ï¼Œä½†æ˜¯åªæœ‰è¿™äº”ç§ç”¨æˆ·ï¼Œè¿˜æ˜¯ä¼šè¢«æ€€ç–‘æ˜¯çˆ¬è™«ã€‚


#### ç¬¬ä¸‰æ–¹çš„user-agent
ç¬¬ä¸‰ç§æ–¹æ³•æ˜¯ï¼Œåœ¨`midllewares`é‡Œé¢ï¼Œä½¿ç”¨æ¥è‡ªç¬¬ä¸‰æ–¹ç½‘ç«™çš„ç”¨æˆ·ä»£ç†ã€‚
[ScrapeOps - The DevOps Tool For Web Scraping. | ScrapeOps](https://scrapeops.io/)
ç„¶å
![](images/Pasted%20image%2020240830162922.png)

è¿™é‡Œä½¿ç”¨æ–¹æ³•å·²ç»è¯´çš„å¾ˆæ˜ç™½äº†ï¼š![](images/Pasted%20image%2020240830171157.png)
ç¼–å†™ä¸ª`middleware`ï¼Œåˆå§‹åŒ–æ—¶è·å–å¾ˆå¤šä¸ªä¼ªé€ çš„`User-Agent`ï¼Œç„¶åè¯·æ±‚æ—¶éšæœºå–å…¶ä¸­ä¸€ä¸ªï¼Œä»£ç åŸºæœ¬æ˜¯é€šç”¨çš„ï¼š
```python
from urllib.parse import urlencode
from random import randint
import requests
  
class ScrapeOpsFakeUserAgentMiddleware:
Â  Â  @classmethod
Â  Â  def from_crawler(cls, crawler):
Â  Â  Â  Â  return cls(crawler.settings)
Â  Â  def __init__(self,settings) -> None:
Â  Â  Â  Â  self.scrapeops_api_key = settings.get("SCRAPEOPS_API_KEY")
Â  Â  Â  Â  self.scrapeops_endpoint = settings.get("SCRAPEOPS_FAKE_USER_AGENT_ENDPOINT","https://headers.scrapeops.io/v1/user-agents")
Â  Â  Â  Â  self.scrapeops_fake_user_agents_active = settings.get("SCRAPEOPS_FAKE_USER_AGENT_ENABLED",False)
Â  Â  Â  Â  self.scrapeops_num_results = settings.get("SCRAPEOPS_NUM_RESULTS")
Â  Â  Â  Â  self.headers_list =[]
Â  Â  Â  Â  self._get_user_agents_list()
Â  Â  Â  Â  self._scrapeops_fake_user_agents_enabled()
Â  Â  def _get_user_agents_list(self):
Â  Â  Â  Â  payload = {'api_key': self.scrapeops_api_key}
Â  Â  Â  Â  if self.scrapeops_num_results is not None:
Â  Â  Â  Â  Â  Â  payload['num_results'] = self.scrapeops_num_results
Â  Â  Â  Â  response = requests.get(self.scrapeops_endpoint,params=urlencode(payload))
Â  Â  Â  Â  json_response = response.json()
Â  Â  Â  Â  self.user_agents_list = json_response.get('result',[])
Â  Â  def _get_random_user_agent(self):
Â  Â  Â  Â  random_index = randint(0,len(self.user_agents_list)-1)
Â  Â  Â  Â  return self.user_agents_list[random_index]
Â  Â  def _scrapeops_fake_user_agents_enabled(self):
Â  Â  Â  Â  if(self.scrapeops_api_key is None or self.scrapeops_api_key=='' or self.scrapeops_num_results==0):
Â  Â  Â  Â  Â  Â  self.scrapeops_fake_user_agents_active=False
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  self.scrapeops_fake_user_agents_active=True
Â  Â  def process_request(self, request, spider):
Â  Â  Â  Â  random_user_agent = self._get_random_user_agent()
Â  Â  Â  Â  request.headers['User-Agent']=random_user_agent
```
ç„¶ååœ¨`settings`é‡Œé¢é…ç½®ç›¸å…³å‚æ•°ï¼š
```python
# æ”¹æˆè‡ªå·±çš„API_KEY
SCRAPEOPS_API_KEY="0265994d-3869-4db2-bd67-2513b4513d2b"
SCRAPEOPS_FAKE_USER_AGENT_ENDPOINT="https://headers.scrapeops.io/v1/user-agents"
SCRAPEOPS_FAKE_USER_AGENT_ENABLED=True
SCRAPEOPS_NUM_RESULTS=50
```
å¹¶ä¸”åœ¨`settings`é‡Œé¢å¯ç”¨è¿™ä¸ªmiddlewareï¼š
```python
DOWNLOADER_MIDDLEWARES = {
Â  Â "bookscraper.middlewares.ScrapeOpsFakeUserAgentMiddleware": 400,
}
```
æ³¨æ„æ˜¯`DOWNLOADER_MIDDLEWARES`


#### ç¬¬ä¸‰æ–¹çš„header
ä½¿ç”¨è¿™ä¸ªå°±ä¸è¦ä½¿ç”¨ä¸Šé¢çš„äº†ï¼Œå› ä¸ºè¿™é‡ŒåŒæ—¶æ›´æ”¹äº†`user-agent`
é™¤äº†`user-agent`ï¼Œ`headers`ä¹Ÿæ¯æ¬¡éƒ½æ˜¯æ–°çš„æ˜¯æœ€ä¿é™©çš„ï¼Œä»£ç å¦‚ä¸‹ï¼š
```python
class ScrapeOpsFakeBrowserHeaderAgentMiddleware:
Â  Â  @classmethod
Â  Â  def from_crawler(cls, crawler):
Â  Â  Â  Â  return cls(crawler.settings)
Â  Â  def __init__(self, settings):
Â  Â  Â  Â  self.scrapeops_api_key = settings.get('SCRAPEOPS_API_KEY')
Â  Â  Â  Â  self.scrapeops_endpoint = settings.get('SCRAPEOPS_FAKE_BROWSER_HEADER_ENDPOINT', 'https://headers.scrapeops.io/v1/browser-headers')
Â  Â  Â  Â  self.scrapeops_fake_browser_headers_active = settings.get('SCRAPEOPS_FAKE_BROWSER_HEADER_ENABLED', False)
Â  Â  Â  Â  self.scrapeops_num_results = settings.get('SCRAPEOPS_NUM_RESULTS')
Â  Â  Â  Â  self.headers_list = []
Â  Â  Â  Â  self._get_headers_list()
Â  Â  Â  Â  self._scrapeops_fake_browser_headers_enabled()
Â  Â  def _get_headers_list(self):
Â  Â  Â  Â  payload = {'api_key': self.scrapeops_api_key}
Â  Â  Â  Â  if self.scrapeops_num_results is not None:
Â  Â  Â  Â  Â  Â  payload['num_results'] = self.scrapeops_num_results
Â  Â  Â  Â  response = requests.get(self.scrapeops_endpoint, params=urlencode(payload))
Â  Â  Â  Â  json_response = response.json()
Â  Â  Â  Â  self.headers_list = json_response.get('result', [])
Â  Â  def _get_random_browser_header(self):
Â  Â  Â  Â  random_index = randint(0, len(self.headers_list) - 1)
Â  Â  Â  Â  return self.headers_list[random_index]
Â  Â  def _scrapeops_fake_browser_headers_enabled(self):
Â  Â  Â  Â  if self.scrapeops_api_key is None or self.scrapeops_api_key == '' or self.scrapeops_fake_browser_headers_active == False:
Â  Â  Â  Â  Â  Â  self.scrapeops_fake_browser_headers_active = False
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  self.scrapeops_fake_browser_headers_active = True
Â  Â  def process_request(self, request, spider): Â  Â  Â  Â 
Â  Â  Â  Â  random_browser_header = self._get_random_browser_header()
Â  Â  Â  Â  request.headers['accept-language'] = random_browser_header['accept-language']
Â  Â  Â  Â  request.headers['sec-fetch-user'] = random_browser_header['sec-fetch-user']
Â  Â  Â  Â  request.headers['sec-fetch-mod'] = random_browser_header['sec-fetch-mod']
Â  Â  Â  Â  request.headers['sec-fetch-site'] = random_browser_header['sec-fetch-site']
Â  Â  Â  Â  request.headers['sec-ch-ua-platform'] = random_browser_header['sec-ch-ua-platform']
Â  Â  Â  Â  request.headers['sec-ch-ua-mobile'] = random_browser_header['sec-ch-ua-mobile']
Â  Â  Â  Â  request.headers['sec-ch-ua'] = random_browser_header['sec-ch-ua']
Â  Â  Â  Â  request.headers['accept'] = random_browser_header['accept']
Â  Â  Â  Â  request.headers['user-agent'] = random_browser_header['user-agent']
Â  Â  Â  Â  request.headers['upgrade-insecure-requests'] = random_browser_header.get('upgrade-insecure-requests')
Â  Â  Â  Â  print("************ NEW HEADER ATTACHED *******")
Â  Â  Â  Â  print(request.headers)
```
ç„¶ååœ¨`settings`é‡Œé¢è®¾ç½®ç›¸å…³å‚æ•°ï¼Œå¹¶å¼€å¯è¿™ä¸ª`middleware`
```python
SCRAPEOPS_API_KEY="0265994d-3869-4db2-bd67-2513b4513d2b"
SCRAPEOPS_FAKE_BROWSER_HEADER_ENDPOINT="https://headers.scrapeops.io/v1/browser-headers"
SCRAPEOPS_FAKE_BROWSER_HEADER_ENABLED=True
SCRAPEOPS_NUM_RESULTS=5
DOWNLOADER_MIDDLEWARES = {
Â  #"bookscraper.middlewares.ScrapeOpsFakeUserAgentMiddleware": 400,
Â Â "bookscraper.middlewares.ScrapeOpsFakeBrowserHeaderAgentMiddleware": 400,
}
```


# è§£å†³anti-bot
ä¸Šé¢æ˜¯è§£å†³äº†ç½‘ç«™é€šè¿‡è¯·æ±‚å¤´æ¥ç¡®è®¤æ˜¯å¦çˆ¬è™«çš„æƒ…å†µï¼Œä½†æ˜¯éšç€è¯·æ±‚å¤´ä¼ è¾“çš„è¿˜æœ‰IPåœ°å€ï¼Œè¿™ä¸ªä¹Ÿæœ‰å¯èƒ½è¢«ç½‘ç«™ç”¨æ¥åˆ¤æ–­æ˜¯å¦ä¸ºçˆ¬è™«ã€‚æ‰€ä»¥è¿™é‡Œä½¿ç”¨ä»£ç†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°±æ˜¯è½®æ¢IPåœ°å€ã€‚

#### scrapy-rotating-proxies
é¦–å…ˆå®‰è£…
```python
pip install scrapy-rotating-proxies
```
ç„¶ååœ¨settingsé‡Œé¢æ·»åŠ 
```python
ROTATING_PROXY_LIST = [
Â  Â  'proxy1.com:8000',
Â  Â  'proxy2.com:8031',
]
```
è¿™é‡Œçš„IPå’Œç«¯å£å°±æ˜¯è¦ä»£ç†åˆ°çš„å…¶ä»–æœåŠ¡å™¨ï¼Œé‚£ä¹ˆè¿™é‡Œæœ‰åœ¨çº¿çš„æœåŠ¡å™¨å¯ä¾›ä½¿ç”¨ï¼š
[ğŸ¤– Free Proxy List [1000+ IPs Online] (geonode.com)](https://geonode.com/free-proxy-list)
![](images/Pasted%20image%2020240830203417.png)
å°±åœ¨è¿™é‡Œé¢æŒ‘ä¸¤ä¸‰ä¸ªå‡ºæ¥ä½¿ç”¨ï¼Œä¸Šé¢çš„åˆ—è¡¨å°±æ”¹æˆ
```python
ROTATING_PROXY_LIST = [
Â  Â  '160.248.190.253:3128',
Â  Â  '15.204.161.192:18080',
Â  Â  '51.89.14.70:80'
]
```
ç„¶ååœ¨`settings`é‡Œé¢å¼€å¯è¿™ä¸ªmiddleware
```python
DOWNLOADER_MIDDLEWARES = {
Â  Â 'rotating_proxies.middlewares.RotatingProxyMiddleware': 610,
Â  Â 'rotating_proxies.middlewares.BanDetectionMiddleware': 620,
}
```

```ad-todo
ç„¶åæœ‰ä¸€å † `IP:ç«¯å£` çš„è¯ï¼Œå¯ä»¥æ”¾åœ¨ä¸€ä¸ªtxtæ–‡ä»¶é‡Œï¼Œç„¶ååœ¨settingsé‡Œé¢æŒ‡å®š`ROTATING_PROXY_LIST_PATH = '/my/path/proxies.txt'` ,è¿™ä¸ªå¥½åƒå’Œclash for Windowsä½¿ç”¨çš„æ–¹æ³•æ˜¯ä¸€æ ·çš„ï¼Ÿ
```

![](images/Pasted%20image%2020240830205502.png)
è¿™ä¸ªå°±æ˜¯å¯ç”¨çš„æ—¥å¿—ï¼Œä½†æ˜¯ç»å¸¸æ€§å‘ç”Ÿçš„å°±æ˜¯å…¨éƒ¨éƒ½æ˜¯`dead`ï¼Œå› ä¸ºä½¿ç”¨çš„æ˜¯å…è´¹çš„æœåŠ¡å™¨ã€‚

é™¤æ­¤ä¹‹å¤–éƒ½æ˜¯ä¸€äº›ä¸“é—¨çš„æœåŠ¡æä¾›å•†ï¼Œä¾‹å¦‚smartProxyä¹‹ç±»çš„ï¼Œéƒ½éœ€è¦ä»˜è´¹



# Selenium
å¦‚æœç½‘é¡µä¸Šçš„åˆ†é¡µæ˜¯é€šè¿‡ç‚¹å‡»äº‹ä»¶è§¦å‘çš„ï¼Œè€Œä¸æ˜¯é€šè¿‡ç®€å•çš„è¶…é“¾æ¥ï¼ˆhrefå±æ€§ï¼‰ï¼Œè¿™é€šå¸¸æ„å‘³ç€åˆ†é¡µæ˜¯ç”± JavaScript åŠ¨æ€æ§åˆ¶çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨åƒ Scrapy è¿™æ ·çš„åŸºäºé™æ€åˆ†æçš„çˆ¬è™«å·¥å…·å¯èƒ½å°±æ— æ³•æœ‰æ•ˆè·å–åˆ°æ•°æ®ï¼Œå› ä¸º Scrapy ä¸æ‰§è¡Œ JavaScriptã€‚

Selenium åœ¨è¿™ç§æƒ…å†µä¸‹éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥æ¨¡æ‹Ÿå®é™…çš„æµè§ˆå™¨ç¯å¢ƒï¼Œæ‰§è¡Œ JavaScriptï¼Œå¹¶ä¸é¡µé¢ä¸Šçš„å…ƒç´ è¿›è¡Œäº¤äº’ï¼Œå°±åƒçœŸå®ç”¨æˆ·ä¸€æ ·ã€‚
é¦–å…ˆå®‰è£…
```python
pip install selenium
```
```ad-tip
æ³¨æ„å…ˆåœ¨æœ¬åœ°åˆ›å»ºè‡ªå·±çš„è™šæ‹Ÿç¯å¢ƒ
```


ç„¶åéœ€è¦ä¸€ä¸ª WebDriver æ¥ä¸æµè§ˆå™¨äº¤äº’ã€‚å¦‚æœä½ é€‰æ‹©ä½¿ç”¨ Chrome æµè§ˆå™¨è¿›è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•æˆ–çˆ¬è™«ï¼Œé‚£ä¹ˆéœ€è¦ä¸‹è½½ ChromeDriverï¼Œå¦‚æœæ˜¯Edgeæµè§ˆå™¨å°±ä¸‹è½½Edge Driverï¼Œæˆ‘è¿™é‡Œé€‰æ‹©Edgeï¼Œ
[Microsoft Edge WebDriver | Microsoft Edge Developer](https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/?form=MA13LH#downloads)
ä¸‹è½½ç¨³å®šç‰ˆæœ¬![](images/Pasted%20image%2020240831151404.png)
ç„¶åè§£å‹é‡Œé¢æœ‰ä¸ª`msedgedriver.exe`ï¼Œè¿™å°±æ˜¯seleniuméœ€è¦çš„

ç„¶ååŸºç¡€ä½¿ç”¨ï¼š
```python
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
import time

service = Service(executable_path="msedgedriver.exe")
driver = webdriver.Edge(service=service)
  
driver.get("https://google.com")
time.sleep(10)
driver.quit()
```
ä»£ç æ˜¯å’Œ`msedgedriver.exe`æ”¾åœ¨åŒä¸€ä¸ªè·¯å¾„ä¸‹çš„ã€‚
ä½œç”¨æ˜¯æ‰“å¼€Edgeæµè§ˆå™¨ï¼Œè®¿é—®è°·æ­Œé¡µé¢ï¼Œåç§’é’Ÿåå…³é—­é¡µé¢


å¯»æ‰¾å…ƒç´ çš„æ–¹æ³•ï¼š
```python
element = driver.find_element(By.XPATH,"//*[@id='APjFqb']")
```
è¿™é‡Œæ˜¯`XPATH`ï¼Œè¿˜æœ‰`CLASS_NAME`å’Œ`CSS_SELECTOR`è¿™äº›ã€‚

å¯»æ‰¾ä¹‹åæ— éæ˜¯è·å–å±æ€§å€¼å’Œè·å–å…ƒç´ å†…çš„æ–‡æœ¬ï¼Œåˆ†åˆ«æ˜¯ï¼š
```python
# è·å–æ–‡æœ¬
text = driver.find_element(By.ID, "justanotherlink").text
# è·å–å±æ€§å€¼
value_info = email_txt.get_attribute("value")
```



è·å–ä¿¡æ¯åå°±æ˜¯äº¤äº’ã€‚

####  å’Œé”®ç›˜æœ‰å…³
`send_keys()`Â ç”¨äºæ¨¡æ‹Ÿé”®ç›˜è¾“å…¥ï¼Œå°†æŒ‡å®šçš„å­—ç¬¦åºåˆ—ï¼ˆå¦‚æ–‡æœ¬å­—ç¬¦ä¸²ã€æ•°å­—ã€ç”šè‡³æ˜¯é”®ç›˜å¿«æ·é”®ï¼Œå¦‚å›è½¦ã€ç©ºæ ¼ç­‰ï¼‰å‘é€åˆ°å½“å‰ç„¦ç‚¹æ‰€åœ¨çš„å…ƒç´ ä¸­ã€‚åƒæœç´¢è¿™ç§å°±æ˜¯éœ€è¦ä¸€äº›å­—ç¬¦ä¸²åŠ ä¸Šå›è½¦ã€‚
- å‘é€å›è½¦é”®ï¼š`element.send_keys(Keys.RETURN)`Â æˆ–Â `element.send_keys(Keys.ENTER)`
- å‘é€ç©ºæ ¼ï¼š`element.send_keys("Hello, ")`Â æˆ–Â `element.send_keys(Keys.SPACE)`
- å‘é€åˆ¶è¡¨ç¬¦ï¼ˆTabï¼‰ï¼š`element.send_keys(Keys.TAB)`
- å‘é€åˆ é™¤é”®ï¼š`element.send_keys(Keys.BACK_SPACE)`
è¿™é‡ŒÂ `Keys`Â æ˜¯ä¸€ä¸ª Selenium æä¾›çš„ç±»ï¼ŒåŒ…å«äº†æ‰€æœ‰æ”¯æŒçš„é”®ç›˜æŒ‰é”®å¸¸é‡ã€‚
```python
input_element = driver.find_element(By.CSS_SELECTOR,"//*[@id='APjFqb']")
input_element.send_keys("tech with him" + Keys.ENTER)
```
åƒä¸Šé¢ç¬¬ä¸€è¡Œæ˜¯æ‰¾åˆ°é‚£ä¸ªç”¨äºæœç´¢çš„`textarea`ï¼Œç„¶åç¬¬äºŒè¡Œä½¿ç”¨é”®ç›˜è¾“å…¥ï¼Œæœ€åå¸¦ä¸ªå›è½¦ï¼Œå°±ä¼šè½¬åˆ°æœç´¢æˆåŠŸçš„é‚£ä¸ªé¡µé¢ã€‚



é‚£ä¹ˆä¸Šé¢è¾“å…¥äº†æ•°æ®ï¼Œä¹Ÿéœ€è¦è€ƒè™‘æ€ä¹ˆåˆ é™¤æˆ‘è¾“å…¥çš„å†…å®¹ï¼Œå¦‚ä¸‹ï¼š
```python
driver.find_element(By.NAME, "email_input").clear()
```


ç„¶åäº¤äº’åä¸€èˆ¬éœ€è¦ä¸€ç‚¹æ—¶é—´æ‰èƒ½æŠŠäº¤äº’ç»“æœåŠ è½½å®Œæˆï¼Œæ‰€ä»¥å°±éœ€è¦ç­‰å¾…ï¼Œä½¿ç”¨ä¸‹é¢çš„ä»£ç 
```python
    wait = WebDriverWait(driver, timeout=2)
    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'new-content'))
```
å¦‚æœåœ¨æŒ‡å®šçš„è¶…æ—¶å€¼ä¹‹å‰æœªæ»¡è¶³æ¡ä»¶ï¼Œåˆ™ è¯¥ä»£ç å°†ç»™å‡ºè¶…æ—¶é”™è¯¯ã€‚
æ„æ€å°±æ˜¯åœ¨2ç§’ä¹‹å†…éœ€è¦å­˜åœ¨è¿™ä¸ªç±»ä¸º`new-content`çš„å…ƒç´ ï¼Œå¦‚æœ2ç§’åè¿˜ä¸å­˜åœ¨ï¼Œå°±æŠ¥é”™è¶…æ—¶




#### å’Œscrapyçš„é›†æˆ
å› ä¸ºéœ€è¦äº¤äº’ä¸€èˆ¬éƒ½æ˜¯åœ¨åŠ è½½å¼€å§‹å°±è¿›è¡Œçš„ï¼Œæ¯”å¦‚æœç´¢ï¼Œæ¯”å¦‚`load more`è¿™ç§ï¼Œäºæ˜¯å°±å¯ä»¥å…ˆç‚¹å‡»ä¹‹åï¼Œå°†ç‚¹å‡»åçš„é¡µé¢å‘ç»™`parse()`
åœ¨`settings`é‡Œå…ˆè®¾ç½®
```python
# ç¡®ä¿ä¸‹è½½å»¶è¿Ÿè¶³ä»¥åŠ è½½ JavaScript 
DOWNLOAD_DELAY = 1
```

ç„¶ååœ¨`spider`é‡Œé¢
```python
def __init__(self): 
	service = Service(executable_path="msedgedriver.exe")
	driver = webdriver.Edge(service=service)
```
ç„¶ååœ¨`start_request`é‡Œé¢ç¬¬ä¸€æ¬¡ä½¿ç”¨get
```python
Â def start_requests(self):
Â  Â  Â  Â  for url in self.start_urls:
Â  Â  Â  Â  Â  Â  self.driver.get(url)
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  # ç­‰å¾…â€œåŠ è½½æ›´å¤šâ€æŒ‰é’®å‡ºç°å¹¶ç‚¹å‡»å®ƒ
Â  Â  Â  Â  Â  Â  Â  Â  load_more_button = WebDriverWait(self.driver, 10).until(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  EC.presence_of_element_located((By.CLASS_NAME, 'load-more'))
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  load_more_button.click()
Â  Â  Â  Â  Â  Â  Â  Â  # ç­‰å¾…å¯èƒ½ç”±äºç‚¹å‡»è€Œè§¦å‘çš„åŠ¨æ€å†…å®¹åŠ è½½
Â  Â  Â  Â  Â  Â  Â  Â  WebDriverWait(self.driver, 10).until(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  EC.presence_of_element_located((By.CLASS_NAME, 'new-content'))
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  self.logger.error(f"Error clicking load more button: {e}")
Â  Â  Â  Â  Â  Â  # ç°åœ¨å¤„ç†é¡µé¢ä¸Šçš„å†…å®¹
Â  Â  Â  Â  Â  Â  yield self.parse_initial(self.driver.page_source)
```
åŸºæœ¬çš„æ–¹æ³•éƒ½æ˜¯å…ˆ`driver`å»è·å–è¿æ¥ï¼Œç„¶åä½¿ç”¨`selenium`æ¥äº¤äº’ï¼Œç„¶åå¤„ç†äº¤äº’ä¹‹åçš„é¡µé¢å†…å®¹ï¼Œç„¶åå†æŠŠäº¤äº’åçš„é¡µé¢é‡Œå¯èƒ½å­˜åœ¨çš„urlæ‹¿å‡ºæ¥é€’å½’



















