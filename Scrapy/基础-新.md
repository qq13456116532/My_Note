这里选择在Vscode里面使用scrapy

首先使用Python官方推荐的虚拟环境管理模块来创建环境：
```python
python -m venv venv
```
- **`-m venv`**：使用运行库模块`venv`，用于创建虚拟环境。
- **`venv`**：这是创建的虚拟环境的目录名。
然后是在当前文件夹下创建这个虚拟环境
![](images/Pasted%20image%2020240828162550.png)
然后所有通过pip安装的Python包都将被安装在这个目录的相应子目录中，而不会影响到系统中的其他Python环境。
然后使用下面的命令激活：
```ls
.\venv\Scripts\activate
```
结果是这样，这就是成功地激活了当前的环境，在此命令提示符下运行的任何Python和pip命令都将仅在虚拟环境中运行。![](images/Pasted%20image%2020240828162816.png)


然后安装scrapy包
```python
pip install scrapy
```

# 流程大概说明
首先就得把settings文件里的
```python
# Obey robots.txt rules
ROBOTSTXT_OBEY = True
```
改成`False`
在spiders里面有个`start_requests`，里面使用Url请求之后，得到响应后自动传递给`parse()`来处理响应的内容，提取需要的数据字段，将这些数据封装到 `items` 中定义的类中，然后这些类通过`pipelines`，保存到数据库里面去。
然后里面的`middlewares` 可以处理超时设置、请求头的自定义、Cookie 管理等逻辑。中间件可以减少爬虫类中的代码量，将一些通用的逻辑转移到中间件中进行处理。
然后`settings`里面主要就是启用和不启用某些设置，例如可以启用或禁用某个中间件、是否启用 Pipelines、设置下载延迟等。
# 开发流程
首先要确保前面的scrapy包和虚拟环境已经激活，再做这一步。

创建的代码：
```python
scrapy startproject bookscraper
```
这里第三部分`bookscraper`是这个项目的名称。

#### 开启设置
在`settings.py`里面打开`pipelines`和`middlewares`，否则无法使用:

```python
# Enable or disable downloader middlewares
DOWNLOADER_MIDDLEWARES = {
   "bookscraper.middlewares.BookscraperDownloaderMiddleware": 543,
}
# Configure item pipelines
# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
   "bookscraper.pipelines.BookscraperPipeline": 300,
}
```

#### 创建spider
创建代码：
```js
scrapy genspider bookspider books.toscrape.com
```
`bookspider`是这个spider的名字，`books.toscrape.com`是这个spider要爬取的网站
```ad-important
books.toscrape.com这个网站就是专门用于训练爬虫的网站
```
![](images/Pasted%20image%2020240828215546.png)
![](images/Pasted%20image%2020240828215600.png)
上面就是创建出的最基本的spider， 其中的allowed_domains 主要是防止因为有外部链接而导致一直向外访问到其他的地方去了。


#### 手动查看元素
然后安装`ipython`
```js
pip install ipython
```
然后在`scrapy.cfg`里面配置shell属性
```python
[settings]
default = bookscraper.settings
shell = ipython
```
然后就可以在终端使用`scrapy shell`命令，得到下面的结果：
![](images/Pasted%20image%2020240829095258.png)
然后使用
```python
fetch('https://books.toscrape.com')
```
然后所有的结果都存在这个元素里了
![](images/Pasted%20image%2020240829100552.png)
现在需要做的是到浏览器手动查看一个元素的CSS是什么，例如![](images/Pasted%20image%2020240829101047.png)
然后就以Css选择器的形式拿到元素
```python
response.css('article.product_pod')
response.css('article.product_pod').get() #拿到html字符串
```

如果是要拿一个元素内的文本，例如：![](images/Pasted%20image%2020240829102347.png)那么是使用
```python
In [13]: book.css('h3 a::text').get()
Out[13]: 'A Light in the ...'
```


如果是要拿一个元素的属性，例如`<a>`里的href属性，就使用
```python
In [16]: book.css('h3 a').attrib['href']
Out[16]: 'catalogue/a-light-in-the-attic_1000/index.html'
```


#### 编写parse()方法
前面已经知道了元素的获取方法，然后就可以编写parse函数了，循环地处理页面里的所有类似元素。在`spider.py`里面
```python
    def parse(self, response):
        books = response.css('article.product_pod')
        for book in books:
            yield{
                'name': book.css('h3 a::text').get(),
                'price': book.css('div.product_price p.price_color::text').get(),
                'url':book.css('h3 a').attrib['href']
   }
```

写完parse后，就可以启动spider了，
```python
scrapy crawl bookspider
```
`bookspider`是在spider.py中的name属性值
```ad-attention
使用这个命令必须在scrapy.cfg的同级目录下，这里先把pipelines和middlewares全部关闭，并且不要使用代理！！！
```
然后结果里面会有个`item_scrapped_count`，这个属性表示的是 代码中`yield`的总数
![](images/Pasted%20image%2020240829154242.png)



#### 分页处理
数据一般是分很多页处理，如下，所以需要在当前页转到下一页
![](images/Pasted%20image%2020240829111117.png)
首先使用css选择器拿到这个按钮对应的链接
```python
In [2]: response.css('li.next a').attrib['href']
Out[2]: 'catalogue/page-2.html'
```
然后在parse()中使用如下的代码：
```python
# 如果到了最后一页就没有链接了
next_page = response.css('li.next a').attrib['href']
if next_page is not None:
  next_page_url = "https://books.toscrape.com/"+next_page
  yield response.follow(next_page_url,callback = self.parse)
```
`follow` 方法发送一个新的请求，目标是构建的 `next_page_url`，`callback=self.parse` 表示当收到响应时，继续调用当前的 `parse` 方法来处理下一页的内容。这就形成了一个递归过程，爬虫会一页接一页地抓取，直到没有“下一页”为止。

#### 详情页处理
一般都是从列表页-->详情页，在列表页面里面一个一个走完详情页之后再去下一页，那么这里使用的方法和分页处理大差不差，就是找到去详情页面的链接，然后分析链接对应的页面
```python
for book in books:
   # 拿到详情页的链接
   relative_url = book.css('h3 a').attrib['href']
   if 'catalogue/' in relative_url:
       next_relative_url = "https://books.toscrape.com/"+relative_url
   else:
       next_relative_url = next_page_url = "https://books.toscrape.com/catalogue/"+relative_url
       # 详情页链接返回响应后，回调后面的函数处理
       yield response.follow(next_relative_url,callback = self.parse_book_page)
```



#### 使用Xpath
有些极端情况可能让我们没有简单的类名或HTML标签的ID，比如下面这个![](images/Pasted%20image%2020240829164032.png)想得到的是那个 Poetry ,但是父标签`li`无法找到，所以这就需要使用到Xpath了，因为`ul`和下面的`li .active`可以找到，所以就相对这两个标签来找到当前的标签，如下
```js
response.xpath("//ul[@class='breadcrumb']/li[@class='active']/preceding-sibling::li[1]/a/text()").get()
```
**注意注意注意，这个是不需要自己看着写的，可以直接右键复制出来：**
![](images/Pasted%20image%2020240831200046.png)

```ad-attention

- `//ul[@class='breadcrumb']` 会选中整个 `<ul>`。
- `/li[@class='active']` 会选中 `Current Page` 所在的 `<li>` 元素。
- `/preceding-sibling::li[1]` 会选中紧靠 `Current Page` 之前的那个 `<li>`，也就是包含 `Category` 链接的 `<li>`。对应的有`following-sibling`。使用[1]是因为兄弟标签有很多个`<li>`
- `/a/text()` 会提取出 `Category` 链接的文本，即 `"Category"`。

```



#### 定义Item
`Item`是帮助我们定义我们想要的东西，主要是规范化和标准化抓取的数据。
![](images/Pasted%20image%2020240829192132.png)像上面这样的返回方式就是很乱，但是如果是返回一个对象就会显得更加整齐。
定义这个类：
```python
class BookItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    url = scrapy.Field()
    title = scrapy.Field()
    product_type = scrapy.Field()
    price_excl_tax = scrapy.Field()
    price_incl_tax = scrapy.Field()
    tax = scrapy.Field()
    availability = scrapy.Field()
    num_reviews = scrapy.Field()
    stars = scrapy.Field()
    category = scrapy.Field()
    description = scrapy.Field()
    price = scrapy.Field()
```
使用：![](images/Pasted%20image%2020240829201125.png)


#### 使用pipelines
这个主要是来对收集到的数据进行处理，比如说把字符串转为数字，去掉空格，加个符号之类的。
里面有个类是`ItemAdapter`，**很重要**，可以遍历传入的类的属性，然后进行处理，使用示例：
```python
from itemadapter import ItemAdapter

class BookscraperPipeline:
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        field_names = adapter.field_names()
        # 去掉空格
        for field_name in field_names:
            if field_name != 'description':
                value = adapter.get(field_name)
                adapter[field_name] = value.strip()
        return item
```
可以看到主要是 `get()` 和`[]`两种用法。

编写`pipelines`完成后要注意在`settings.py`里面启动它。


#### 数据保存文件
最直接的方式就是在命令上指定：
```python
 scrapy crawl bookspider -O bookdata.csv
```
可以是`csv`，也可以是`json`


也可以通过在`settings.py`里面设置数据的保存方法：
```python
FEEDS = {
    'booksdata.json':{'format':'json'}
}
```
然后这样就不需要指定 -0 了。只需要
```python
 scrapy crawl bookspider
```


第三种就是在`pipelines`里面保存数据，也只有这种方式可以保存到数据库内。
首先安装：
```python
pip install mysql mysql-connector-python
```
然后创建一个使用MySQL的pipeline，在初始化类时创建出连接，编写`process_item(self, item, spider)`方法，然后spider关闭时关闭连接，如下
```python
import mysql.connector
class SaveToMySQLPipeline:
    def __init__(self) :
        self.conn = mysql.connector.connect(
            host = 'localhost',
            user='root',
            password='root',
            database='books'
        )
        self.cur = self.conn.cursor()
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS books (
            id INT AUTO_INCREMENT PRIMARY KEY ,
            url VARCHAR(255),
            price DECIMAL
            );
        """)
    def process_item(self, item, spider):
        self.cur.execute("""
            INSERT INTO books (
                url,
                price
            ) VALUES (%s,%s,%s); """,item["url"],item["price"])
        self.conn.commit()
        return item
    # 当spider准备关闭时
    def close_spider(self,spider):
        self.cur.close()
        self.conn.close()
```
然后在`settings`里面开启这个pipeline，
```python
ITEM_PIPELINES = {
   "bookscraper.pipelines.BookscraperPipeline": 300,
   "bookscraper.pipelines.SaveToMySQLPipeline": 400,
}
```
后面的数字是决定执行的顺序，数字越小，越先执行。









# 解决Blocked
通过检查 HTTP 请求中的 `User-Agent` 字符串，网站可以区分请求是来自浏览器还是其他类型的 HTTP 客户端，如 API 请求或特定类型的爬虫。`User-Agent` 字符串通常包含了关于发出请求的浏览器或其他客户端的信息，比如它的名称、版本以及操作系统。
如果 User-Agent 显示为某些特定的库（如 Python 的 `requests` 默认 User-Agent 或者某些爬虫框架），网站可能会选择阻止这些请求。
如果Headers里面内容每次都相似的话也容易被看出来是爬虫

所以解决方法就是让每次请求的这些内容都发生改变。

#### 复制自己的user-agent
在浏览器访问要爬取的页面找到那个User-Agent, 然后复制到settings.py,
![](images/Pasted%20image%2020240830154818.png)
在`settings`里
```python
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0"
```
但是这种固定一个的方式就会被网站认为同一个人连续请求了很多次，容易被怀疑为爬虫。


#### 轮询agent
在`spider`内放一个`user-agent`的列表
```python
user_agent_list = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0'
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0"
]
```
然后在请求的地方使用这个`user-agent`来代替
```python
yield response.follow(next_relative_url,callback = self.parse_book_page , headers={"User-Agent": self.user_agent_list[random.randint(0,len(self.user_agent_list)-1)]})
```
就是随机挑一个`user-agent`去请求

但是！！！如果短时间几千个请求，但是只有这五种用户，还是会被怀疑是爬虫。


#### 第三方的user-agent
第三种方法是，在`midllewares`里面，使用来自第三方网站的用户代理。
[ScrapeOps - The DevOps Tool For Web Scraping. | ScrapeOps](https://scrapeops.io/)
然后
![](images/Pasted%20image%2020240830162922.png)

这里使用方法已经说的很明白了：![](images/Pasted%20image%2020240830171157.png)
编写个`middleware`，初始化时获取很多个伪造的`User-Agent`，然后请求时随机取其中一个，代码基本是通用的：
```python
from urllib.parse import urlencode
from random import randint
import requests
  
class ScrapeOpsFakeUserAgentMiddleware:
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.settings)
    def __init__(self,settings) -> None:
        self.scrapeops_api_key = settings.get("SCRAPEOPS_API_KEY")
        self.scrapeops_endpoint = settings.get("SCRAPEOPS_FAKE_USER_AGENT_ENDPOINT","https://headers.scrapeops.io/v1/user-agents")
        self.scrapeops_fake_user_agents_active = settings.get("SCRAPEOPS_FAKE_USER_AGENT_ENABLED",False)
        self.scrapeops_num_results = settings.get("SCRAPEOPS_NUM_RESULTS")
        self.headers_list =[]
        self._get_user_agents_list()
        self._scrapeops_fake_user_agents_enabled()
    def _get_user_agents_list(self):
        payload = {'api_key': self.scrapeops_api_key}
        if self.scrapeops_num_results is not None:
            payload['num_results'] = self.scrapeops_num_results
        response = requests.get(self.scrapeops_endpoint,params=urlencode(payload))
        json_response = response.json()
        self.user_agents_list = json_response.get('result',[])
    def _get_random_user_agent(self):
        random_index = randint(0,len(self.user_agents_list)-1)
        return self.user_agents_list[random_index]
    def _scrapeops_fake_user_agents_enabled(self):
        if(self.scrapeops_api_key is None or self.scrapeops_api_key=='' or self.scrapeops_num_results==0):
            self.scrapeops_fake_user_agents_active=False
        else:
            self.scrapeops_fake_user_agents_active=True
    def process_request(self, request, spider):
        random_user_agent = self._get_random_user_agent()
        request.headers['User-Agent']=random_user_agent
```
然后在`settings`里面配置相关参数：
```python
# 改成自己的API_KEY
SCRAPEOPS_API_KEY="0265994d-3869-4db2-bd67-2513b4513d2b"
SCRAPEOPS_FAKE_USER_AGENT_ENDPOINT="https://headers.scrapeops.io/v1/user-agents"
SCRAPEOPS_FAKE_USER_AGENT_ENABLED=True
SCRAPEOPS_NUM_RESULTS=50
```
并且在`settings`里面启用这个middleware：
```python
DOWNLOADER_MIDDLEWARES = {
   "bookscraper.middlewares.ScrapeOpsFakeUserAgentMiddleware": 400,
}
```
注意是`DOWNLOADER_MIDDLEWARES`


#### 第三方的header
使用这个就不要使用上面的了，因为这里同时更改了`user-agent`
除了`user-agent`，`headers`也每次都是新的是最保险的，代码如下：
```python
class ScrapeOpsFakeBrowserHeaderAgentMiddleware:
    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.settings)
    def __init__(self, settings):
        self.scrapeops_api_key = settings.get('SCRAPEOPS_API_KEY')
        self.scrapeops_endpoint = settings.get('SCRAPEOPS_FAKE_BROWSER_HEADER_ENDPOINT', 'https://headers.scrapeops.io/v1/browser-headers')
        self.scrapeops_fake_browser_headers_active = settings.get('SCRAPEOPS_FAKE_BROWSER_HEADER_ENABLED', False)
        self.scrapeops_num_results = settings.get('SCRAPEOPS_NUM_RESULTS')
        self.headers_list = []
        self._get_headers_list()
        self._scrapeops_fake_browser_headers_enabled()
    def _get_headers_list(self):
        payload = {'api_key': self.scrapeops_api_key}
        if self.scrapeops_num_results is not None:
            payload['num_results'] = self.scrapeops_num_results
        response = requests.get(self.scrapeops_endpoint, params=urlencode(payload))
        json_response = response.json()
        self.headers_list = json_response.get('result', [])
    def _get_random_browser_header(self):
        random_index = randint(0, len(self.headers_list) - 1)
        return self.headers_list[random_index]
    def _scrapeops_fake_browser_headers_enabled(self):
        if self.scrapeops_api_key is None or self.scrapeops_api_key == '' or self.scrapeops_fake_browser_headers_active == False:
            self.scrapeops_fake_browser_headers_active = False
        else:
            self.scrapeops_fake_browser_headers_active = True
    def process_request(self, request, spider):        
        random_browser_header = self._get_random_browser_header()
        request.headers['accept-language'] = random_browser_header['accept-language']
        request.headers['sec-fetch-user'] = random_browser_header['sec-fetch-user']
        request.headers['sec-fetch-mod'] = random_browser_header['sec-fetch-mod']
        request.headers['sec-fetch-site'] = random_browser_header['sec-fetch-site']
        request.headers['sec-ch-ua-platform'] = random_browser_header['sec-ch-ua-platform']
        request.headers['sec-ch-ua-mobile'] = random_browser_header['sec-ch-ua-mobile']
        request.headers['sec-ch-ua'] = random_browser_header['sec-ch-ua']
        request.headers['accept'] = random_browser_header['accept']
        request.headers['user-agent'] = random_browser_header['user-agent']
        request.headers['upgrade-insecure-requests'] = random_browser_header.get('upgrade-insecure-requests')
        print("************ NEW HEADER ATTACHED *******")
        print(request.headers)
```
然后在`settings`里面设置相关参数，并开启这个`middleware`
```python
SCRAPEOPS_API_KEY="0265994d-3869-4db2-bd67-2513b4513d2b"
SCRAPEOPS_FAKE_BROWSER_HEADER_ENDPOINT="https://headers.scrapeops.io/v1/browser-headers"
SCRAPEOPS_FAKE_BROWSER_HEADER_ENABLED=True
SCRAPEOPS_NUM_RESULTS=5
DOWNLOADER_MIDDLEWARES = {
  #"bookscraper.middlewares.ScrapeOpsFakeUserAgentMiddleware": 400,
  "bookscraper.middlewares.ScrapeOpsFakeBrowserHeaderAgentMiddleware": 400,
}
```


# 解决anti-bot
上面是解决了网站通过请求头来确认是否爬虫的情况，但是随着请求头传输的还有IP地址，这个也有可能被网站用来判断是否为爬虫。所以这里使用代理来解决这个问题，就是轮换IP地址。

#### scrapy-rotating-proxies
首先安装
```python
pip install scrapy-rotating-proxies
```
然后在settings里面添加
```python
ROTATING_PROXY_LIST = [
    'proxy1.com:8000',
    'proxy2.com:8031',
]
```
这里的IP和端口就是要代理到的其他服务器，那么这里有在线的服务器可供使用：
[🤖 Free Proxy List [1000+ IPs Online] (geonode.com)](https://geonode.com/free-proxy-list)
![](images/Pasted%20image%2020240830203417.png)
就在这里面挑两三个出来使用，上面的列表就改成
```python
ROTATING_PROXY_LIST = [
    '160.248.190.253:3128',
    '15.204.161.192:18080',
    '51.89.14.70:80'
]
```
然后在`settings`里面开启这个middleware
```python
DOWNLOADER_MIDDLEWARES = {
   'rotating_proxies.middlewares.RotatingProxyMiddleware': 610,
   'rotating_proxies.middlewares.BanDetectionMiddleware': 620,
}
```

```ad-todo
然后有一堆 `IP:端口` 的话，可以放在一个txt文件里，然后在settings里面指定`ROTATING_PROXY_LIST_PATH = '/my/path/proxies.txt'` ,这个好像和clash for Windows使用的方法是一样的？
```

![](images/Pasted%20image%2020240830205502.png)
这个就是启用的日志，但是经常性发生的就是全部都是`dead`，因为使用的是免费的服务器。

除此之外都是一些专门的服务提供商，例如smartProxy之类的，都需要付费



# Selenium
如果网页上的分页是通过点击事件触发的，而不是通过简单的超链接（href属性），这通常意味着分页是由 JavaScript 动态控制的。在这种情况下，使用像 Scrapy 这样的基于静态分析的爬虫工具可能就无法有效获取到数据，因为 Scrapy 不执行 JavaScript。

Selenium 在这种情况下非常有用，因为它可以模拟实际的浏览器环境，执行 JavaScript，并与页面上的元素进行交互，就像真实用户一样。
首先安装
```python
pip install selenium
```
```ad-tip
注意先在本地创建自己的虚拟环境
```


然后需要一个 WebDriver 来与浏览器交互。如果你选择使用 Chrome 浏览器进行自动化测试或爬虫，那么需要下载 ChromeDriver，如果是Edge浏览器就下载Edge Driver，我这里选择Edge，
[Microsoft Edge WebDriver | Microsoft Edge Developer](https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/?form=MA13LH#downloads)
下载稳定版本![](images/Pasted%20image%2020240831151404.png)
然后解压里面有个`msedgedriver.exe`，这就是selenium需要的

然后基础使用：
```python
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
import time

service = Service(executable_path="msedgedriver.exe")
driver = webdriver.Edge(service=service)
  
driver.get("https://google.com")
time.sleep(10)
driver.quit()
```
代码是和`msedgedriver.exe`放在同一个路径下的。
作用是打开Edge浏览器，访问谷歌页面，十秒钟后关闭页面


寻找元素的方法：
```python
element = driver.find_element(By.XPATH,"//*[@id='APjFqb']")
```
这里是`XPATH`，还有`CLASS_NAME`和`CSS_SELECTOR`这些。

寻找之后无非是获取属性值和获取元素内的文本，分别是：
```python
# 获取文本
text = driver.find_element(By.ID, "justanotherlink").text
# 获取属性值
value_info = email_txt.get_attribute("value")
```



获取信息后就是交互。

####  和键盘有关
`send_keys()` 用于模拟键盘输入，将指定的字符序列（如文本字符串、数字、甚至是键盘快捷键，如回车、空格等）发送到当前焦点所在的元素中。像搜索这种就是需要一些字符串加上回车。
- 发送回车键：`element.send_keys(Keys.RETURN)` 或 `element.send_keys(Keys.ENTER)`
- 发送空格：`element.send_keys("Hello, ")` 或 `element.send_keys(Keys.SPACE)`
- 发送制表符（Tab）：`element.send_keys(Keys.TAB)`
- 发送删除键：`element.send_keys(Keys.BACK_SPACE)`
这里 `Keys` 是一个 Selenium 提供的类，包含了所有支持的键盘按键常量。
```python
input_element = driver.find_element(By.CSS_SELECTOR,"//*[@id='APjFqb']")
input_element.send_keys("tech with him" + Keys.ENTER)
```
像上面第一行是找到那个用于搜索的`textarea`，然后第二行使用键盘输入，最后带个回车，就会转到搜索成功的那个页面。



那么上面输入了数据，也需要考虑怎么删除我输入的内容，如下：
```python
driver.find_element(By.NAME, "email_input").clear()
```


然后交互后一般需要一点时间才能把交互结果加载完成，所以就需要等待，使用下面的代码
```python
    wait = WebDriverWait(driver, timeout=2)
    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'new-content'))
```
如果在指定的超时值之前未满足条件，则 该代码将给出超时错误。
意思就是在2秒之内需要存在这个类为`new-content`的元素，如果2秒后还不存在，就报错超时




#### 和scrapy的集成
因为需要交互一般都是在加载开始就进行的，比如搜索，比如`load more`这种，于是就可以先点击之后，将点击后的页面发给`parse()`
在`settings`里先设置
```python
# 确保下载延迟足以加载 JavaScript 
DOWNLOAD_DELAY = 1
```

然后在`spider`里面
```python
def __init__(self): 
	service = Service(executable_path="msedgedriver.exe")
	driver = webdriver.Edge(service=service)
```
然后在`start_request`里面第一次使用get
```python
 def start_requests(self):
        for url in self.start_urls:
            self.driver.get(url)
            try:
                # 等待“加载更多”按钮出现并点击它
                load_more_button = WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CLASS_NAME, 'load-more'))
                )
                load_more_button.click()
                # 等待可能由于点击而触发的动态内容加载
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CLASS_NAME, 'new-content'))
                )
            except Exception as e:
                self.logger.error(f"Error clicking load more button: {e}")
            # 现在处理页面上的内容
            yield self.parse_initial(self.driver.page_source)
```
基本的方法都是先`driver`去获取连接，然后使用`selenium`来交互，然后处理交互之后的页面内容，然后再把交互后的页面里可能存在的url拿出来递归



















