对于序列数据，可以使用循环神经网络（Recurrent Natural Network，RNN），它特 别适合处理序列数据，RNN是一种常用的神经网络结构，已经成功应用于自然语言处理 （Neuro-Linguistic Programming，NLP）、语音识别、图片标注、机器翻译等众多时序问 题中。
![](images/Pasted%20image%2020230725151249.png)

把隐含层再细化就可得如图
![](images/Pasted%20image%2020230725151328.png)
循环神网络的反向传播训练算法称为随时间反向传播（Backpropagation Through Time，BPTT）算法，其基本原理和反向传播算法是一样的。只不过，反向传播算法是按 照层进行反向传播，而BPTT是按照时间t进行反向传播


实际上梯度消失或爆炸问题是深度学习中的一个基本问题，在任何深度神经网络中都 可能存在，而不仅是循环神经网络所独有。在RNN中，相邻时间步是连接在一起的，因 此，它们的权重偏导数要么都小于1，要么都大于1，RNN中每个权重都会向相同方向变 化，这样，与前馈神经网络相比，RNN的梯度消失或爆炸更为明显。由于简单RNN遇到 时间步（timestep）较大时，容易出现梯度消失或爆炸问题，且随着层数的增加，网络最 终无法训练，无法实现长时记忆。这就导致RNN存在短时记忆问题，这个问题在自然语 言处理中是非常致命的，那如何解决这个问题？解决RNN中的梯度消失方法很多，常用 的有：LSTM结构

长短时记忆网络（Long Short-Term Memory,LSTM）:
LSTM用两个门来控制 单元状态c的内容，一个是遗忘门（Forget Gate），它决定了上一时刻的单元状态ct-1有多 少保留到当前时刻ct；另一个是输入门（Input Gate），它决定了当前时刻网络的输入xt有 多少保存到单元状态ct。LSTM用输出门（Output Gate）来控制单元状态ct有多少输出到 LSTM的当前输出值ht![](images/Pasted%20image%2020230725180257.png)

